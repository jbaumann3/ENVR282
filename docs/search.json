[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ENVR 282",
    "section": "",
    "text": "Welcome to ENVR 282: Research Methods in Environmental Science. All of your lab assignments and lecture slides are available here! Check Lyceum for additional details and to turn in work."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Lab 0: Getting Started",
    "section": "",
    "text": "IN THIS TUTORIAL YOU WILL LEARN:\n1.) How to access and/or install R and RStudio\n2.) How to navigate RStudio\n3.) How to set and change the working directory\n4.) How to setup an RStudio Project\n5.) How to make a quarto doc\n6.) Some R basics"
  },
  {
    "objectID": "getting_started.html#how-to-set-the-working-directory",
    "href": "getting_started.html#how-to-set-the-working-directory",
    "title": "Lab 0: Getting Started",
    "section": "How to SET the working directory",
    "text": "How to SET the working directory\n1.) Using the “Files” tab to set manually: a.) Using the ‘…’ in the ‘Files’ tab you can select any directory (folder) on your computer. You can also set a google drive, box, dropbox, or other shared folder as your working directory if you’d like (as long as you are syncing a folder between the cloud and your computer – ASK me if you have questions about this!) b.) Once you navigate to a directory you still need to SET IT as your working directory. You do this in the “More” cog– select “Set as working directory”\n2.) Set working directory with code: We use the ‘setwd()’ function for this. Below is an example. You will need to replace the path details with your own!\n\nsetwd(\"C:/Users/Justin Baumann/Teaching/Bates College/ENV 282 - Research Design in Env Sci\")"
  },
  {
    "objectID": "getting_started.html#to-make-a-quarto-document",
    "href": "getting_started.html#to-make-a-quarto-document",
    "title": "Lab 0: Getting Started",
    "section": "To make a Quarto document",
    "text": "To make a Quarto document\nclick file -&gt; new file -&gt; Quarto document / Complete the pop up prompts and then wait for the document to load. / We want to replace the top bit (our YAML header, everything between the two lines that contains just — at the top) with the following (use your name and title!)\n\n---\ntitle: \"Lab 1: Intro to R, RStudio, and Quarto\"\nauthor: \"Justin Baumann\"\nformat: \n  html:\n    toc: true\n  pdf:\n    toc: true\n    number-sections: true\n    colorlinks: true\neditor: visual\n---"
  },
  {
    "objectID": "getting_started.html#formatting-text",
    "href": "getting_started.html#formatting-text",
    "title": "Lab 0: Getting Started",
    "section": "Formatting text",
    "text": "Formatting text\nUnlike in a regular R script, using the ‘#’ at the start of a line will not comment that line out. Instead, you can type as you would normally in an R Markdown (Rmd) document. We can format our text in the following ways:\n\nBold: ‘’ on either end of a word, phrase, or line will make it bold! this is in bold** =’‘this is in bold’’ without the quotes around the **\n\nLine breaks: DO you want text to be on different lines? Insert a ’’ at the end of a line to make a line break!"
  },
  {
    "objectID": "getting_started.html#making-a-code-chunk",
    "href": "getting_started.html#making-a-code-chunk",
    "title": "Lab 0: Getting Started",
    "section": "Making a code chunk",
    "text": "Making a code chunk\nSince qmd documents are text based, we need to tell RStudio when we want to actually include code. To do this, we will insert a code chunk. To insert a code chunk:\n\n1.) Use the keyboard shortcut ‘ctrl’+‘alt’+‘i’ (PC) or ‘cmd’+‘alt’+‘i’ (Mac) to insert a code chunk.\n\n2.) Navigate to the top bar (of the top left quadrant of RStudio), find “+c” at the right of the bar to insert an R code chink.\nOnce you have a code chunk inserted you will notice that the background of the chunk is gray instead of your default background color (white or black if you are in dark mode)\n\n#this is an example code chunk\n\n# Using '#' at the start of a line indicates a comment, which is not runnable code!"
  },
  {
    "objectID": "getting_started.html#rendering-your-report",
    "href": "getting_started.html#rendering-your-report",
    "title": "Lab 0: Getting Started",
    "section": "Rendering your report",
    "text": "Rendering your report\nTo Visualize what your report will look like, click the ‘visual’ tab in the top bar (on the left). Note that if you do this, it CAN change your code–so be careful. You can also use the GUI to alter your report in the visual tab. This provides a nice alternative to the code based formatting options in the ‘source’ tab.\n\nTo actually render into an html or pdf document, you must click “Render”. You can use the arrow to the right of “Render” to choose render to html or render to pdf. I suggest using HTML most of the time but you can use pdf if you prefer. You will need to successfully Render your quarto document into an html or pdf report in order to turn in your labs!"
  },
  {
    "objectID": "images/Lab 4_ T-tests.html",
    "href": "images/Lab 4_ T-tests.html",
    "title": "Lab 4: T-tests",
    "section": "",
    "text": "IN THIS TUTORIAL YOU WILL LEARN:  1.) The theory behind a T-test and how to perform one  2.) Practice data preparation skills  3.) How to pair graphs and stats to test hypotheses"
  },
  {
    "objectID": "images/Lab 4_ T-tests.html#t-test-theory",
    "href": "images/Lab 4_ T-tests.html#t-test-theory",
    "title": "Lab 4: T-tests",
    "section": "T-test theory",
    "text": "T-test theory\nThe t-test (or students’ t-test) is a basic statistical test used to assess whether or not the means of two groups are different from one another. In this test, the null hypothesis is that the two means are equal (or that there is no difference between the two means).\nA t-test should only be used if the following assumptions are met:  1.) the two distributions whose means we are comparing must be normally distributed  2.) The variances of the two groups must be equal \nGenerate example data\n\niris2&lt;-iris %&gt;%\n  filter(Species != 'setosa') %&gt;%\n  droplevels() #removes the empty levels so when we check levels below we only get the ones that are still in the data!\n\n#check levels to make sure we only have 2 species!\nhead(iris2)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1          7.0         3.2          4.7         1.4 versicolor\n2          6.4         3.2          4.5         1.5 versicolor\n3          6.9         3.1          4.9         1.5 versicolor\n4          5.5         2.3          4.0         1.3 versicolor\n5          6.5         2.8          4.6         1.5 versicolor\n6          5.7         2.8          4.5         1.3 versicolor\n\nlevels(iris2$Species)\n\n[1] \"versicolor\" \"virginica\" \n\n\nWe will use these data for our examples today. T-test requires only 2 groups/populations. We will assess the alternative hypothesis that one of our numerical variables (sepal length, sepal width, petal length, or petal width) differs by species.\nBut first, we must test our assumptions"
  },
  {
    "objectID": "images/Lab 4_ T-tests.html#assumption-1.-assessing-normality",
    "href": "images/Lab 4_ T-tests.html#assumption-1.-assessing-normality",
    "title": "Lab 4: T-tests",
    "section": "Assumption 1.) Assessing normality",
    "text": "Assumption 1.) Assessing normality\nMethod 1: the Shapiro-Wilk Test If p &lt; 0.05 then the distribution is significantly different from normal.\nStep 1: we need to create separate data frames for each species to assess normality of each variable by species!\n\nversi&lt;-iris2 %&gt;%\n  filter(Species=='versicolor') %&gt;%\n  droplevels()\n\nvirg&lt;-iris2 %&gt;%\n  filter(Species=='virginica') %&gt;%\n  droplevels()\n\n\nStep 2: We can run our shapiro-wilk tests on each variable if we’d like\n\nshapiro.test(versi$Petal.Length) #this is normally distributed\n\n\n    Shapiro-Wilk normality test\n\ndata:  versi$Petal.Length\nW = 0.966, p-value = 0.1585\n\nshapiro.test(versi$Petal.Width) # this is not\n\n\n    Shapiro-Wilk normality test\n\ndata:  versi$Petal.Width\nW = 0.94763, p-value = 0.02728\n\nshapiro.test(versi$Sepal.Length) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  versi$Sepal.Length\nW = 0.97784, p-value = 0.4647\n\nshapiro.test(versi$Sepal.Width) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  versi$Sepal.Width\nW = 0.97413, p-value = 0.338\n\nshapiro.test(virg$Petal.Length) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  virg$Petal.Length\nW = 0.96219, p-value = 0.1098\n\nshapiro.test(virg$Petal.Width) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  virg$Petal.Width\nW = 0.95977, p-value = 0.08695\n\nshapiro.test(virg$Sepal.Length) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  virg$Sepal.Length\nW = 0.97118, p-value = 0.2583\n\nshapiro.test(virg$Sepal.Width) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  virg$Sepal.Width\nW = 0.96739, p-value = 0.1809\n\n\n Method 2: Visualization\nExplore the following visualizations. Do you see clear evidence of normality?\n\na1&lt;-ggplot(data=iris2, aes(Petal.Length, fill=Species))+\n  geom_histogram(binwidth = 0.3)+ \n  facet_wrap(~Species)+\n  theme_classic()+\n  scale_fill_aaas()\n\na2&lt;-ggplot(data=iris2, aes(x=Petal.Length, y=Species, fill=Species))+\n  geom_density_ridges()+ #makes a smooth density curve instead of a histogram!\n  theme_classic()+\n  scale_fill_aaas()\n\na1/a2 #compare the visualizations (they are of the same data)- do we see normality here?\n\nPicking joint bandwidth of 0.206\n\n\n\n\n\n\nb1&lt;-ggplot(data=iris2, aes(Petal.Width, fill=Species))+\n  geom_histogram(binwidth = 0.3)+ \n  facet_wrap(~Species)+\n  theme_classic()+\n  scale_fill_aaas()\n\nb2&lt;-ggplot(data=iris2, aes(x=Petal.Width, y=Species, fill=Species))+\n  geom_density_ridges()+ #makes a smooth density curve instead of a histogram!\n  theme_classic()+\n  scale_fill_aaas()\n\nb1/b2 #compare the visualizations (they are of the same data)- do we see normality here?\n\nPicking joint bandwidth of 0.0972\n\n\n\n\n\n\nc1&lt;-ggplot(data=iris2, aes(Sepal.Width, fill=Species))+\n  geom_histogram(binwidth = 0.3)+ \n  facet_wrap(~Species)+\n  theme_classic()+\n  scale_fill_aaas()\n\nc2&lt;-ggplot(data=iris2, aes(x=Sepal.Width, y=Species, fill=Species))+\n  geom_density_ridges()+ #makes a smooth density curve instead of a histogram!\n  theme_classic()+\n  scale_fill_aaas()\n\nc1/c2 #compare the visualizations (they are of the same data)- do we see normality here?\n\nPicking joint bandwidth of 0.122\n\n\n\n\n\n\nd1&lt;-ggplot(data=iris2, aes(Sepal.Length, fill=Species))+\n  geom_histogram(binwidth = 0.3)+ \n  facet_wrap(~Species)+\n  theme_classic()+\n  scale_fill_aaas()\n\nd2&lt;-ggplot(data=iris2, aes(x=Sepal.Length, y=Species, fill=Species))+\n  geom_density_ridges()+ #makes a smooth density curve instead of a histogram!\n  theme_classic()+\n  scale_fill_aaas()\n\nd1/d2 #compare the visualizations (they are of the same data)- do we see normality here?\n\nPicking joint bandwidth of 0.21"
  },
  {
    "objectID": "images/Lab 4_ T-tests.html#assumption-2.-assessing-equal-variance",
    "href": "images/Lab 4_ T-tests.html#assumption-2.-assessing-equal-variance",
    "title": "Lab 4: T-tests",
    "section": "Assumption 2.) Assessing equal variance",
    "text": "Assumption 2.) Assessing equal variance\nAKA homogeneity of variance \nMethods 1: F-test We will use the F-Test to compare the variance of two populations. This can only be used with 2 populations and is thus only useful when we run a t-test.\nH0 for an F-test is: The variances of the two groups are equal.  Ha: The variances are different  p&lt;0.05 allows us to reject the null (H0) and suggests that the variances are different   note: The F-test assumes our data are already normal! You should not run it on non-normal data\n\n#we use var.test to run an F-test\nf1&lt;- var.test(Petal.Length ~ Species, data=iris2)\nf1 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\n\n    F test to compare two variances\n\ndata:  Petal.Length by Species\nF = 0.72497, num df = 49, denom df = 49, p-value = 0.2637\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.411402 1.277530\nsample estimates:\nratio of variances \n         0.7249678 \n\nf2&lt;- var.test(Petal.Width ~ Species, data=iris2)\nf2 # p&lt;0.05, so we reject H0 (variances are likely different)\n\n\n    F test to compare two variances\n\ndata:  Petal.Width by Species\nF = 0.51842, num df = 49, denom df = 49, p-value = 0.02335\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.2941935 0.9135614\nsample estimates:\nratio of variances \n         0.5184243 \n\nf3&lt;- var.test(Sepal.Length ~ Species, data=iris2)\nf3 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\n\n    F test to compare two variances\n\ndata:  Sepal.Length by Species\nF = 0.65893, num df = 49, denom df = 49, p-value = 0.1478\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3739257 1.1611546\nsample estimates:\nratio of variances \n         0.6589276 \n\nf4&lt;- var.test(Sepal.Width ~ Species, data=iris2)\nf4 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\n\n    F test to compare two variances\n\ndata:  Sepal.Width by Species\nF = 0.94678, num df = 49, denom df = 49, p-value = 0.849\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.5372773 1.6684117\nsample estimates:\nratio of variances \n         0.9467839 \n\n\n Method 2: Levene Test  A more flexible test of homogeneity of variance is the Levene Test. It can be used to compare the variance of many populations (not just 2) and is more flexible than the F-test, so it can be used even if the normality assumption is violated.  this is the most commonly used test for homogeneity of variance  leveneTest() is in the car package in R! \nN0: Variances of all populationos are equal  p&lt;0.05 allows us to reject H0\n\nl1&lt;- leveneTest(Petal.Length ~ Species, data=iris2)\nl1 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  1.0674 0.3041\n      98               \n\nl2&lt;- leveneTest(Petal.Width ~ Species, data=iris2)\nl2 # p&lt;0.05, so we reject H0 (variances are likely different)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)  \ngroup  1  6.5455 0.01205 *\n      98                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nl3&lt;- leveneTest(Sepal.Length ~ Species, data=iris2)\nl3 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  1.0245 0.3139\n      98               \n\nl4&lt;- leveneTest(Sepal.Width ~ Species, data=iris2)\nl4 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  0.0873 0.7683\n      98               \n\n\n Method 3: Visualization  Since p-values are more like guidelines, we also want to visualize our data to assess homogeniety of variance. We can do that in several ways. You might already have some ideas about this! In general, it seems smart to display the raw data as points and as boxplots. Let’s start there!\n\nv1.1&lt;-ggplot(data=iris2, aes(x=Species, y=Petal.Length, color=Species))+\n  geom_point()+\n  theme_classic()+\n  scale_color_aaas()\n\nv1.2&lt;-ggplot(data=iris2, aes(x=Species, y=Petal.Length, color=Species))+\n  geom_boxplot()+\n  theme_classic()+\n  scale_color_aaas()\n\nv1.1+v1.2\n\n\n\n\n\nv2.1&lt;-ggplot(data=iris2, aes(x=Species, y=Petal.Width, color=Species))+\n  geom_point()+\n  theme_classic()+\n  scale_color_aaas()\n\nv2.2&lt;-ggplot(data=iris2, aes(x=Species, y=Petal.Width, color=Species))+\n  geom_boxplot()+\n  theme_classic()+\n  scale_color_aaas()\n\nv2.1+v2.2\n\n\n\n\n\nv3.1&lt;-ggplot(data=iris2, aes(x=Species, y=Sepal.Width, color=Species))+\n  geom_point()+\n  theme_classic()+\n  scale_color_aaas()\n\nv3.2&lt;-ggplot(data=iris2, aes(x=Species, y=Sepal.Width, color=Species))+\n  geom_boxplot()+\n  theme_classic()+\n  scale_color_aaas()\n\nv3.1+v3.2\n\n\n\n\n\nv4.1&lt;-ggplot(data=iris2, aes(x=Species, y=Sepal.Length, color=Species))+\n  geom_point()+\n  theme_classic()+\n  scale_color_aaas()\n\nv4.2&lt;-ggplot(data=iris2, aes(x=Species, y=Sepal.Length, color=Species))+\n  geom_boxplot()+\n  theme_classic()+\n  scale_color_aaas()\n\nv4.1+v4.2"
  },
  {
    "objectID": "images/Lab 4_ T-tests.html#when-can-we-ignore-assumptions",
    "href": "images/Lab 4_ T-tests.html#when-can-we-ignore-assumptions",
    "title": "Lab 4: T-tests",
    "section": "When can we ignore assumptions?",
    "text": "When can we ignore assumptions?\nWe can if our sample sizes are large. If n is small, we should not ignore this assumption. There are alternatives to dealing with normality that we can discuss in the ANOVA section (such as transforming the data)\nFor more info on that\nWe can also ignore the equal variance requirement if we use the Welch t-test (default in R)"
  },
  {
    "objectID": "images/Lab 4_ T-tests.html#a-basic-t-test-in-r",
    "href": "images/Lab 4_ T-tests.html#a-basic-t-test-in-r",
    "title": "Lab 4: T-tests",
    "section": "A basic T-test in R",
    "text": "A basic T-test in R\nFinally, let’s do some T-tests! \nH0: No difference between the means of the 2 populations p&lt;0.05 allows us to reject this H0 (indicating a likely difference)\nStep 1: Calculate means and error and plot!\n\nmeaniris&lt;-iris2 %&gt;%\n  group_by(Species) %&gt;%\n  summarize(meanpl=mean(Petal.Length), sdpl=sd(Petal.Length), n=n(), sepl=sdpl/sqrt(n), meanpw=mean(Petal.Width), sdpw=sd(Petal.Width), n=n(), sepw=sdpw/sqrt(n), meansl=mean(Sepal.Length), sdsl=sd(Sepal.Length), n=n(), sesl=sdpl/sqrt(n), meansw=mean(Sepal.Width), sdsw=sd(Sepal.Width), n=n(), sesw=sdsw/sqrt(n))\n\nmeaniris\n\n# A tibble: 2 × 14\n  Species    meanpl  sdpl     n   sepl meanpw  sdpw   sepw meansl  sdsl   sesl\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 versicolor   4.26 0.470    50 0.0665   1.33 0.198 0.0280   5.94 0.516 0.0665\n2 virginica    5.55 0.552    50 0.0780   2.03 0.275 0.0388   6.59 0.636 0.0780\n# ℹ 3 more variables: meansw &lt;dbl&gt;, sdsw &lt;dbl&gt;, sesw &lt;dbl&gt;\n\n\n\n\np1&lt;-ggplot(meaniris, aes(x=Species, y=meanpl, color=Species))+\n  geom_point()+\n  geom_errorbar(aes(x=Species, ymin=meanpl-sepl, ymax=meanpl+sepl), width=0.2)+\n  scale_color_aaas()+\n  theme_classic()+\n  labs(title='Petal Length')\n\np2&lt;-ggplot(meaniris, aes(x=Species, y=meanpw, color=Species))+\n  geom_point()+\n  geom_errorbar(aes(x=Species, ymin=meanpw-sepw, ymax=meanpw+sepw), width=0.2)+\n  scale_color_aaas()+\n  theme_classic()+\n  labs(title='Petal Width')\n\np3&lt;-ggplot(meaniris, aes(x=Species, y=meansl, color=Species))+\n  geom_point()+\n  geom_errorbar(aes(x=Species, ymin=meansl-sesl, ymax=meansl+sesl), width=0.2)+\n  scale_color_aaas()+\n  theme_classic()+\n  labs(title='Sepal Length')\n\np4&lt;-ggplot(meaniris, aes(x=Species, y=meansw, color=Species))+\n  geom_point()+\n  geom_errorbar(aes(x=Species, ymin=meansw-sesw, ymax=meansw+sesw), width=0.2)+\n  scale_color_aaas()+\n  theme_classic()+\n  labs(title='Sepal Width')\n\n(p1+p2)/(p3+p4)\n\n\n\n\nDoes Petal Length differ by species?\n\nt1&lt;-t.test(data=iris2, Petal.Length~Species, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. BUt, we can also change them (as I will show later)\n\nt1 #p&lt;0.05 suggests that there is a significant difference in petal length between species\n\n\n    Welch Two Sample t-test\n\ndata:  Petal.Length by Species\nt = -12.604, df = 95.57, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -1.49549 -1.08851\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   4.260                    5.552 \n\n\n Our p&lt;0.05 suggests that there is a significant effect of species on petal length (petal length differs by species). BUT, do we get a clear explanation of which group is higher or lower? Look at the Welch T-test output and you can see the means! You can also use the graph we made to visualize this!\nDoes Petal Width differ by species?\n\nt2&lt;-t.test(data=iris2, Petal.Width~Species, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. BUt, we can also change them (as I will show later)\n\nt2\n\n\n    Welch Two Sample t-test\n\ndata:  Petal.Width by Species\nt = -14.625, df = 89.043, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -0.7951002 -0.6048998\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   1.326                    2.026 \n\n\n Does Sepal Width differ between species?\n\nt3&lt;-t.test(data=iris2, Sepal.Width~Species, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. BUt, we can also change them (as I will show later)\n\nt3\n\n\n    Welch Two Sample t-test\n\ndata:  Sepal.Width by Species\nt = -3.2058, df = 97.927, p-value = 0.001819\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -0.33028364 -0.07771636\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   2.770                    2.974 \n\n\n Does Sepal Length differ between species?\n\nt4&lt;-t.test(data=iris2, Sepal.Length~Species, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. BUt, we can also change them (as I will show later)\n\nt4\n\n\n    Welch Two Sample t-test\n\ndata:  Sepal.Length by Species\nt = -5.6292, df = 94.025, p-value = 1.866e-07\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -0.8819731 -0.4220269\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   5.936                    6.588 \n\n\nSO, when is a t-test actually useful and when isn’t it? We use a T-test ONLY when we want to compare two means / two populations. If we have more than 2 groups, a T-test is not appropriate! Instead, we need to use an analysis of variance (ANOVA) or possibly something more complex!"
  },
  {
    "objectID": "getting_started.html#install-r-and-rstudio.-get-rstudio-running-on-your-computer-and-get-familiar-with-the-layout.",
    "href": "getting_started.html#install-r-and-rstudio.-get-rstudio-running-on-your-computer-and-get-familiar-with-the-layout.",
    "title": "Lab 0: Getting Started",
    "section": "1. Install R and Rstudio. Get Rstudio running on your computer and get familiar with the layout.",
    "text": "1. Install R and Rstudio. Get Rstudio running on your computer and get familiar with the layout."
  },
  {
    "objectID": "getting_started.html#make-a-folder-directory-on-your-computer-for-this-course-and-then-make-an-rstudio-project-for-this-course-that-runs-from-that-folder",
    "href": "getting_started.html#make-a-folder-directory-on-your-computer-for-this-course-and-then-make-an-rstudio-project-for-this-course-that-runs-from-that-folder",
    "title": "Lab 0: Getting Started",
    "section": "2. Make a folder (directory) on your computer for this course and then make an Rstudio project for this course that runs from that folder",
    "text": "2. Make a folder (directory) on your computer for this course and then make an Rstudio project for this course that runs from that folder"
  },
  {
    "objectID": "getting_started.html#make-a-new-quarto-document-this-is-where-you-will-do-the-lab-assignment-that-you-will-turn-in.-make-a-title-and-subject-headers-for-each-question.-copy-the-instructions-and-then-add-your-work-below.-make-the-question-numbers-bold.-for-numberr-1-install-rstudio--write-me-a-short-description-of-what-you-did-i.e.-i-installed-r-and-rstudio-following-this-tutorial-where-this-is-a-hyperlink-to-the-tutorial-you-used-this-lab-for-example.-for-number-2-set-your-folder-for-this-class-as-your-working-directory.-navigate-to-it-in-the-files-tab-on-rstudio.-take-a-screenshot-and-insert-that-screenshot-as-an-image-into-your-quarto-doc.",
    "href": "getting_started.html#make-a-new-quarto-document-this-is-where-you-will-do-the-lab-assignment-that-you-will-turn-in.-make-a-title-and-subject-headers-for-each-question.-copy-the-instructions-and-then-add-your-work-below.-make-the-question-numbers-bold.-for-numberr-1-install-rstudio--write-me-a-short-description-of-what-you-did-i.e.-i-installed-r-and-rstudio-following-this-tutorial-where-this-is-a-hyperlink-to-the-tutorial-you-used-this-lab-for-example.-for-number-2-set-your-folder-for-this-class-as-your-working-directory.-navigate-to-it-in-the-files-tab-on-rstudio.-take-a-screenshot-and-insert-that-screenshot-as-an-image-into-your-quarto-doc.",
    "title": "Lab 0: Getting Started",
    "section": "3. Make a new quarto document, this is where you will do the lab assignment that you will turn in. Make a title and subject headers for each question. Copy the instructions and then add your work below. Make the question numbers bold. For numberr 1 (install Rstudio)- write me a short description of what you did (i.e.: “I installed R and Rstudio following this tutorial”), where “this” is a hyperlink to the tutorial you used (this lab, for example). For number 2: Set your folder for this class as your working directory. Navigate to it in the “files” tab on Rstudio. Take a screenshot and insert that screenshot as an image into your quarto doc.",
    "text": "3. Make a new quarto document, this is where you will do the lab assignment that you will turn in. Make a title and subject headers for each question. Copy the instructions and then add your work below. Make the question numbers bold. For numberr 1 (install Rstudio)- write me a short description of what you did (i.e.: “I installed R and Rstudio following this tutorial”), where “this” is a hyperlink to the tutorial you used (this lab, for example). For number 2: Set your folder for this class as your working directory. Navigate to it in the “files” tab on Rstudio. Take a screenshot and insert that screenshot as an image into your quarto doc."
  },
  {
    "objectID": "getting_started.html#install-the-following-packages-tidyverse-lubridate-performance-palmerpenguins-patchwork-ggsci.-once-installed-load-them-using-library-in-a-code-chunk-in-your-quarto-doc.-remember-that-we-generally-want-to-load-packages-and-any-data-at-the-top-of-our-quarto-doc-but-for-this-assignment-we-will-just-do-it-in-section-4.",
    "href": "getting_started.html#install-the-following-packages-tidyverse-lubridate-performance-palmerpenguins-patchwork-ggsci.-once-installed-load-them-using-library-in-a-code-chunk-in-your-quarto-doc.-remember-that-we-generally-want-to-load-packages-and-any-data-at-the-top-of-our-quarto-doc-but-for-this-assignment-we-will-just-do-it-in-section-4.",
    "title": "Lab 0: Getting Started",
    "section": "4. Install the following packages: Tidyverse, lubridate, performance, palmerpenguins, patchwork, ggsci. Once installed, load them (using library()) in a code chunk in your quarto doc. remember that we generally want to load packages and any data at the top of our quarto doc, but for this assignment we will just do it in section 4.",
    "text": "4. Install the following packages: Tidyverse, lubridate, performance, palmerpenguins, patchwork, ggsci. Once installed, load them (using library()) in a code chunk in your quarto doc. remember that we generally want to load packages and any data at the top of our quarto doc, but for this assignment we will just do it in section 4."
  },
  {
    "objectID": "getting_started.html#read-in-these-data-and-take-a-look-at-the-shape-of-the-data-using-head-tail-str",
    "href": "getting_started.html#read-in-these-data-and-take-a-look-at-the-shape-of-the-data-using-head-tail-str",
    "title": "Lab 0: Getting Started",
    "section": "5. Read in THESE data and take a look at the shape of the data using head, tail, str,",
    "text": "5. Read in THESE data and take a look at the shape of the data using head, tail, str,"
  },
  {
    "objectID": "Lab_1.html",
    "href": "Lab_1.html",
    "title": "Lab 1: Intro to Data Wrangling",
    "section": "",
    "text": "Learning Objectives\nIn this tutorial we will learn:\n1.) Basic data wrangling functions in the tidyverse framework\n2.) Pivoting data\n3.) How to deal with date / time formats in R\n\n\n\n1.) Introduction to the Tidyverse\nThe Tidyverse is a collection of R packages that can be used together for many different data science practices. They share syntax and are very versatile. For most users, the Tidyverse provides a structure of “best practices” that will allow a user to do just about anything with data.\nWe can load the Tidyverse as a single package in R:\n\nlibrary(tidyverse)\n\nThe tidyverse package contains the following packages:\n1.) ggplot2: the best graphing package in R\n2.) dplyr: most of our data wrangling tools come from here\n3.) tidyr: tools for data tidying (cleaning, reshaping)\n4.) readr: tools for reading in different types of data – this is where the read_csv() function comes from\n5.) purrr: tools for working with functions and vectors (useful but likely not right away for beginners)\n6.) stringr: functions to help us work with strings (like sentences, paragraphs, lists, etc)\n7.) forcats: “for categories” - makes working with factors (categorical data) easier!\nLearn more about the Tidyverse\nThis section contains some worked examples of Tidyverse best practices for data manipulation. If you just want a quick refresher, you can take a look at the cheat sheet below!\n\n\n\n\n2.) Prepare data for wrangling\n\nRead in some dataload the data\n\n\nWe can mess with a few data sets that are built into R or into R packages.\nA common one is mtcars, which is part of base R (attributes of a bunch of cars)\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nAnother fun one is CO2, which is also part of base R (CO2 uptake from different plants). Note: co2 (no caps) is also a dataset in R. It’s just the CO2 concentration at Maona Loa observatory every year (as a list).\n\nhead(CO2)\n\n  Plant   Type  Treatment conc uptake\n1   Qn1 Quebec nonchilled   95   16.0\n2   Qn1 Quebec nonchilled  175   30.4\n3   Qn1 Quebec nonchilled  250   34.8\n4   Qn1 Quebec nonchilled  350   37.2\n5   Qn1 Quebec nonchilled  500   35.3\n6   Qn1 Quebec nonchilled  675   39.2\n\n\nYou are welcome to use these to practice with or you can choose from any of the datasets in the ‘datasets’ or ‘MASS’ packages (you have to load the package to get the datasets).\nYou can also load in your own data or pick something from online, as we learned how to do last time.\nLet’s stick with what we know for now– I will use the penguins data from the palmerpenguins package\n\n\n\nlibrary(palmerpenguins)\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nadd the dataframe to our environment As you learned in the Rstudio basics tutorial above, one of the four main panels of the RStudio window contains the Environment tab. In this tab, we can see data that are stored locally in our session of R. While penguins is pre-loaded in R, it is nice to make a local copy so we can modify it easily.\nHere’s how we do that:\n\npenguins&lt;-penguins \n\nHere, the name of the new dataframe we want in our environment is to the left of the arrow and the name of the object we are calling is to the right. In simpler terms, we are defining a new dataframe called penguins (or any name we want) and it is defined as just an exact copy of penguins (the object that is already defined within palmerpenguins. This is the simplest example – we will quickly move on to more complex things. You will see that when you run this the dataframe ‘penguins’ appears in the local environment. You can call your local file anything you want, it does not need to be an exact copy of the original name! Choose names that are meaningful to you, but keep the names short and avoid spaces and other special characters as much as possible.\n\n\n\n\n\n3.) Tidyverse data wrangling\n\nSelect or remove columns/rowsSubsetting and filtering dataAdd new columns or change existing onesPivot data (wide to long / long to wide)\n\n\nLet’s look at penguins\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nNow let’s say we only really care about species and bill length. We can select those columns to keep and remove the rest of the columns because they are just clutter at this point. There are two ways we can do this: 1.) Select the columns we want to keep 2.) Select the columns we want to remove\nHere are two ways to do that:\nBase R example For those with some coding experience you may like this method as this syntax is common in other coding languages\nStep 1.) Count the column numbers. Column 1 is the left most column. Remember we can use ncol() to count the total number of columns (useful when we have a huge number of columns)\n\nncol(penguins) # we have 8 columns\n\n[1] 8\n\n\nSpecies is column 1 and bill length is column 3. Those are the only columns we want!\nStep 2.) Select columns we want to keep using bracket syntax. Here we will use this basic syntax: df[rows, columns] We can input the rows and/or columns we want inside our brackets. If we want more than 1 row or column we will need to use a ‘c()’ for concatenate (combine). To select just species and bill length we would do the following:\n\nhead(penguins[,c(1,3)]) #Selecting NO specific rows and 2 columns (numbers 1 and 3)\n\n# A tibble: 6 × 2\n  species bill_length_mm\n  &lt;fct&gt;            &lt;dbl&gt;\n1 Adelie            39.1\n2 Adelie            39.5\n3 Adelie            40.3\n4 Adelie            NA  \n5 Adelie            36.7\n6 Adelie            39.3\n\n\nIMPORTANT When we do this kind of manipulation it is super helpful to NAME the output. In the above example I didn’t do that. If I don’t name the output I cannot easily call it later. If I do name it, I can use it later and see it in my ‘Environment’ tab. So, I should do this:\n\npens&lt;-penguins[,c(1,3)]\nhead(pens)\n\n# A tibble: 6 × 2\n  species bill_length_mm\n  &lt;fct&gt;            &lt;dbl&gt;\n1 Adelie            39.1\n2 Adelie            39.5\n3 Adelie            40.3\n4 Adelie            NA  \n5 Adelie            36.7\n6 Adelie            39.3\n\n\nNow, here’s how you do the same selection step by removing the columns you DO NOT want.\n\npens2&lt;-penguins[,-c(2,4:8)] #NOTE that ':' is just shorthand for all columns between 4 and 8. I could also use -c(2,4,5,6,7,8)\nhead(pens2)\n\n# A tibble: 6 × 2\n  species bill_length_mm\n  &lt;fct&gt;            &lt;dbl&gt;\n1 Adelie            39.1\n2 Adelie            39.5\n3 Adelie            40.3\n4 Adelie            NA  \n5 Adelie            36.7\n6 Adelie            39.3\n\n\n\nTidyverse example (select())\nPerhaps that example above was a little confusing? This is why we like Tidyverse! We can do the same thing using the select() function in Tidyverse and it is easier!\nI still want just species and bill length. Here’s how I select them:\n\nhead(select(penguins, species, bill_length_mm))\n\n# A tibble: 6 × 2\n  species bill_length_mm\n  &lt;fct&gt;            &lt;dbl&gt;\n1 Adelie            39.1\n2 Adelie            39.5\n3 Adelie            40.3\n4 Adelie            NA  \n5 Adelie            36.7\n6 Adelie            39.3\n\n\nEASY. Don’t forget to name the output for use later :)\nLike this:\n\nshortpen&lt;-select(penguins, species, bill_length_mm)\nhead(shortpen)\n\n# A tibble: 6 × 2\n  species bill_length_mm\n  &lt;fct&gt;            &lt;dbl&gt;\n1 Adelie            39.1\n2 Adelie            39.5\n3 Adelie            40.3\n4 Adelie            NA  \n5 Adelie            36.7\n6 Adelie            39.3\n\n\n\n\n\nSometimes we only want to look at data from a subset of the data frame\n\nFor example, maybe we only want to examine data from chinstrap penguins in the penguins data. OR perhaps we only care about 4 cylinder cars in mtcars. We can filter out the data we don’t want easily using Tidyverse (filter) or base R (subset)\nTidyverse example - Using filter()\nLet’s go ahead and filter the penguins data to only include chinstraps and the mtcars data to only include 4 cylinder cars\nThe syntax for filter is: filter(df, column =&gt;&lt;== number or factor)\n\n#filter penguins to only contain chinstrap\nchins&lt;-filter(penguins, species=='Chinstrap')\nhead(chins)\n\n# A tibble: 6 × 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Chinstrap Dream            46.5          17.9               192        3500\n2 Chinstrap Dream            50            19.5               196        3900\n3 Chinstrap Dream            51.3          19.2               193        3650\n4 Chinstrap Dream            45.4          18.7               188        3525\n5 Chinstrap Dream            52.7          19.8               197        3725\n6 Chinstrap Dream            45.2          17.8               198        3950\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n#confirm that we only have chinstraps\nchins$species\n\n [1] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n [8] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[15] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[22] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[29] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[36] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[43] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[50] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[57] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[64] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\nLevels: Adelie Chinstrap Gentoo\n\n\nNow for mtcars…\n\n#filter mtcars to only contain 4 cylinder cars\ncars4cyl&lt;-filter(mtcars, cyl == \"4\")\nhead(cars4cyl)\n\n                mpg cyl  disp hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0 93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7 66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7 52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1 65 4.22 1.835 19.90  1  1    4    1\n\n#confirm it worked\nstr(cars4cyl) #str shows us the observations and variables in each column\n\n'data.frame':   11 obs. of  11 variables:\n $ mpg : num  22.8 24.4 22.8 32.4 30.4 33.9 21.5 27.3 26 30.4 ...\n $ cyl : num  4 4 4 4 4 4 4 4 4 4 ...\n $ disp: num  108 146.7 140.8 78.7 75.7 ...\n $ hp  : num  93 62 95 66 52 65 97 66 91 113 ...\n $ drat: num  3.85 3.69 3.92 4.08 4.93 4.22 3.7 4.08 4.43 3.77 ...\n $ wt  : num  2.32 3.19 3.15 2.2 1.61 ...\n $ qsec: num  18.6 20 22.9 19.5 18.5 ...\n $ vs  : num  1 1 1 1 1 1 1 1 0 1 ...\n $ am  : num  1 0 0 1 1 1 0 1 1 1 ...\n $ gear: num  4 4 4 4 4 4 3 4 5 5 ...\n $ carb: num  1 2 2 1 2 1 1 1 2 2 ...\n\ncars4cyl$cyl #shows us only the observations in the cyl column!\n\n [1] 4 4 4 4 4 4 4 4 4 4 4\n\n\nBase R example (subset) In this case, the subset() function that is in base R works almost exactly like the filter() function. You can essentially use them interchangeably.\n\n#subset mtcars to include only 4 cylinder cars\ncars4cyl2.0&lt;-subset(mtcars, cyl=='4')\ncars4cyl2.0\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\n\n\nAdding a new column Sometimes we may want to do some math on a column (or a series of columns). Maybe we want to calculate a ratio, volume, or area. Maybe we just want to scale a variable by taking the log or changing it from cm to mm. We can do all of this with the mutate() function in Tidyverse!\n\n#convert bill length to cm (and make a new column)\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\nmutpen&lt;-(mutate(penguins, bill_length_cm=bill_length_mm/10))\nhead(mutpen)         \n\n# A tibble: 6 × 9\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_length_cm &lt;dbl&gt;\n\n\nChange existing column The code above makes a new column in which bill length in cm is added as a new column to the data frame. We could have also just done the math in the original column if we wanted. That would look like this:\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\nmutpen&lt;-(mutate(penguins, bill_length_mm=bill_length_mm/10))\nhead(mutpen) \n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           3.91          18.7               181        3750\n2 Adelie  Torgersen           3.95          17.4               186        3800\n3 Adelie  Torgersen           4.03          18                 195        3250\n4 Adelie  Torgersen          NA             NA                  NA          NA\n5 Adelie  Torgersen           3.67          19.3               193        3450\n6 Adelie  Torgersen           3.93          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nNOTE This is misleading because now the values in bill_length_mm are in cm. Thus, it was better to just make a new column in this case. But you don’t have to make a new column every time if you would prefer not to. Just be careful.\nColumn math in Base R Column manipulation is easy enough in base R as well. We can do the same thing we did above without Tidyverse like this:\n\npenguins$bill_length_cm = penguins$bill_length_mm /10\nhead(penguins)\n\n# A tibble: 6 × 9\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_length_cm &lt;dbl&gt;\n\n\n\n\n‘Pivoting’ data means changing the format of the data. Tidyverse and ggplot in particular tend to like data in ‘long’ format. Long format means few columns and many rows. Wide format is the opposite- many columns and fewer rows.\nWide format is usually how the human brain organizes data. For example, a spreadsheet in which every species is in its own column is wide format. You might take this sheet to the field and record present/absence or count of each species at each site or something. This is great but it might be easier for us to calculate averages or do group based analysis in R if we have a column called ‘species’ in which every single species observation is a row. This leads to A LOT of repeated categorical variables (site, date, etc), which is fine.\nExample of Long Format The built in dataset ‘fish_encounters’ is a simple example of long format data. Penguins, iris, and others are also in long format but are more complex\n\nhead(fish_encounters) # here we see 3 columns that track each fish (column 1) across MANY stations (column 2) \n\n# A tibble: 6 × 3\n  fish  station  seen\n  &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n1 4842  Release     1\n2 4842  I80_1       1\n3 4842  Lisbon      1\n4 4842  Rstr        1\n5 4842  Base_TD     1\n6 4842  BCE         1\n\n\nConverting from long to wide using pivot_wider (Tidyverse) Although we know that long format is preferred for working in Tidyverse and doing graphing and data analysis in R, we sometimes do want data to be in wide format. There are certain functions and operations that may require wide format. This is also the format that we are most likely to use in the field. So, let’s convert fish_encounters back to what it likely was when the data were recorded in the field…\n\n#penguins long to wide using pivot_wider\n\nwidefish&lt;-fish_encounters %&gt;%\n  pivot_wider(names_from= station, values_from = seen)\n\nhead(widefish)\n\n# A tibble: 6 × 12\n  fish  Release I80_1 Lisbon  Rstr Base_TD   BCE   BCW  BCE2  BCW2   MAE   MAW\n  &lt;fct&gt;   &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 4842        1     1      1     1       1     1     1     1     1     1     1\n2 4843        1     1      1     1       1     1     1     1     1     1     1\n3 4844        1     1      1     1       1     1     1     1     1     1     1\n4 4845        1     1      1     1       1    NA    NA    NA    NA    NA    NA\n5 4847        1     1      1    NA      NA    NA    NA    NA    NA    NA    NA\n6 4848        1     1      1     1      NA    NA    NA    NA    NA    NA    NA\n\n\nThe resulting data frame above is a wide version of the original in which each station now has its own column. This is likely how we would record the data in the field!\nExample of Wide Format Data Let’s just use widefish for this since we just made it into wide format :)\n\nhead(widefish)\n\n# A tibble: 6 × 12\n  fish  Release I80_1 Lisbon  Rstr Base_TD   BCE   BCW  BCE2  BCW2   MAE   MAW\n  &lt;fct&gt;   &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 4842        1     1      1     1       1     1     1     1     1     1     1\n2 4843        1     1      1     1       1     1     1     1     1     1     1\n3 4844        1     1      1     1       1     1     1     1     1     1     1\n4 4845        1     1      1     1       1    NA    NA    NA    NA    NA    NA\n5 4847        1     1      1    NA      NA    NA    NA    NA    NA    NA    NA\n6 4848        1     1      1     1      NA    NA    NA    NA    NA    NA    NA\n\n\nConverting from Wide to Long using pivot_longer (Tidyverse)\n\nlongfish&lt;- widefish %&gt;%\n  pivot_longer(!fish, names_to = 'station', values_to = 'seen')\n\nhead(longfish)\n\n# A tibble: 6 × 3\n  fish  station  seen\n  &lt;fct&gt; &lt;chr&gt;   &lt;int&gt;\n1 4842  Release     1\n2 4842  I80_1       1\n3 4842  Lisbon      1\n4 4842  Rstr        1\n5 4842  Base_TD     1\n6 4842  BCE         1\n\n\nAnd now we are back to our original data frame! The ‘!fish’ means simply that we do not wish to pivot the fish column. It remains unchanged. A ‘!’ before something in code usually means to exclude or remove. We’ve used names_to and values_to to give names to our new columns. pivot_longer will look for factors and put those in the names_to column and it will look for values (numeric) to put in the values_to column.\nNOTES There are MANY other ways to modify pivot_wider() and pivot_longer(). I encourage you to look in the help tab, the tidyR/ Tidyverse documentation online, and for other examples on google and stack overflow.\n\n\n\n\n\n\n4.) Dealing with Date and Time in R\nDate and time are often important variables in scientific data analysis. We are often interested in change over time and we also often do time series sampling. Learning how to manage dates and times in R is essential! Luckily, there is a user friendly and tidyverse friendly package that can help us with dates, times, and datetimes. That package is called ‘lubridate’ and we will learn all about it below.\nFirst, we need to load packages (**NOTE: It is BEST to load all packages that you need for an entire script or .qmd at the top of the document). Here, we just need to add the lubridate package. Keep in mind that you may need to install it first if you have not yet done so.\n\nlibrary(lubridate)\n\n\nDate and Time in RRead in some data to practice withChange date column (factor) to date/time formatWhy this matters\n\n\nR and really all programming languages have a difficult time with dates and times. Luckily, programmers have developed ways to get computer to understand dates and times as time series (so we can plot them on a graph axis and do analysis, for example).\nThere are several common formats of date and time that we don’t need to get into, but for many tools we use in the field we have a timestamp that includes day, month, year, and time (hours, minutes, and maybe seconds). When all of that info ends up in 1 column of a .csv it can be annoying and difficult to get R to understand what that column means. There are tons of ways to solve this problem but the easiest is definitely to just use some simple functions in the Lubridate package!\n\n\n\ndat&lt;-read.csv('https://raw.githubusercontent.com/jbaumann3/Intro-to-R-for-Ecology/main/final_bucket_mesocosm_apex_data.csv')\nhead(dat) #take a look at the data to see how it is formatted\n\n  X                date probe_name probe_type value\n1 1 07/01/2021 00:00:00      B2_T2       Temp 18.10\n2 2 07/01/2021 00:00:00     B2_pH2         pH  4.53\n3 3 07/01/2021 00:00:00     B1_pH2         pH  8.12\n4 4 07/01/2021 00:00:00      B1_T2       Temp 17.70\n5 5 07/01/2021 00:00:00      B1_T1       Temp 17.70\n6 6 07/01/2021 00:00:00     B1_pH1         pH  8.12\n\nstr(dat) #what are the attributes of each column (NOTE the attributes of the date column -- it is a factor and we want it to be a date/time0)\n\n'data.frame':   47200 obs. of  5 variables:\n $ X         : int  1 2 3 4 5 6 7 8 9 10 ...\n $ date      : chr  \"07/01/2021 00:00:00\" \"07/01/2021 00:00:00\" \"07/01/2021 00:00:00\" \"07/01/2021 00:00:00\" ...\n $ probe_name: chr  \"B2_T2\" \"B2_pH2\" \"B1_pH2\" \"B1_T2\" ...\n $ probe_type: chr  \"Temp\" \"pH\" \"pH\" \"Temp\" ...\n $ value     : num  18.1 4.53 8.12 17.7 17.7 8.12 19.7 7.99 18.1 4.53 ...\n\n\n\n\n\nTo do this we just need to recognize the order of or date/time. For example, we might have year, month, day, hours, minutes OR day, month, year, hours, minutes in order from left to right.\nIn this case we have: 07/01/2021 00:00:00 or month/day/year hours:minutes:seconds. We care about the order of these. So to simply, we have mdy_hms Lubridate has functions for all combinations of these formats. So, mdy_hms() is one. You may also have ymd_hm() or any other combo. You just enter your date info followed by an underscore and then your time info. Here’s how you apply this!\n\nstr(dat)\n\n'data.frame':   47200 obs. of  5 variables:\n $ X         : int  1 2 3 4 5 6 7 8 9 10 ...\n $ date      : chr  \"07/01/2021 00:00:00\" \"07/01/2021 00:00:00\" \"07/01/2021 00:00:00\" \"07/01/2021 00:00:00\" ...\n $ probe_name: chr  \"B2_T2\" \"B2_pH2\" \"B1_pH2\" \"B1_T2\" ...\n $ probe_type: chr  \"Temp\" \"pH\" \"pH\" \"Temp\" ...\n $ value     : num  18.1 4.53 8.12 17.7 17.7 8.12 19.7 7.99 18.1 4.53 ...\n\ndat$date&lt;-mdy_hms(dat$date) #converts our date column into a date/time object based on the format (order) of our date and time \n\nstr(dat)# date is no longer a factor but is now a POSIXct object, which means it is in date/time format and can be used for plots and time series!\n\n'data.frame':   47200 obs. of  5 variables:\n $ X         : int  1 2 3 4 5 6 7 8 9 10 ...\n $ date      : POSIXct, format: \"2021-07-01 00:00:00\" \"2021-07-01 00:00:00\" ...\n $ probe_name: chr  \"B2_T2\" \"B2_pH2\" \"B1_pH2\" \"B1_T2\" ...\n $ probe_type: chr  \"Temp\" \"pH\" \"pH\" \"Temp\" ...\n $ value     : num  18.1 4.53 8.12 17.7 17.7 8.12 19.7 7.99 18.1 4.53 ...\n\n\n\n\n\nHere we have two example graphs that show why dates are annoying and how using lubridate helps us!\nA graph using the raw data alone (not changing date to a date/time object)\n\n\n\n\n\nsame graph after making date into a date/time object\n\n\n\n\n\n\n\n\n\n\n\n5.) Lab 1 Assignment\n\nGeneral Instructions\n1.) Please label your responses with a number and organize your assignment file in a neat and easy to read fashion! You should be able to explain what every line of code does – please do include some writing in the document so I (and future you) can follow your logic and work.\n\n2.) IF you modify a data frame, make a graph, or DO anything with a line of code, you should check your work! A visual check to make sure that what you did worked and actually worked as intended is very important. When you modify a dataframe you should give the resulting dataframe a name and then have a look at it (you can use head(df) or glimpse(df) in most cases). If you make a graph, make sure it will show up below. I need to see a confirmation step for all of your work. This will also help you, so when you go back over this work you can understand what everything does.\n\n1.) Make a new data frame called ‘trees_dat’ from the data ‘trees’ that is pre-loaded in R. Note that there are 3 columns in this data frame. ‘Girth’ is the estimated diameter of the tree in inches measured at 4.5 feet off the ground. ‘Height’ is the height of the tree in feet and ‘Volume’ is the volume of the tree in feet. We will use our knowledge of geometry to see how cylindrical the trees are.\n2.) Using the ‘trees’ data, calculate the diameter and radius of the trees in feet (you will need to make new columns and use math).\n3.) Now, convert your calculated diameter to inches and compare to the ‘girth’ column. Does it match? If not, what might explain the differences?\n4.) Next, make a new data frame called ‘pens’ in your local environment from the ‘penguins’ data in the PalmerPenguins package. Subset the data to only include Adelie penguins.\n5.) Now, subset that data again so that you only have Adelie penguins from the island called ‘Dream’.\n6.) Trim the dataset so that we only have the columns ‘species’, ‘island’, and ‘bill_length_mm’.\n7.) Make a new data frame called ‘lobs’ from the ‘Loblolly’ data that is pre-loaded in R. These data show height (ft) and age (yr) of trees, identified by a numerical code (Seed).\n8.) Pivot this data wider such that every row is an age and every column is a different ‘Seed’. We should see height data across ages for each individual ‘Seed’ (tree) in each column.\n9.) Once you successful pivot the data wider, let’s pivot it back to long format. This should give us just three columns again (age, seed, and height). Note that when you pivot_longer you will need to name your new columns. See help for pivot_longer() for some examples. This should look similar or the same as our original ‘lobs’ data frame.\n10.) Render your document and turn in your .html file on Lyceum. Don’t forget embed-resources: true in your header!"
  },
  {
    "objectID": "Lab_1.html#read-in-some-data",
    "href": "Lab_1.html#read-in-some-data",
    "title": "Lab 1: Intro to Data Wrangling",
    "section": "Read in some data",
    "text": "Read in some data\nWe can mess with a few data sets that are built into R or into R packages.\nA common one is mtcars, which is part of base R (attributes of a bunch of cars)\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nAnother fun one is CO2, which is also part of base R (CO2 uptake from different plants). Note: co2 (no caps) is also a dataset in R. It’s just the CO2 concentration at Maona Loa observatory every year (as a list).\n\nhead(CO2)\n\n  Plant   Type  Treatment conc uptake\n1   Qn1 Quebec nonchilled   95   16.0\n2   Qn1 Quebec nonchilled  175   30.4\n3   Qn1 Quebec nonchilled  250   34.8\n4   Qn1 Quebec nonchilled  350   37.2\n5   Qn1 Quebec nonchilled  500   35.3\n6   Qn1 Quebec nonchilled  675   39.2\n\n\nYou are welcome to use these to practice with or you can choose from any of the datasets in the ‘datasets’ or ‘MASS’ packages (you have to load the package to get the datasets).\nYou can also load in your own data or pick something from online, as we learned how to do last time.\nLet’s stick with what we know for now– I will use the penguins data from the palmerpenguins package\nload the data\n\nlibrary(palmerpenguins)\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nadd the dataframe to our environment As you learned in the Rstudio basics tutorial above, one of the four main panels of the RStudio window contains the Environment tab. In this tab, we can see data that are stored locally in our session of R. While penguins is pre-loaded in R, it is nice to make a local copy so we can modify it easily.\nHere’s how we do that:\n\npenguins&lt;-penguins \n\nHere, the name of the new dataframe we want in our environment is to the left of the arrow and the name of the object we are calling is to the right. In simpler terms, we are defining a new dataframe called penguins (or any name we want) and it is defined as just an exact copy of penguins (the object that is already defined within palmerpenguins. This is the simplest example – we will quickly move on to more complex things. You will see that when you run this the dataframe ‘penguins’ appears in the local environment. You can call your local file anything you want, it does not need to be an exact copy of the orignal name! Choose names that are meaningful to you, but keep the names short and avoid spaces and other special characters as much as possible."
  },
  {
    "objectID": "Lab_2.html",
    "href": "Lab_2.html",
    "title": "Lab 2: Intro to graphing in ggplot",
    "section": "",
    "text": "Tutorials and Resources for graphs in ggplot\nBasics of ggplot\nColors with ggsci\nMany plots, 1 page w/ Patchwork\n\n\n1.) Load packages we need\nMaking nice looking graphs is a key feature of R and of data science in general. The best way to do this in R is through use of the ggplot2 package. This package is the most user friendly and flexible way to make nice plots in R. Notably, ggplot2 is a package that is contained within the tidyverse package, which is more of a style of R usage than a package. So, let’s load tidyverse and a few other useful packages for today.\n\n#Load packages\nlibrary(tidyverse)\nlibrary(ggsci) #for easy color scales\nlibrary(patchwork) #to make multi-panel plots \nlibrary(palmerpenguins) # our fave penguin friends :)\n\n\n\n2.) What makes a good graph vs a bad graph?\nTake a look at some graphs of data for your field of interest. You may have a look at papers you have recently read or graphs you find in textbooks or assignments. Consider what you like or don’t like about these graphs. What looks good and/or makes a graph easy to interpret? What doesn’t? Making figures is both an art and a science.\nTo learn more about what makes graphs good (or bad), read Chapter 1 of Kieran Healy’s online data visualization book –&gt; What makes figures bad?\n\nTo continue your learning, have a look at this more detailed data visualization book by Claus Wilke Fundamentals of Data Visualization\n\n\n\n3.) ggplot basics\n\nIntroductionggplot()histogramboxplotbar graphline graphscatter plotAdding error bars\n\n\nggplot2 is the preferred graphics package for most R users. It allows users to build a graph piece by piece from your own data through mapping of aesthetics. It is much easier to make pretty (publication and presentation quality) plots with ggplot2 than it is with the base plot function in R. If you prefer base plot() that is ok. You can use whatever you’d like but when we talk about graphs we will be using the language of ggplot.\nAttached here are the Tidyverse Cheat Sheets for ggplot2\n\n\n\n\n\n\nThe ggplot() function is the base of the ggplot2 package. Using it creates the space that we use to build a graph. If we run just the ggplot() function we will get a gray rectangle. This is the space (and background) of our plot!\n\nggplot()\n\n\n\n\nTo build a plot on the background, we must add to the ggplot call. First, we need to tell it what data to use. Next, we need to tell it where in the data frame to pull data from to build the axes and data points. The part of the ggplot() function we use to build a graph is called aes() or aesthetics.\nHere is an example using penguins: I am telling ggplot that the data we are using is ‘penguins’ and then defining the x and y axis in the aes() call with column names from penguins\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) \n\n\n\n\nLike anything in R, we can give our plot a name and call it later\n\nplot1&lt;-ggplot(data=penguins, aes(x=species, y= bill_length_mm)) \n\nplot1\n\n\n\n\nThis is incredibly useful in ggplot as we can essentially add pieces to make a more complete graph\n\nplot1+\n  geom_boxplot()+\n  geom_point()+\n  theme_bw()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nBefore we get too excited about making perfect graphs, let’s take a look at the types of graphs we have available to us…\n\n\nHistograms are used to explore the frequency distribution of a single variable. We can check for normality (a bell curve) using this feature. We can also look for means, skewed data, and other trends.\n\nggplot(data=penguins, aes(bill_length_mm))+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nWithin geom_histogram we can use bin_width to change the width of our x-axis groupings.\n\nggplot(data=penguins, aes(bill_length_mm))+\n  geom_histogram(binwidth=5)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\nA boxplot is a really useful plot to assess median and range of data. It can also identify outliers! The defaults for a boxplot in ggplot produce a median and interquartile range (IQR). The 1st quartile is the bottom of the box and the 3rd quartile is the top. The whiskers show the spread of the data where the ends of the whiskers represent the data points that are the furthest from the median in either direction. Notably, if a data point is 1.5 * IQR from the box (either the 1st or 3rd quartile) it is an outlier. Outliers are excluded from whiskers and are presented as points. There\nHere’s an example\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nWe can use geom_violin to combine boxplot with a density plot (similar to a histogram) Here we can see the distribution of values within bill length by species.\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  #geom_boxplot()+\n  geom_violin()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\n\n\n\n\n\nWe can make bar graphs in ggplot using geom_bar(). There are some tricks to getting bar graphs to work exactly right, which I will try to detail below. NOTE Bar graphs are very rarely useful. If we want to show means, why not just use points + error bars? What does the bar actually represent? There aren’t that many cases where we really need bar graphs. There are exceptions, like when we have a population and we want to see the demographics of that population by count or percentage (see example below)\nHere is a simple bar chart.\n\nggplot(data=penguins, aes(species)) +\n  geom_bar()\n\n\n\n\nHere is a more elaborate boxplot that shows species breakdown by island! Note that we use an aes() call within geom_bar to define a fill. That means fill by species, or add a color for each species.\n\nggplot(data=penguins, aes(island)) +\n  geom_bar(aes(fill=species))\n\n\n\n\nAnd here is that same plot with the bars unstacked. Instead of stacking, we have used “dodged” each color to be its own bar.\n\nggplot(data=penguins, aes(island)) +\n  geom_bar(aes(fill=species), position= position_dodge())\n\n\n\n\nWe learned when the best (only) times to use bar graphs are. Do you remember what those were? Are the examples above representative of that?\n\n\nA line graph can be extremely useful, especially if we are looking at time series data or rates!\nHere is an example of CO2 uptake vs concentration in plants. Each color represents a different plant. NOTE: the dataset called ‘CO2’ is built into R, so we can just use it without loading anything :)\n\nhead(CO2)\n\n  Plant   Type  Treatment conc uptake\n1   Qn1 Quebec nonchilled   95   16.0\n2   Qn1 Quebec nonchilled  175   30.4\n3   Qn1 Quebec nonchilled  250   34.8\n4   Qn1 Quebec nonchilled  350   37.2\n5   Qn1 Quebec nonchilled  500   35.3\n6   Qn1 Quebec nonchilled  675   39.2\n\nggplot(data=CO2, aes(x=conc, y= uptake, color=Plant)) +\n  geom_line()\n\n\n\n\nWe can change the aesthetics of the lines using color, linetype, size, etc. Here I am changing the linetype based on the plant species and increasing the size of ALL lines to 2. This is a good example of how aes() works. Anything within the aes() call is conditional. That means, I give it a name (such as a column or variable name) and it changes based on that column or variable. To change an aesthetic across all lines, points, etc, I just put the code outside of the aes(). As I did for size. That makes the size of ALL lines = 2.\n\nggplot(data=CO2, aes(x=conc, y= uptake, color=Plant)) +\n  geom_line(aes(linetype=Plant),size=2)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nThe scatter plot is probably the most commonly used graphical tool in ggplot. It is based on the geom_point() function\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nImportantly, we can use the data= and aes() calls within geom_point() or any other geom instead of within ggplot() if needed. Why might this be important?\n\nggplot() +\n  geom_point(data=penguins, aes(x=species, y= bill_length_mm))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nSometimes we don’t want to plot all of our points on the same vertical line. If that is the case, we can use geom_jitter()\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_jitter()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\nWe often want to present means and error in our visualizations. This can be done through the use of geom_boxplot() or through combining geom_point() with geom_errorbar()\nHere is an example of the later…\n\n#First, we need to calculate a mean bill length for our penguins by species and island\nsumpens&lt;- penguins %&gt;%\n  group_by(species, island) %&gt;%\n  na.omit() %&gt;% #removes rows with NA values (a few rows may otherwise have NA due to sampling error in the field)\n  summarize(meanbill=mean(bill_length_mm), sd=sd(bill_length_mm), n=n(), se=sd/sqrt(n))\n\nsumpens\n\n# A tibble: 5 × 6\n# Groups:   species [3]\n  species   island    meanbill    sd     n    se\n  &lt;fct&gt;     &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 Adelie    Biscoe        39.0  2.48    44 0.374\n2 Adelie    Dream         38.5  2.48    55 0.335\n3 Adelie    Torgersen     39.0  3.03    47 0.442\n4 Chinstrap Dream         48.8  3.34    68 0.405\n5 Gentoo    Biscoe        47.6  3.11   119 0.285\n\n# Now we can plot! \nggplot(data=sumpens, aes(x=species, y=meanbill, color=island))+\n  geom_point()+\n  geom_errorbar(data=sumpens, aes(x=species, ymin=meanbill-se, ymax=meanbill+se), width=0.2)\n\n\n\n\nAnd if we want to be extra fancy (and rigorous), we can plot the raw data behind the mean+error This is considered a graphical best practice as we can see the mean, error, and the true spread of the data!\n\nggplot()+\n  geom_jitter(data= penguins, aes(x=species, y=bill_length_mm, color=island), alpha=0.5, width=0.2)+ #this is the raw data\n  geom_point(data=sumpens, aes(x=species, y=meanbill, color=island), size=3)+ #this is the averages\n  geom_errorbar(data=sumpens, aes(x=species, ymin=meanbill-se, ymax=meanbill+se), width=0.1)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nAn alternative to geom_jitter, which doesn’t always work, is to use geom_point but force the points to not overlap with position_dodge. Here is an example\n\n#first we should define the distance of our position_dodge\npd&lt;-position_dodge(width=0.2)\n\nggplot(data=sumpens, aes(x=species, y=meanbill, color=island))+\n  geom_point(data= penguins, aes(x=species, y=bill_length_mm, color=island), alpha=0.2, width=0.2, position=pd)+ #raw data\n  geom_point(size=3, position=pd)+ #averages\n  geom_errorbar(aes(ymin=meanbill-se, ymax=meanbill+se), width=0.2, position=pd)\n\nWarning in geom_point(data = penguins, aes(x = species, y = bill_length_mm, :\nIgnoring unknown parameters: `width`\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nThis code will produce the same graph as above. Note that in geom_jitter we just replaced width = with position =\n\nggplot(sumpens, aes(x=species, y= meanbill, color=island))+\n  geom_jitter(data= penguins, aes(x=species, y=bill_length_mm, color=island), alpha=0.5, position=pd)+ #this is the raw data\n  geom_point(size=3,position=pd)+ #this is the averages\n  geom_errorbar(aes(ymin=meanbill-se, ymax=meanbill+se), width=0.2, position=pd)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n4.) Intermediate Aesthetics\n\ntitles and axis labelsColorsShapesFacetsMultiple plots on the same pageThemes\n\n\nTitles and axis labels are easy to add and change in ggplot! We simply add another line to our code. NOTE you can also add a subtitle, caption, or change the legend title using labs!\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot(aes(fill=species))+\n  scale_fill_aaas()+\n  theme_classic()+\n  labs(x = 'Species', y='Bill length (mm)', title='Penguin bill length by species', fill='Species')+ #here I change the x-axis and y-axis labels, add a title, and change the legend label (to capitalize the 'S' in 'species')\n  theme(text=element_text(size=18))\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\nWe can change colors conditionally or manually.\nConditional Color Change To change colors conditionally, we use color= or fill= within an aes() call.\nHere I have changed the outline color (color=) for a series of boxplots based on species\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm, color=species)) +\n  geom_boxplot()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nI can also change the fill of the boxplots\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm, fill=species)) +\n  geom_boxplot()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nManual Color Change We can also change colors manually by using one of many options within ggplot. scale_color_manual (or scale_fill_manual) is the easiest. We simply define colors we want to use by name or hexcode.\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot(aes(fill=species))+\n  scale_fill_manual(values=c('red', 'black', 'blue'))\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nHere’s a giant table of color options in ggplot  You can also make your own color palette and apply that to your figure!\n\nmypal&lt;-c('dodgerblue', 'forestgreen', 'coral') # here I've made a 3 color palette\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot(aes(fill=species))+\n  scale_fill_manual(values=mypal)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nYou can use the package RColorBrewer to make palettes as well. I’ll let you explore that one on your own!\nFinally, EASY and nice looking palettes with ggsci ggsci is a simple and neat package that allows us to use scientific journal color themes for our data (usually colorblind friendly and nice looking). we simply change our “scale_color_manual” to “scale_color_palname” where “palname” is one of many provided by ggsci. For example, we might use scale_color_aaas()\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot(aes(fill=species))+\n  scale_fill_aaas()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\nggplot gives us options to change point shape using the aesthetic option ‘shape’  We can either change shape based on a characteristic of the data (‘cyl’, for example), make all the shapes the same, or manually control shape\nBelow is a table of shape options:\n\n\n\nggplot shape options\n\n\nConditional Shape Change\n\nggplot(data=penguins, aes(x=species, y=bill_length_mm, color=island, shape=island))+ \n  geom_jitter(size=2)+\n  theme_classic()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nChange all shapes to triangles\n\nggplot(data=mtcars, aes(x=cyl, y=mpg, color=cyl))+\n  geom_point(shape=17) #Here 'shape=' is inside the settings for geom_point. Note that it is outside the aes() function, as that applied aesthetics conditionally)\n\n\n\n#example 2, same w/ different syntax\nggplot()+\n  geom_point(data=mtcars, aes(x=cyl, y=mpg, color=cyl), shape=17)\n\n\n\n\nManual shape changes\n\nggplot(data=penguins, aes(x=species, y=bill_length_mm, color=island, shape=island))+ \n  geom_jitter(size=2)+\n  theme_classic()+  \n  scale_shape_manual(values=c(2,3,4)) #scale_shape_manual allows us to choose shapes for each group (cyl in this case). c stands for concatenate, as we've seen before\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nChanging Size of points\nConditional Shape Change\n\nggplot(data=mtcars, aes(x=cyl, y=mpg, color=cyl, size=cyl))+ #note that we added 'size=' to our aes. \n  geom_point()\n\n\n\n#note the warning message that using size for a discrete variable is not best practice. \n#Instead, let's use the size to five us an idea of hp (a 3rd variable)\n\nggplot(data=mtcars, aes(x=cyl, y=mpg, color=cyl, size=hp))+ #note that we added 'size=' to our aes. \n  geom_point()\n\n\n\n\nChange size of all points (all points must be same size)\n\nggplot(data=mtcars, aes(x=cyl, y=mpg, color=cyl))+  \n  geom_point(size=5) #as w/ shape, point needs to be outside the aes() here. \n\n\n\n\n\n\nOften in science we are interested in comparing several graphs at once or looking at 3 or 4 variables at a time. This means we may want to have multi-panel graphs or multiple graphs on the same page. While it is common to produce graphs in R and combine them into “final” manuscript ready version in other programs, such as Adobe Illustrator or Inkscape (a free alternative to Illustrator), producing manuscript quality figures in R is possible! In fact, it is only getting easier, thanks to some new packages (like patchwork). Below I will show you how to make multipanel figures (aka facets) and how to put many figures on one page (using the patchwork package– the easiest of the many options for doing this).\nFacets allow us to produce multiple graph panels with one ggplot code. We can separate out a variable for easier viewing or even create a grid of graphs using multiple variables.\nfacet_wrap() allows us to make multiple panels. The panels are aligned in columns and rows. We need to use ‘~’ in our facet_wrap code. The ‘~’ essentially means “by”\n\nggplot(data=penguins, aes(x=island, y= bill_length_mm, fill=species)) +\n  geom_boxplot()+\n  facet_wrap(~island)+\n  scale_color_aaas()+\n  theme_classic()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nWe can specify the number of columns and rows we want to built the panels how we like them\n\nggplot(data=penguins, aes(x=year, y= bill_length_mm, fill=species)) +\n  geom_boxplot()+\n  facet_wrap(~island, ncol=2)+ #2 columns \n  scale_color_aaas()+\n  theme_classic()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\nggplot(data=penguins, aes(x=year, y= bill_length_mm, fill=species)) +\n  geom_boxplot()+\n  facet_wrap(~island, nrow=3)+ #3 rows\n  scale_color_aaas()+\n  theme_classic()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nWe can even use a formula for building our facets if we’d like!\n\nggplot(data=penguins, aes(x=island, y= bill_length_mm, fill=species)) +\n  geom_boxplot()+\n  facet_wrap(~species+year)+\n  scale_color_aaas()+\n  theme_classic()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\nUsing the simple and wonderful patchwork package, we can place multiple plots on the same page. To do this, we must actually name each plot. Here’s an example.\nPatchwork is super easy! Learn more here(with examples)\nFirst, let’s make some graphs and name them\n\n#First, we need to calculate a mean bill length for our penguins by species and island\nsumpens&lt;- penguins %&gt;%\n  group_by(species, island) %&gt;%\n  na.omit() %&gt;% #removes rows with NA values (a few rows may otherwise have NA due to sampling error in the field)\n  summarize(meanbill=mean(bill_length_mm), sd=sd(bill_length_mm), n=n(), se=sd/sqrt(n))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\nsumpens\n\n# A tibble: 5 × 6\n# Groups:   species [3]\n  species   island    meanbill    sd     n    se\n  &lt;fct&gt;     &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 Adelie    Biscoe        39.0  2.48    44 0.374\n2 Adelie    Dream         38.5  2.48    55 0.335\n3 Adelie    Torgersen     39.0  3.03    47 0.442\n4 Chinstrap Dream         48.8  3.34    68 0.405\n5 Gentoo    Biscoe        47.6  3.11   119 0.285\n\n# Next, we can make our graphs!\n\np1&lt;-ggplot(data=penguins, aes(bill_length_mm))+\n  geom_histogram()+\n  theme_classic()\n\n\np2&lt;-ggplot()+\n  geom_jitter(data= penguins, aes(x=species, y=bill_length_mm, color=island), alpha=0.5, width=0.2)+\n  geom_point(data=sumpens, aes(x=species, y=meanbill, color=island), size=3)+\n  geom_errorbar(data=sumpens, aes(x=species, ymin=meanbill-se, ymax=meanbill+se), width=0.1)+\n  theme_classic()+\n  scale_color_aaas()\n\np3&lt;-ggplot(data=penguins, aes(island)) +\n  geom_bar(aes(fill=species), position= position_dodge())+\n  theme_classic()+\n  scale_fill_aaas()\n\nNow let’s patchwork them together! We make a simple formula to make a patchwork. Addition puts everything in the same row. But we can use division and other symbols to organize.\n\nlibrary(patchwork)\n\np1+p2+p3\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nDivision allows us to put panels in columns\n\np1/p2/p3\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nWe can also combine addition and division (order of operations is still a thing!)\n\n(p1+p2) / p3\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nThere are other functions in patchwork that allow us to annotate plots, give them labels, move/combine legends, etc.\n\n\nThemes allow us to change the background color and most other aspects of a plot. There are a range of theme options within ggplot that will allow us to quickly make clean plots. The two that are most commonly used are theme_bw() and theme_classic()\nDefault theme (with terrible gray background)\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot(aes(fill=species))+\n  scale_fill_aaas()+\n  labs(x = 'Species', y='Bill length (mm)', title='Penguin bill length by species')\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\ntheme_bw() (removes gray background)\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot(aes(fill=species))+\n  scale_fill_aaas()+\n  labs(x = 'Species', y='Bill length (mm)', title='Penguin bill length by species')+\n  theme_bw()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\ntheme_classic() (removes gray and grid lines)\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot(aes(fill=species))+\n  scale_fill_aaas()+\n  labs(x = 'Species', y='Bill length (mm)', title='Penguin bill length by species')+\n  theme_classic()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nThe theme() function in ggplot is SUPER flexible. You can pretty much do anything with it. This is key for customizing plots. I’d encourage you to play around with this a bit. Here is a great place to learn more and see examples.\n##Some examples of using theme()\nChanging text size\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot(aes(fill=species))+\n  scale_fill_aaas()+\n  labs(x = 'Species', y='Bill length (mm)', title='Penguin bill length by species')+\n  theme(text=element_text(size=24))\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nRemove the gray background\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot(aes(fill=species))+\n  scale_fill_aaas()+\n  labs(x = 'Species', y='Bill length (mm)', title='Penguin bill length by species')+\n  theme(text=element_text(size=24), panel.background = element_rect(fill=\"white\")) #can use any color\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nTurn the X-Axis text\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot(aes(fill=species))+\n  scale_fill_aaas()+\n  labs(x = 'Species', y='Bill length (mm)', title='Penguin bill length by species')+\n  theme(text=element_text(size=24), panel.background = element_rect(fill=\"white\"), axis.text.x=element_text(angle=90, vjust=0.5, hjust=0.8)) #can adjust vertical and horizontal text positions\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n5.) Lab 2 Assignment\n\nGeneral Instructions\n1.) Please label your responses with a number and organize your assignment file in a neat and easy to read fashion! You should be able to explain what every line of code does – please do include some writing in the document so I (and future you) can follow your logic and work.\n\n2.) IF you modify a data frame, make a graph, or DO anything with a line of code, you should check your work! A visual check to make sure that what you did worked and actually worked as intended is very important. When you modify a dataframe you should give the resulting dataframe a name and then have a look at it (you can use head(df) or glimpse(df) in most cases). If you make a graph, make sure it will show up below. I need to see a confirmation step for all of your work. This will also help you, so when you go back over this work you can understand what everything does.\n\n\n1. Make a new dataframe called ‘irisdata’ from the ‘iris’ date built into R.\n\n2. Make a histogram of Sepal.Length that compares distributions for all 3 species in the same graph. Note that color= changes the color of lines and fill= changes the color of the fill!\n\n3.) Make a boxplot that shows how Sepal.Length differs by Species. Remove the gray background (there are many ways to do that– any way you want is fine).\n\n4.) Make a bar graph that shows Sepal.Length by species. Is this a good graph or no? Consider the aspects of good vs bad graphs in the tutorial.\n\n5.) Make a scatter plot that shows Sepal.Length by species. Compare this to your bar graph. Which is more useful and why?\n\n6.) Make a line graph comparing Sepal.Length and Sepal.Width by species. What do you see? This is often the kind of graph we pair with a linear regression, so thinking about what it shows us is important.\n\nALL graphs below should not have a grey background. Use a theme to remove that\n\n7.) Pick any of your above graphs. Change the colors away from default to something else. You can either make your own palette or use a scale_color_manual(). Next, do the same using the ggsci package.\n\n8.) Next, take the graph from 7 and make each species a different shape.\n\n9.) Take the graph from 8, add a title, change the axes titles, and make the text larger (I like font size 18).\n\n10.) Take the graph from 6 and facet_wrap() it by species.\n\n11.) Using the patchwork package, take any three of your graphs and panel them so that they all fit together on one page.\n\n12.) Render your quarto doc and submit your .html file on Lyceum."
  },
  {
    "objectID": "Lab_2.html#histogram",
    "href": "Lab_2.html#histogram",
    "title": "Lab 2: Intro to graphing in ggplot",
    "section": "",
    "text": "Histograms are used to explore the frequency distribution of a single variable. We can check for normality (a bell curve) using this feature. We can also look for means, skewed data, and other trends.\n\nggplot(data=penguins, aes(bill_length_mm))+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nWithin geom_histogram we can use bin_width to change the width of our x-axis groupings.\n\nggplot(data=penguins, aes(bill_length_mm))+\n  geom_histogram(binwidth=5)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`)."
  },
  {
    "objectID": "Lab_2.html#boxplot",
    "href": "Lab_2.html#boxplot",
    "title": "Lab 2: Intro to graphing in ggplot",
    "section": "",
    "text": "A boxplot is a really useful plot to assess median and range of data. It can also identify outliers! The defaults for a boxplot in ggplot produce a median and interquartile range (IQR). The 1st quartile is the bottom of the box and the 3rd quartile is the top. The whiskers show the spread of the data where the ends of the whiskers represent the data points that are the furthest from the median in either direction. Notably, if a data point is 1.5 * IQR from the box (either the 1st or 3rd quartile) it is an outlier. Outliers are excluded from whiskers and are presented as points. There\nHere’s an example\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nWe can use geom_violin to combine boxplot with a density plot (similar to a histogram) Here we can see the distribution of values within bill length by species.\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  #geom_boxplot()+\n  geom_violin()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`)."
  },
  {
    "objectID": "Lab_2.html#bar-graph",
    "href": "Lab_2.html#bar-graph",
    "title": "Lab 2: Intro to graphing in ggplot",
    "section": "",
    "text": "We can make bar graphs in ggplot using geom_bar(). There are some tricks to getting bar graphs to work exactly right, which I will try to detail below. NOTE Bar graphs are very rarely useful. If we want to show means, why not just use points + error bars? What does the bar actually represent? There aren’t that many cases where we really need bar graphs. There are exceptions, like when we have a population and we want to see the demographics of that population by count or percentage (see example below)\nHere is a simple bar chart.\n\nggplot(data=penguins, aes(species)) +\n  geom_bar()\n\n\n\n\nHere is a more elaborate boxplot that shows species breakdown by island! Note that we use an aes() call within geom_bar to define a fill. That means fill by species, or add a color for each species.\n\nggplot(data=penguins, aes(island)) +\n  geom_bar(aes(fill=species))\n\n\n\n\nAnd here is that same plot with the bars unstacked. Instead of stacking, we have used “dodged” each color to be its own bar.\n\nggplot(data=penguins, aes(island)) +\n  geom_bar(aes(fill=species), position= position_dodge())\n\n\n\n\nWe learned when the best (only) times to use bar graphs are. Do you remember what those were? Are the examples above representative of that?"
  },
  {
    "objectID": "Lab_2.html#line-graph",
    "href": "Lab_2.html#line-graph",
    "title": "Lab 2: Intro to graphing in ggplot",
    "section": "",
    "text": "A line graph can be extremely useful, especially if we are looking at time series data or rates!\nHere is an example of CO2 uptake vs concentration in plants. Each color represents a different plant. NOTE: the dataset called ‘CO2’ is built into R, so we can just use it without loading anything :)\n\nhead(CO2)\n\n  Plant   Type  Treatment conc uptake\n1   Qn1 Quebec nonchilled   95   16.0\n2   Qn1 Quebec nonchilled  175   30.4\n3   Qn1 Quebec nonchilled  250   34.8\n4   Qn1 Quebec nonchilled  350   37.2\n5   Qn1 Quebec nonchilled  500   35.3\n6   Qn1 Quebec nonchilled  675   39.2\n\nggplot(data=CO2, aes(x=conc, y= uptake, color=Plant)) +\n  geom_line()\n\n\n\n\nWe can change the aesthetics of the lines using color, linetype, size, etc. Here I am changing the linetype based on the plant species and increasing the size of ALL lines to 2. This is a good example of how aes() works. Anything within the aes() call is conditional. That means, I give it a name (such as a column or variable name) and it changes based on that column or variable. To change an aesthetic across all lines, points, etc, I just put the code outside of the aes(). As I did for size. That makes the size of ALL lines = 2.\n\nggplot(data=CO2, aes(x=conc, y= uptake, color=Plant)) +\n  geom_line(aes(linetype=Plant),size=2)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "Lab_2.html#scatter-plot",
    "href": "Lab_2.html#scatter-plot",
    "title": "Lab 2: Intro to graphing in ggplot",
    "section": "",
    "text": "The scatter plot is probably the most commonly used graphical tool in ggplot. It is based on the geom_point() function\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nImportantly, we can use the data= and aes() calls within geom_point() or any other geom instead of within ggplot() if needed. Why might this be important?\n\nggplot() +\n  geom_point(data=penguins, aes(x=species, y= bill_length_mm))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nSometimes we don’t want to plot all of our points on the same vertical line. If that is the case, we can use geom_jitter()\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_jitter()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "Lab_2.html#adding-error-bars",
    "href": "Lab_2.html#adding-error-bars",
    "title": "Lab 2: Intro to graphing in ggplot",
    "section": "",
    "text": "We often want to present means and error in our visualizations. This can be done through the use of geom_boxplot() or through combining geom_point() with geom_errorbar()\nHere is an example of the later…\n\n#First, we need to calculate a mean bill length for our penguins by species and island\nsumpens&lt;- penguins %&gt;%\n  group_by(species, island) %&gt;%\n  na.omit() %&gt;% #removes rows with NA values (a few rows may otherwise have NA due to sampling error in the field)\n  summarize(meanbill=mean(bill_length_mm), sd=sd(bill_length_mm), n=n(), se=sd/sqrt(n))\n\nsumpens\n\n# A tibble: 5 × 6\n# Groups:   species [3]\n  species   island    meanbill    sd     n    se\n  &lt;fct&gt;     &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 Adelie    Biscoe        39.0  2.48    44 0.374\n2 Adelie    Dream         38.5  2.48    55 0.335\n3 Adelie    Torgersen     39.0  3.03    47 0.442\n4 Chinstrap Dream         48.8  3.34    68 0.405\n5 Gentoo    Biscoe        47.6  3.11   119 0.285\n\n# Now we can plot! \nggplot(data=sumpens, aes(x=species, y=meanbill, color=island))+\n  geom_point()+\n  geom_errorbar(data=sumpens, aes(x=species, ymin=meanbill-se, ymax=meanbill+se), width=0.2)\n\n\n\n\nAnd if we want to be extra fancy (and rigorous), we can plot the raw data behind the mean+error This is considered a graphical best practice as we can see the mean, error, and the true spread of the data!\n\nggplot()+\n  geom_jitter(data= penguins, aes(x=species, y=bill_length_mm, color=island), alpha=0.5, width=0.2)+ #this is the raw data\n  geom_point(data=sumpens, aes(x=species, y=meanbill, color=island), size=3)+ #this is the averages\n  geom_errorbar(data=sumpens, aes(x=species, ymin=meanbill-se, ymax=meanbill+se), width=0.1)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nAn alternative to geom_jitter, which doesn’t always work, is to use geom_point but force the points to not overlap with position_dodge. Here is an example\n\n#first we should define the distance of our position_dodge\npd&lt;-position_dodge(width=0.2)\n\nggplot(data=sumpens, aes(x=species, y=meanbill, color=island))+\n  geom_point(data= penguins, aes(x=species, y=bill_length_mm, color=island), alpha=0.2, width=0.2, position=pd)+ #raw data\n  geom_point(size=3, position=pd)+ #averages\n  geom_errorbar(aes(ymin=meanbill-se, ymax=meanbill+se), width=0.2, position=pd)\n\nWarning in geom_point(data = penguins, aes(x = species, y = bill_length_mm, :\nIgnoring unknown parameters: `width`\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nThis code will produce the same graph as above. Note that in geom_jitter we just replaced width = with position =\n\nggplot(sumpens, aes(x=species, y= meanbill, color=island))+\n  geom_jitter(data= penguins, aes(x=species, y=bill_length_mm, color=island), alpha=0.5, position=pd)+ #this is the raw data\n  geom_point(size=3,position=pd)+ #this is the averages\n  geom_errorbar(aes(ymin=meanbill-se, ymax=meanbill+se), width=0.2, position=pd)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n:::\n\n\n# 4.) Intermediate Aesthetics\n\n\n:::panel-tabset\n\n\n## titles and axis labels\n\n\nTitles and axis labels are easy to add and change in ggplot! We simply add another line to our code. NOTE you can also add a subtitle, caption, or change the legend title using labs!\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(data=penguins, aes(x=species, y= bill_length_mm)) + geom_boxplot(aes(fill=species))+ scale_fill_aaas()+ theme_classic()+ labs(x = 'Species', y='Bill length (mm)', title='Penguin bill length by species', fill='Species')+ #here I change the x-axis and y-axis labels, add a title, and change the legend label (to capitalize the 'S' in 'species') theme(text=element_text(size=18))\n\n\n::: {.cell-output .cell-output-stderr}\n\n\n::: {.cell-output-display}  ::: :::\n\n\n## Colors\n\n\nWe can change colors conditionally or manually.\n\n\nConditional Color Change To change colors conditionally, we use color= or fill= within an aes() call.\n\n\nHere I have changed the outline color (color=) for a series of boxplots based on species\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(data=penguins, aes(x=species, y= bill_length_mm, color=species)) + geom_boxplot()\n\n\n::: {.cell-output .cell-output-stderr}\n\n\n::: {.cell-output-display}  ::: :::\n\n\nI can also change the fill of the boxplots\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(data=penguins, aes(x=species, y= bill_length_mm, fill=species)) + geom_boxplot()\n\n\n::: {.cell-output .cell-output-stderr}\n\n\n::: {.cell-output-display}  ::: :::\n\n\nManual Color Change We can also change colors manually by using one of many options within ggplot. scale_color_manual (or scale_fill_manual) is the easiest. We simply define colors we want to use by name or hexcode.\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(data=penguins, aes(x=species, y= bill_length_mm)) + geom_boxplot(aes(fill=species))+ scale_fill_manual(values=c('red', 'black', 'blue'))\n\n\n::: {.cell-output .cell-output-stderr}\n\n\n::: {.cell-output-display}  ::: :::\n\n\nHere’s a giant table of color options in ggplot  You can also make your own color palette and apply that to your figure!\n\n\n::: {.cell}\n\n\n```{.r .cell-code} mypal&lt;-c(‘dodgerblue’, ‘forestgreen’, ‘coral’) # here I’ve made a 3 color palette\n\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) + geom_boxplot(aes(fill=species))+ scale_fill_manual(values=mypal) ```\n\n\n::: {.cell-output .cell-output-stderr}\n\n\n::: {.cell-output-display}  ::: :::\n\n\nYou can use the package RColorBrewer to make palettes as well. I’ll let you explore that one on your own!\n\n\nFinally, EASY and nice looking palettes with ggsci ggsci is a simple and neat package that allows us to use scientific journal color themes for our data (usually colorblind friendly and nice looking). we simply change our “scale_color_manual” to “scale_color_palname” where “palname” is one of many provided by ggsci. For example, we might use scale_color_aaas()\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(data=penguins, aes(x=species, y= bill_length_mm)) + geom_boxplot(aes(fill=species))+ scale_fill_aaas()\n\n\n::: {.cell-output .cell-output-stderr}\n\n\n::: {.cell-output-display}  ::: :::\n\n\n## Shapes\n\n\nggplot gives us options to change point shape using the aesethic option ‘shape’  We can either change shape based on a characterstic of the data (‘cyl’, for example), make all the shapes the same, or manually control shape\n\n\nBelow is a table of shape options:\n\n\n\n\n\nConditional Shape Change\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(data=penguins, aes(x=species, y=bill_length_mm, color=island, shape=island))+ geom_jitter(size=2)+ theme_classic()\n\n\n::: {.cell-output .cell-output-stderr}\n\n\n::: {.cell-output-display}  ::: :::\n\n\nChange all shapes to triangles\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(data=mtcars, aes(x=cyl, y=mpg, color=cyl))+ geom_point(shape=17) #Here 'shape=' is inside the settings for geom_point. Note that it is outside the aes() function, as that applied aesethics conditionally)\n\n\n::: {.cell-output-display}  :::\n\n\n{.r .cell-code} #example 2, same w/ different syntax ggplot()+ geom_point(data=mtcars, aes(x=cyl, y=mpg, color=cyl), shape=17)\n\n\n::: {.cell-output-display}  ::: :::\n\n\nManual shape changes\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(data=penguins, aes(x=species, y=bill_length_mm, color=island, shape=island))+ geom_jitter(size=2)+ theme_classic()+ scale_shape_manual(values=c(2,3,4)) #scale_shape_manual allows us to choose shapes for each group (cyl in this case). c stands for concatenate, as we've seen before\n\n\n::: {.cell-output .cell-output-stderr}\n\n\n::: {.cell-output-display}  ::: :::\n\n\nChanging Size of points\n\n\nConditional Shape Change\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(data=mtcars, aes(x=cyl, y=mpg, color=cyl, size=cyl))+ #note that we added 'size=' to our aes. geom_point()\n\n\n::: {.cell-output-display}  :::\n\n\n```{.r .cell-code} #note the warning message that using size for a discrete variable is not best practice. #Instead, let’s use the size to five us an idea of hp (a 3rd variable)\n\n\nggplot(data=mtcars, aes(x=cyl, y=mpg, color=cyl, size=hp))+ #note that we added ‘size=’ to our aes. geom_point() ```\n\n\n::: {.cell-output-display}  ::: :::\n\n\nChange size of all points (all points must be same size)\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(data=mtcars, aes(x=cyl, y=mpg, color=cyl))+ geom_point(size=5) #as w/ shape, point needs to be outside the aes() here.\n\n\n::: {.cell-output-display}  ::: :::\n\n\n## Facets Often in science we are interested in comparing several graphs at once or looking at 3 or 4 variables at a time. This means we may want to have multi-panel graphs or multiple graphs on the same page. While it is common to produce graphs in R and combine them into “final” manuscript ready version in other programs, such as Adobe Illustrator or Inkscape (a free alternative to Illustrator), producing manuscript quality figures in R is possible! In fact, it is only getting easier, thanks to some new packages (like patchwork). Below I will show you how to make multipanel figures (aka facets) and how to put many figures on one page (using the patchwork package– the easiest of the many options for doing this).\nFacets allow us to produce multiple graph panels with one ggplot code. We can separate out a variable for easier viewing or even create a grid of graphs using multiple variables.\n\n\nfacet_wrap() allows us to make multiple panels. The panels are aligned in columns and rows. We need to use ‘~’ in our facet_wrap code. The ‘~’ essentially means “by”\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(data=penguins, aes(x=island, y= bill_length_mm, fill=species)) + geom_boxplot()+ facet_wrap(~island)+ scale_color_aaas()+ theme_classic()\n\n\n::: {.cell-output .cell-output-stderr}\n\n\n::: {.cell-output-display}  ::: :::\n\n\nWe can specify the number of columns and rows we want to built the panels how we like them\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(data=penguins, aes(x=year, y= bill_length_mm, fill=species)) + geom_boxplot()+ facet_wrap(~island, ncol=2)+ #2 columns scale_color_aaas()+ theme_classic()\n\n\n::: {.cell-output .cell-output-stderr}\n\n\n::: {.cell-output-display}  ::: :::\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(data=penguins, aes(x=year, y= bill_length_mm, fill=species)) + geom_boxplot()+ facet_wrap(~island, nrow=3)+ #3 rows scale_color_aaas()+ theme_classic()\n\n\n::: {.cell-output .cell-output-stderr}\n\n\n::: {.cell-output-display}  ::: :::\n\n\nWe can even use a formula for building our facets if we’d like!\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(data=penguins, aes(x=island, y= bill_length_mm, fill=species)) + geom_boxplot()+ facet_wrap(~species+year)+ scale_color_aaas()+ theme_classic()\n\n\n::: {.cell-output .cell-output-stderr}\n\n\n::: {.cell-output-display}  ::: :::"
  },
  {
    "objectID": "Lab_2.html#multiple-plots-on-the-same-page",
    "href": "Lab_2.html#multiple-plots-on-the-same-page",
    "title": "Lab 2: Intro to graphing in ggplot",
    "section": "",
    "text": "Using the simple and wonderful patchwork package, we can place multiple plots on the same page. To do this, we must actually name each plot. Here’s an example.\nPatchwork is super easy! Learn more here(with examples)\nFirst, let’s make some graphs and name them\n\n#First, we need to calculate a mean bill length for our penguins by species and island\nsumpens&lt;- penguins %&gt;%\n  group_by(species, island) %&gt;%\n  na.omit() %&gt;% #removes rows with NA values (a few rows may otherwise have NA due to sampling error in the field)\n  summarize(meanbill=mean(bill_length_mm), sd=sd(bill_length_mm), n=n(), se=sd/sqrt(n))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\nsumpens\n\n# A tibble: 5 × 6\n# Groups:   species [3]\n  species   island    meanbill    sd     n    se\n  &lt;fct&gt;     &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 Adelie    Biscoe        39.0  2.48    44 0.374\n2 Adelie    Dream         38.5  2.48    55 0.335\n3 Adelie    Torgersen     39.0  3.03    47 0.442\n4 Chinstrap Dream         48.8  3.34    68 0.405\n5 Gentoo    Biscoe        47.6  3.11   119 0.285\n\n# Next, we can make our graphs!\n\np1&lt;-ggplot(data=penguins, aes(bill_length_mm))+\n  geom_histogram()+\n  theme_classic()\n\n\np2&lt;-ggplot()+\n  geom_jitter(data= penguins, aes(x=species, y=bill_length_mm, color=island), alpha=0.5, width=0.2)+\n  geom_point(data=sumpens, aes(x=species, y=meanbill, color=island), size=3)+\n  geom_errorbar(data=sumpens, aes(x=species, ymin=meanbill-se, ymax=meanbill+se), width=0.1)+\n  theme_classic()+\n  scale_color_aaas()\n\np3&lt;-ggplot(data=penguins, aes(island)) +\n  geom_bar(aes(fill=species), position= position_dodge())+\n  theme_classic()+\n  scale_fill_aaas()\n\nNow let’s patchwork them together! We make a simple formula to make a patchwork. Addition puts everything in the same row. But we can use division and other symbols to organize.\n\nlibrary(patchwork)\n\np1+p2+p3\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nDivision allows us to put panels in columns\n\np1/p2/p3\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nWe can also combine addition and division (order of operations is still a thing!)\n\n(p1+p2) / p3\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nThere are other functions in patchwork that allow us to annotate plots, give them labels, move/combine legends, etc."
  },
  {
    "objectID": "Lab_2.html#themes",
    "href": "Lab_2.html#themes",
    "title": "Lab 2: Intro to graphing in ggplot",
    "section": "",
    "text": "Themes allow us to change the background color and most other aspects of a plot. There are a range of theme options within ggplot that will allow us to quickly make clean plots. The two that are most commonly used are theme_bw() and theme_classic()\nDefault theme (with terrible gray background)\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot(aes(fill=species))+\n  scale_fill_aaas()+\n  labs(x = 'Species', y='Bill length (mm)', title='Penguin bill length by species')\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\ntheme_bw() (removes gray background)\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot(aes(fill=species))+\n  scale_fill_aaas()+\n  labs(x = 'Species', y='Bill length (mm)', title='Penguin bill length by species')+\n  theme_bw()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\ntheme_classic() (removes gray and grid lines)\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot(aes(fill=species))+\n  scale_fill_aaas()+\n  labs(x = 'Species', y='Bill length (mm)', title='Penguin bill length by species')+\n  theme_classic()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nThe theme() function in ggplot is SUPER flexible. You can pretty much do anything with it. This is key for customizing plots. I’d encourage you to play around with this a bit. Here is a great place to learn more and see examples.\n##Some examples of using theme()\nChanging text size\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot(aes(fill=species))+\n  scale_fill_aaas()+\n  labs(x = 'Species', y='Bill length (mm)', title='Penguin bill length by species')+\n  theme(text=element_text(size=24))\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nRemove the gray background\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot(aes(fill=species))+\n  scale_fill_aaas()+\n  labs(x = 'Species', y='Bill length (mm)', title='Penguin bill length by species')+\n  theme(text=element_text(size=24), panel.background = element_rect(fill=\"white\")) #can use any color\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nTurn the X-Axis text\n\nggplot(data=penguins, aes(x=species, y= bill_length_mm)) +\n  geom_boxplot(aes(fill=species))+\n  scale_fill_aaas()+\n  labs(x = 'Species', y='Bill length (mm)', title='Penguin bill length by species')+\n  theme(text=element_text(size=24), panel.background = element_rect(fill=\"white\"), axis.text.x=element_text(angle=90, vjust=0.5, hjust=0.8)) #can adjust vertical and horizontal text positions\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n:::"
  },
  {
    "objectID": "Lab_3.html",
    "href": "Lab_3.html",
    "title": "Lab 3: Advanced data wrangling and graphing",
    "section": "",
    "text": "First, we need to load packages!\nlibrary(tidyverse)\nlibrary(palmerpenguins) #for practice data :)\nlibrary(patchwork)\nlibrary(ggsci)"
  },
  {
    "objectID": "Lab_3.html#what-is-a-pipe",
    "href": "Lab_3.html#what-is-a-pipe",
    "title": "Lab 3: Advanced data wrangling and graphing",
    "section": "What is a pipe?",
    "text": "What is a pipe?\nThe pipe, denoted as ‘|’ in most programming languages but as ‘%&gt;%’ in R, is used to link functions together. This is an oversimplification, but it works for our needs.\nA pipe (%&gt;% OR |&gt;) is useful when we want to do a sequence of actions to an original data frame. For example, maybe we want to select() some columns and then filter() the resulting selection before finally calculating an average (or something). We can do all of those steps individually or we can use pipes to do them all at once and create one output.\nWe can think of the pipe as the phrase “and then.” I will show examples in the next section.\nWhen not to use a pipe: 1.) When you want to do manipulate multiple data frames at the same time 2.) When there are meaningful intermediate objects (aka we want an intermediate step to produce a named data frame)"
  },
  {
    "objectID": "Lab_3.html#how-to-use-the-pipe",
    "href": "Lab_3.html#how-to-use-the-pipe",
    "title": "Lab 3: Advanced data wrangling and graphing",
    "section": "How to use the pipe",
    "text": "How to use the pipe\nThe pipe is coded as ‘%&gt;%’ or ‘|&gt;’ and should have a single space on either side of it at all times.\nLet’s do an example with penguins. Here we will select only species and bill length and then we will filter so that we only have chinstrap penguins.\nRemember that we think of pipe as the phrase ‘and then’\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n#pseudocode / logic: look at dataframe penguins AND THEN (%&gt;%) select() species and bill length AND THEN (%&gt;%) filter by chinstrap\n\npipepen&lt;- penguins %&gt;% #first step of the pipe is to call the original dataframe so we can modify it!\n  select(species, bill_length_mm)%&gt;% #selected our columns\n  filter(species == 'Chinstrap') #filtered for chinstrap\n\nhead(pipepen) #it worked! We didn't have to mess with intermediate dataframes and we got exactly what we needed :)\n\n# A tibble: 6 × 2\n  species   bill_length_mm\n  &lt;fct&gt;              &lt;dbl&gt;\n1 Chinstrap           46.5\n2 Chinstrap           50  \n3 Chinstrap           51.3\n4 Chinstrap           45.4\n5 Chinstrap           52.7\n6 Chinstrap           45.2\n\n\nNow we will learn how to use the pipe to do calculations that are more meaningful for us!"
  },
  {
    "objectID": "Lab_4.html",
    "href": "Lab_4.html",
    "title": "Lab 4: Beginner Stats",
    "section": "",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(see)\nlibrary(car)\nlibrary(patchwork)\nlibrary(ggsci)\nlibrary(ggridges)\nlibrary(performance)\nlibrary(Hmisc) #for correlation matrix\nlibrary(corrplot)#to visualize correlation matrices\nlibrary(car) #contains some statistical tests we need to assess assumptions\n\n\n\nA note on statistics and experimental design\nStatistics is a complex field with a long history. We could spend an entire course or even an entire career focusing on the intricate details of statistical decisions and ideas. We’ve already spent some time on this! I want you to have the statistical grounding necessary to plan your experiments and analyze your data. For biologists, statistics are a tool we can leverage to perform the best possible experiments and test our hypotheses. The T-test is the start of our stats journey. It’s a simple test and one that you may not use often, but the theory behind it sets the stage for what is to come!\n\n\n1.) Correlation between numerical variables\n\nCorrelation CoefficientsCalculate a correlation coefficient (r)Spearman’s TestMultiple CorrelationsChi-Square\n\n\nA correlation coefficient (r) tells us the relationship (strength and direction) between two variables. These coefficients can be positive or negative and will range from 0 to 1 (or negative 1). Values nearer to 1 (or negative 1) indicate stronger correlations and values closer to 0 indicate weaker correlations\nLet’s try out some correlations using the iris data.\nIs there a correlation between sepal length and sepal width? Let’s test each species separately for now.\n\nmake a scatterplot to visualize\n\n#filter down to a single species\nvirg&lt;-iris %&gt;%\n  filter(Species=='virginica')\n\n#make a plot\nggplot(virg, aes(x=Sepal.Length, y=Sepal.Width))+\n  geom_point()+\n  theme_classic()\n\n\n\n\n\n\n\ncor(virg$Sepal.Length, virg$Sepal.Width)\n\n[1] 0.4572278\n\n\nThis value (r=0.45) positive and middle of the road/strong. This tells us that some correlation likely exists.\n\n\nA hypothesis test on the correlation H0: The correlation between these two variables is 0\nHa: The correlation != 0\n\n\ncor.test(virg$Sepal.Length, virg$Sepal.Width, method=\"spearman\")\n\nWarning in cor.test.default(virg$Sepal.Length, virg$Sepal.Width, method =\n\"spearman\"): Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  virg$Sepal.Length and virg$Sepal.Width\nS = 11943, p-value = 0.002011\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.4265165 \n\n\nThe above output gives us the r value (cor=0.457) AND a p-value for a hypothesis test that the two correlations do not differ. If p&lt;0.05 we can reject our H0 and say that the correlation differs from 0. Here, p=0.0008 so we can reject H0 and suggest that we have a significant positive correlation! Rho is similar to r and is this case our correlation coefficient (0.42). It is slightly lower than the r we calculated above.\n\n\n\niris2&lt;-iris[,c(1:4)] #filter iris so we only have the numerical columns!\n\niris_cor&lt;-cor(iris2, method=\"spearman\")\n\niris_cor\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1667777    0.8818981   0.8342888\nSepal.Width    -0.1667777   1.0000000   -0.3096351  -0.2890317\nPetal.Length    0.8818981  -0.3096351    1.0000000   0.9376668\nPetal.Width     0.8342888  -0.2890317    0.9376668   1.0000000\n\n\nThe above correlation matrix shows r (correlation coefficient) not p values!\nGetting r and p values\n\nmydata.rcorr = rcorr(as.matrix(iris2))\nmydata.rcorr #top matrix = r, bottom matrix = p\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length         1.00       -0.12         0.87        0.82\nSepal.Width         -0.12        1.00        -0.43       -0.37\nPetal.Length         0.87       -0.43         1.00        0.96\nPetal.Width          0.82       -0.37         0.96        1.00\n\nn= 150 \n\n\nP\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length              0.1519      0.0000       0.0000     \nSepal.Width  0.1519                   0.0000       0.0000     \nPetal.Length 0.0000       0.0000                   0.0000     \nPetal.Width  0.0000       0.0000      0.0000                  \n\n\nPlotting our correlations\n\ncorrplot(iris_cor)\n\n\n\n\n\n\nChi-Square is for categorical correlations\n\nA Chi-square test is a statistical test used to determine if two categorical variables have a significant correlation between them. These two variables should be selected from the same population. An example - Is the color of a thing red or green? Is the answer to a simple question yes or no?\n\nData format Technically, a chi-square test is done on data that are in a contingency table (contains columns (variables) in which numbers represent counts. For example, here is a contingency table of household chore data (exciting)\n\nchore &lt;- read.delim(\"http://www.sthda.com/sthda/RDoc/data/housetasks.txt\", row.names=1)\nchore\n\n           Wife Alternating Husband Jointly\nLaundry     156          14       2       4\nMain_meal   124          20       5       4\nDinner       77          11       7      13\nBreakfeast   82          36      15       7\nTidying      53          11       1      57\nDishes       32          24       4      53\nShopping     33          23       9      55\nOfficial     12          46      23      15\nDriving      10          51      75       3\nFinances     13          13      21      66\nInsurance     8           1      53      77\nRepairs       0           3     160       2\nHolidays      0           1       6     153\n\n\nH0 = The row and column data of the contingency table are independent (no relationship) Ha= Row and column variables are dependent (there is a relationship between them)\nThe test\n\nchorechi&lt;-chisq.test(chore)\nchorechi\n\n\n    Pearson's Chi-squared test\n\ndata:  chore\nX-squared = 1944.5, df = 36, p-value &lt; 2.2e-16\n\n\nThis result demonstrates that there is a significant association between the columns and rows in the data (they are dependent).\n\nA second example\nLet’s try to assess correlation between two categorical variables in a dataframe we know! We will use mtcars\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n#make a contingency table\ncartab&lt;-table(mtcars$carb, mtcars$cyl)\n\nchisq.test(cartab)\n\nWarning in chisq.test(cartab): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  cartab\nX-squared = 24.389, df = 10, p-value = 0.006632\n\n#note that we don't NEED to make the table. We can just do this\nchisq.test(mtcars$carb, mtcars$cyl)\n\nWarning in chisq.test(mtcars$carb, mtcars$cyl): Chi-squared approximation may\nbe incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  mtcars$carb and mtcars$cyl\nX-squared = 24.389, df = 10, p-value = 0.006632\n\n\nBoth tests above are the same (just two options for you). We see that p&lt;0.05, thus we have evidence to reject H0 and suggest that carb and cyl are dependent / correlated.\n\n\n\n\n\n2.) Simple Linear Regression\nA linear regression essentially compares the correlation of one variable with another. The closer the relationship is to 1:1 (a diagonal line at 45 degrees from the x and y axis) the more correlated the two variables are. Does correlation imply causation? NO, it does not. But this type of analysis driven by hypotheses can help us seek causation/ mechanisms and statistically assess relationships.\nLet’s take a look at a simple linear regression. To do this, we will use the lm() function in R. The syntax should always be response variable ~ explanatory variable We will do this with the iris data.\n\nlm1&lt;-lm(Sepal.Length ~ Petal.Length, data=iris)\nsummary(lm1)\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.30660    0.07839   54.94   &lt;2e-16 ***\nPetal.Length  0.40892    0.01889   21.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4071 on 148 degrees of freedom\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \nF-statistic: 468.6 on 1 and 148 DF,  p-value: &lt; 2.2e-16\n\n\nThe above table produces estimates for the slope and intercept of the line.\nAt the bottom we see R2 values (multiple and adjusted. We usually use adjusted Rsquared). We also see an overall p-value for our linear regression model (H0= slope of our regression line = 0).\n\nplotting a regression lineAssumptionsAn alternative assumption checkwhen data are not linearLinear Regression with categorical variables\n\n\nIt is very easy to make a regression line in ggplot. We can plot our scatterplot as we normally would and then we add the regression line using the geom_smooth() argument.\n\nggplot(iris, aes(x=Petal.Length, y=Sepal.Length))+\n  geom_point()+\n  geom_smooth(method='lm')+\n  theme_classic()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nThe blue line represents our regression line (y~x). The gray around the line is the SE. We can add SE=FALSE to our geom_smooth() to turn that off:\ngeom_smooth(method=‘lm’, SE=FALSE)\n\n\nLinear regressions have 4 assumptions:\n1.) Linearity of the data: We assume the relationship between predictor (x) and outcome/dependent variable (y) is approx. linear. At each value of X there is a population of possible Y-values whose mean lies on the regression line.\n\n2.) Normality of residuals: The residual error are assumed to be normally distributed. In other words: at each value of X, the distribution of possible Y values is normal\n\n3.) Homogeneity of residual variance (homoscedasticity): We assume residual variance is approx. constant. In other words: the variance of Y values is the same at all values of X\n4.) Independence of residual error terms: At each value of X, the Y-measurements represent a random sample from the population of possible Y values.\n\nWe can also make a residual plot to check some of our assumptions. Residuals measure the scatter of points above or below the least-squares regression line. When we calculate the residuals for a linear regression and plot them, y=0 is the least squares line. Residuals essentially represent the distance between each point and the linear regression line we see in our regression graph.\n\nresiduals(lm1)\n\n          1           2           3           4           5           6 \n 0.22090540  0.02090540 -0.13820238 -0.31998683  0.12090540  0.39822871 \n          7           8           9          10          11          12 \n-0.27909460  0.08001317 -0.47909460 -0.01998683  0.48001317 -0.16087906 \n         13          14          15          16          17          18 \n-0.07909460 -0.45641792  1.00268985  0.78001317  0.56179762  0.22090540 \n         19          20          21          22          23          24 \n 0.69822871  0.18001317  0.39822871  0.18001317 -0.11552569  0.09822871 \n         25          26          27          28          29          30 \n-0.28355574  0.03912094  0.03912094  0.28001317  0.32090540 -0.26087906 \n         31          32          33          34          35          36 \n-0.16087906  0.48001317  0.28001317  0.62090540 -0.01998683  0.20268985 \n         37          38          39          40          41          42 \n 0.66179762  0.02090540 -0.43820238  0.18001317  0.16179762 -0.33820238 \n         43          44          45          46          47          48 \n-0.43820238  0.03912094  0.01644426 -0.07909460  0.13912094 -0.27909460 \n         49          50          51          52          53          54 \n 0.38001317  0.12090540  0.77146188  0.25324634  0.58967743 -0.44229252 \n         55          56          57          58          59          60 \n 0.31235411 -0.44675366  0.07146188 -0.75604693  0.41235411 -0.70140030 \n         61          62          63          64          65          66 \n-0.73783139 -0.12407698  0.05770748 -0.12853812 -0.17872361  0.59413856 \n         67          68          69          70          71          72 \n-0.54675366 -0.18318475  0.05324634 -0.30140030 -0.36943035  0.15770748 \n         73          74          75          76          77          78 \n-0.01032257 -0.12853812  0.33503079  0.49413856  0.53056965  0.34878520 \n         79          80          81          82          83          84 \n-0.14675366 -0.03783139 -0.36050807 -0.31961584 -0.10140030 -0.39210703 \n         85          86          87          88          89          90 \n-0.74675366 -0.14675366  0.47146188  0.19413856 -0.38318475 -0.44229252 \n         91          92          93          94          95          96 \n-0.60586144 -0.08764589 -0.14229252 -0.65604693 -0.42407698 -0.32407698 \n         97          98          99         100         101         102 \n-0.32407698  0.13503079 -0.43337025 -0.28318475 -0.46013708 -0.59210703 \n        103         104         105         106         107         108 \n 0.38075515 -0.29656817 -0.17835262  0.59450955 -1.24675366  0.41718624 \n        109         110         111         112         113         114 \n 0.02164738  0.39897069  0.10789297 -0.07389149  0.24432406 -0.65121480 \n        115         116         117         118         119         120 \n-0.59210703 -0.07389149 -0.05567594  0.65361733  0.57183287 -0.35121480 \n        121         122         123         124         125         126 \n 0.26253960 -0.71032257  0.65361733 -0.01032257  0.06253960  0.43986292 \n        127         128         129         130         131         132 \n-0.06943035 -0.21032257 -0.19656817  0.52164738  0.59897069  0.97629401 \n        133         134         135         136         137         138 \n-0.19656817 -0.09210703 -0.49656817  0.89897069 -0.29656817 -0.15567594 \n        139         140         141         142         143         144 \n-0.26943035  0.38521629  0.10343183  0.50789297 -0.59210703  0.08075515 \n        145         146         147         148         149         150 \n 0.06253960  0.26700074 -0.05121480  0.06700074 -0.31478371 -0.49210703 \n\nggplot(lm1, aes(x=.fitted, y=.resid))+\n  geom_point()+\n  geom_hline(yintercept=0, linetype='dashed')+\n  labs(x='Petal Legnth', y='Residuals')+\n  theme_classic()\n\n\n\n\n\nIf assumptions of normality and equal variance are met, a residual plot should have: - A roughly symmetric cloud of points above and below the horizontal line at 0 with a higher density of points close to the line ran away from it.\n- Little noticeable curvature as we move from left to right\n- Approx. equal variance of points above and below the line at all values of X\n\n\nThe residual plot above shows meets all assumptions, though this analysis is somewhat subjective.\n\n\nI think it is easier to do a more comprehensive visual check with the performance package in R. We can easily visually check the first 3 assumptions using check_model(). Assumption 4 requires us to think about experimental design.\n\nlm1&lt;-lm(Sepal.Length ~ Petal.Length, data=iris)\n\ncheck_model(lm1)\n\n\n\n\n\nUsing the plots above, we can check 3 / 4 of our assumptions and look for influential observations/outliers. The plots even tell us what to look for on them! This is a bit simpler than trying to analyze the residual plot.\nAs with the residual plot, this analysis of assumptions is somewhat subjective. That is ok.\n\n\nSometimes the relationship between two variables is not linear! There are many types of common relationships including logarithmic and exponential. We can often visualize these relationships and Transform our data to make them linear with some simple math.\nLet’s look at an example:\n\nhead(Loblolly)\n\nGrouped Data: height ~ age | Seed\n   height age Seed\n1    4.51   3  301\n15  10.89   5  301\n29  28.72  10  301\n43  41.74  15  301\n57  52.70  20  301\n71  60.92  25  301\n\np1&lt;-ggplot(Loblolly, aes(x=age, y=height))+\n  geom_point()+\n  geom_smooth()+\n  geom_smooth(method='lm', linetype='dashed', color='firebrick')+\n  theme_classic()+\n  labs(title='original')\n#this is roughly logarithmic in shape\n\nlob&lt;-Loblolly\nlob$age2&lt;-log(lob$age)\n\np2&lt;-ggplot(lob, aes(x=age2, y=height))+\n  geom_point()+\n  geom_smooth()+\n  geom_smooth(method='lm', linetype='dashed', color='firebrick')+\n  theme_classic()+\n  labs(title='log transformed')\n\nlob$age3=(lob$age2)^2\np3&lt;-ggplot(lob, aes(x=age3, y=height))+\n  geom_point()+\n  geom_smooth()+\n  geom_smooth(method='lm', linetype='dashed', color='firebrick')+\n  theme_classic()+\n  labs(title='squared')\n\np1/p2/p3\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nHere we can see that the transformation was fairly trivial (the data were close enough to a straight line already). BUT, technically, the first plot shows a logarithmic trend. We can transform one of the variables to generate a more linear trend. We can guess a transformation and check it with graphs or we can use our knowledge of mathematical relationships to understand how we might make our relationship more linear.\n\n\nWe can look at mtcars this time…\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nNow, I want to hypothesize that there will be no effect of cylinder on horsepower (this is called a “null hypothesis”). We’ve seen similar hypothesis before in our ANOVA.\nFirst, let’s make cylinder a factor and plot a boxplot so we can see whether there may be a trend here…\n\nmtcars$cyl1=as.factor(mtcars$cyl)\n\nggplot(mtcars, aes(x=cyl1, y=hp))+\n         geom_boxplot()+\n         theme_bw()\n\n\n\n\n\nI think it is safe to say we see what we might suspect to be a linear(ish) relationship between cyl and hp, where hp increases as cyl increases. What do you think?\nNow, let’s do some stats on this.\nRun the lm\n\nlmhp&lt;-lm(hp~cyl1, data = mtcars)\nsummary(lmhp)\n\n\nCall:\nlm(formula = hp ~ cyl1, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-59.21 -22.78  -8.25  15.97 125.79 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    82.64      11.43   7.228 5.86e-08 ***\ncyl16          39.65      18.33   2.163   0.0389 *  \ncyl18         126.58      15.28   8.285 3.92e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 37.92 on 29 degrees of freedom\nMultiple R-squared:  0.7139,    Adjusted R-squared:  0.6941 \nF-statistic: 36.18 on 2 and 29 DF,  p-value: 1.319e-08\n\n\nThis time we used a categorical x variable, which makes things a little more interesting. In the coefficients table this time we see cyl = 6 and cyl =8 represented as well as “intercept.” R takes the categorical variables and places them in alpha numeric order in these tables. So “intercept” is actually cyl=4. The “estimate” tells us the effect size of each category relative to “intercept.” SO, the mean of cyl=4 should be 82.64 (check the boxplot above to confirm). The mean of cyl=6 is not 39.65, but is actually 39.65 higher than mean of cyl=4 (82.64 + 39.65 = 132.29, which checks out). The p-values associated with each of the coefficients test the null hypothesis that each coefficient has no effect. A p &lt;0.05 indicates that the coefficient is likely to be meaningful in the model (changes in the predictor’s value are related to changes in the response value). \nFurther down, we see an R-squared of nearly 0.70, which is very good evidence of a linear relationship (70% of the variance in y can be explained by x!). The p-value is very nearly 0.00, which indicates a significant linear correlation.\nCheck assumptions!\n\ncheck_model(lmhp)\n\n\n\n\n\nHere we see some concern about Homoscedasticity and homogeneity of variance. We can probably still assume our model is reliable, but we may want to be careful. We learned ways to numerically assess this last week, but again, with high enough sample size, this won’t be an issue. Here, I would suggest that n is too small, so if this were a real statistical test we would have limitations to discuss.\n\nRemember our hypothesis (null) was: “There will be no effect of cylinder on horsepower.” We are able to reject this null hypothesis and suggest that indeed horsepower increases as cylinder increases. We might also add caveats that homoscedasticity was not confirmed due to low sample size, but the result seems clear enough that this likely doesn’t matter.\n\n\n\n\n\n\n\n3.) T-test\nThe t-test (or students’ t-test) is a basic statistical test used to assess whether or not the means of two groups are different from one another. In this test, the null hypothesis is that the two means are equal (or that there is no difference between the two means).\nA t-test should only be used if the following assumptions are met:\n1.) the two distributions whose means we are comparing must be normally distributed\n2.) The variances of the two groups must be equal\n\n\nGenerate example dataAssumption 1.) Assessing normalityAssumption 2.) Assessing equal varianceWhen can we ignore assumptions?A basic T-test in RSome examples\n\n\n\niris2&lt;-iris %&gt;%\n  filter(Species != 'setosa') %&gt;%\n  droplevels() #removes the empty levels so when we check levels below we only get the ones that are still in the data!\n\n#check levels to make sure we only have 2 species!\nhead(iris2)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1          7.0         3.2          4.7         1.4 versicolor\n2          6.4         3.2          4.5         1.5 versicolor\n3          6.9         3.1          4.9         1.5 versicolor\n4          5.5         2.3          4.0         1.3 versicolor\n5          6.5         2.8          4.6         1.5 versicolor\n6          5.7         2.8          4.5         1.3 versicolor\n\nlevels(iris2$Species)\n\n[1] \"versicolor\" \"virginica\" \n\n\nWe will use these data for our examples today. T-test requires only 2 groups/populations. We will assess the alternative hypothesis that one of our numerical variables (sepal length, sepal width, petal length, or petal width) differs by species.\nBut first, we must test our assumptions\n\n\nMethod 1: the Shapiro-Wilk Test If p &lt; 0.05 then the distribution is significantly different from normal.\nStep 1: we need to create separate data frames for each species to assess normality of each variable by species!\n\nversi&lt;-iris2 %&gt;%\n  filter(Species=='versicolor') %&gt;%\n  droplevels()\n\nvirg&lt;-iris2 %&gt;%\n  filter(Species=='virginica') %&gt;%\n  droplevels()\n\n\n\nStep 2: We can run our shapiro-wilk tests on each variable if we’d like\n\nshapiro.test(versi$Petal.Length) #this is normally distributed\n\n\n    Shapiro-Wilk normality test\n\ndata:  versi$Petal.Length\nW = 0.966, p-value = 0.1585\n\nshapiro.test(versi$Petal.Width) # this is not\n\n\n    Shapiro-Wilk normality test\n\ndata:  versi$Petal.Width\nW = 0.94763, p-value = 0.02728\n\nshapiro.test(versi$Sepal.Length) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  versi$Sepal.Length\nW = 0.97784, p-value = 0.4647\n\nshapiro.test(versi$Sepal.Width) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  versi$Sepal.Width\nW = 0.97413, p-value = 0.338\n\nshapiro.test(virg$Petal.Length) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  virg$Petal.Length\nW = 0.96219, p-value = 0.1098\n\nshapiro.test(virg$Petal.Width) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  virg$Petal.Width\nW = 0.95977, p-value = 0.08695\n\nshapiro.test(virg$Sepal.Length) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  virg$Sepal.Length\nW = 0.97118, p-value = 0.2583\n\nshapiro.test(virg$Sepal.Width) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  virg$Sepal.Width\nW = 0.96739, p-value = 0.1809\n\n\n\nMethod 2: Visualization\nExplore the following visualizations. Do you see clear evidence of normality?\n\na1&lt;-ggplot(data=iris2, aes(Petal.Length, fill=Species))+\n  geom_histogram(binwidth = 0.3)+ \n  facet_wrap(~Species)+\n  theme_classic()+\n  scale_fill_aaas()\n\na2&lt;-ggplot(data=iris2, aes(x=Petal.Length, y=Species, fill=Species))+\n  geom_density_ridges()+ #makes a smooth density curve instead of a histogram!\n  theme_classic()+\n  scale_fill_aaas()\n\na1/a2 #compare the visualizations (they are of the same data)- do we see normality here?\n\nPicking joint bandwidth of 0.206\n\n\n\n\n\n\nb1&lt;-ggplot(data=iris2, aes(Petal.Width, fill=Species))+\n  geom_histogram(binwidth = 0.3)+ \n  facet_wrap(~Species)+\n  theme_classic()+\n  scale_fill_aaas()\n\nb2&lt;-ggplot(data=iris2, aes(x=Petal.Width, y=Species, fill=Species))+\n  geom_density_ridges()+ #makes a smooth density curve instead of a histogram!\n  theme_classic()+\n  scale_fill_aaas()\n\nb1/b2 #compare the visualizations (they are of the same data)- do we see normality here?\n\nPicking joint bandwidth of 0.0972\n\n\n\n\n\n\nc1&lt;-ggplot(data=iris2, aes(Sepal.Width, fill=Species))+\n  geom_histogram(binwidth = 0.3)+ \n  facet_wrap(~Species)+\n  theme_classic()+\n  scale_fill_aaas()\n\nc2&lt;-ggplot(data=iris2, aes(x=Sepal.Width, y=Species, fill=Species))+\n  geom_density_ridges()+ #makes a smooth density curve instead of a histogram!\n  theme_classic()+\n  scale_fill_aaas()\n\nc1/c2 #compare the visualizations (they are of the same data)- do we see normality here?\n\nPicking joint bandwidth of 0.122\n\n\n\n\n\n\nd1&lt;-ggplot(data=iris2, aes(Sepal.Length, fill=Species))+\n  geom_histogram(binwidth = 0.3)+ \n  facet_wrap(~Species)+\n  theme_classic()+\n  scale_fill_aaas()\n\nd2&lt;-ggplot(data=iris2, aes(x=Sepal.Length, y=Species, fill=Species))+\n  geom_density_ridges()+ #makes a smooth density curve instead of a histogram!\n  theme_classic()+\n  scale_fill_aaas()\n\nd1/d2 #compare the visualizations (they are of the same data)- do we see normality here?\n\nPicking joint bandwidth of 0.21\n\n\n\n\n\n\n\nAKA homogeneity of variance\n\nMethods 1: F-test We will use the F-Test to compare the variance of two populations. This can only be used with 2 populations and is thus only useful when we run a t-test.\nH0 for an F-test is: The variances of the two groups are equal.\nHa: The variances are different\np&lt;0.05 allows us to reject the null (H0) and suggests that the variances are different\n\nnote: The F-test assumes our data are already normal! You should not run it on non-normal data\n\n#we use var.test to run an F-test\nf1&lt;- var.test(Petal.Length ~ Species, data=iris2)\nf1 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\n\n    F test to compare two variances\n\ndata:  Petal.Length by Species\nF = 0.72497, num df = 49, denom df = 49, p-value = 0.2637\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.411402 1.277530\nsample estimates:\nratio of variances \n         0.7249678 \n\nf2&lt;- var.test(Petal.Width ~ Species, data=iris2)\nf2 # p&lt;0.05, so we reject H0 (variances are likely different)\n\n\n    F test to compare two variances\n\ndata:  Petal.Width by Species\nF = 0.51842, num df = 49, denom df = 49, p-value = 0.02335\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.2941935 0.9135614\nsample estimates:\nratio of variances \n         0.5184243 \n\nf3&lt;- var.test(Sepal.Length ~ Species, data=iris2)\nf3 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\n\n    F test to compare two variances\n\ndata:  Sepal.Length by Species\nF = 0.65893, num df = 49, denom df = 49, p-value = 0.1478\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3739257 1.1611546\nsample estimates:\nratio of variances \n         0.6589276 \n\nf4&lt;- var.test(Sepal.Width ~ Species, data=iris2)\nf4 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\n\n    F test to compare two variances\n\ndata:  Sepal.Width by Species\nF = 0.94678, num df = 49, denom df = 49, p-value = 0.849\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.5372773 1.6684117\nsample estimates:\nratio of variances \n         0.9467839 \n\n\n\nMethod 2: Levene Test\nA more flexible test of homogeneity of variance is the Levene Test. It can be used to compare the variance of many populations (not just 2) and is more flexible than the F-test, so it can be used even if the normality assumption is violated.\nthis is the most commonly used test for homogeneity of variance\nleveneTest() is in the car package in R!\n\nN0: Variances of all populations are equal\np&lt;0.05 allows us to reject H0\n\nl1&lt;- leveneTest(Petal.Length ~ Species, data=iris2)\nl1 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  1.0674 0.3041\n      98               \n\nl2&lt;- leveneTest(Petal.Width ~ Species, data=iris2)\nl2 # p&lt;0.05, so we reject H0 (variances are likely different)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)  \ngroup  1  6.5455 0.01205 *\n      98                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nl3&lt;- leveneTest(Sepal.Length ~ Species, data=iris2)\nl3 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  1.0245 0.3139\n      98               \n\nl4&lt;- leveneTest(Sepal.Width ~ Species, data=iris2)\nl4 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  0.0873 0.7683\n      98               \n\n\n\nMethod 3: Visualization\nSince p-values are more like guidelines, we also want to visualize our data to assess homogeneity of variance. We can do that in several ways. You might already have some ideas about this! In general, it seems smart to display the raw data as points and as boxplots. Let’s start there!\n\nv1.1&lt;-ggplot(data=iris2, aes(x=Species, y=Petal.Length, color=Species))+\n  geom_point()+\n  theme_classic()+\n  scale_color_aaas()\n\nv1.2&lt;-ggplot(data=iris2, aes(x=Species, y=Petal.Length, color=Species))+\n  geom_boxplot()+\n  theme_classic()+\n  scale_color_aaas()\n\nv1.1+v1.2\n\n\n\n\n\nv2.1&lt;-ggplot(data=iris2, aes(x=Species, y=Petal.Width, color=Species))+\n  geom_point()+\n  theme_classic()+\n  scale_color_aaas()\n\nv2.2&lt;-ggplot(data=iris2, aes(x=Species, y=Petal.Width, color=Species))+\n  geom_boxplot()+\n  theme_classic()+\n  scale_color_aaas()\n\nv2.1+v2.2\n\n\n\n\n\nv3.1&lt;-ggplot(data=iris2, aes(x=Species, y=Sepal.Width, color=Species))+\n  geom_point()+\n  theme_classic()+\n  scale_color_aaas()\n\nv3.2&lt;-ggplot(data=iris2, aes(x=Species, y=Sepal.Width, color=Species))+\n  geom_boxplot()+\n  theme_classic()+\n  scale_color_aaas()\n\nv3.1+v3.2\n\n\n\n\n\nv4.1&lt;-ggplot(data=iris2, aes(x=Species, y=Sepal.Length, color=Species))+\n  geom_point()+\n  theme_classic()+\n  scale_color_aaas()\n\nv4.2&lt;-ggplot(data=iris2, aes(x=Species, y=Sepal.Length, color=Species))+\n  geom_boxplot()+\n  theme_classic()+\n  scale_color_aaas()\n\nv4.1+v4.2\n\n\n\n\n\n\nWe can if our sample sizes are large. If n is small, we should not ignore this assumption. There are alternatives to dealing with normality that we can discuss in the ANOVA section (such as transforming the data)\nFor more info on that\nWe can also ignore the equal variance requirement if we use the Welch t-test (default in R)\n\n\n\nFinally, let’s do some T-tests!\n\nH0: No difference between the means of the 2 populations p&lt;0.05 allows us to reject this H0 (indicating a likely difference)\nStep 1: Calculate means and error and plot!\n\nmeaniris&lt;-iris2 %&gt;%\n  group_by(Species) %&gt;%\n  dplyr::summarise(meanpl=mean(Petal.Length), sdpl=sd(Petal.Length), n=n(), sepl=sdpl/sqrt(n), meanpw=mean(Petal.Width), sdpw=sd(Petal.Width), n=n(), sepw=sdpw/sqrt(n), meansl=mean(Sepal.Length), sdsl=sd(Sepal.Length), n=n(), sesl=sdpl/sqrt(n), meansw=mean(Sepal.Width), sdsw=sd(Sepal.Width), n=n(), sesw=sdsw/sqrt(n))\n\nmeaniris\n\n# A tibble: 2 × 14\n  Species    meanpl  sdpl     n   sepl meanpw  sdpw   sepw meansl  sdsl   sesl\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 versicolor   4.26 0.470    50 0.0665   1.33 0.198 0.0280   5.94 0.516 0.0665\n2 virginica    5.55 0.552    50 0.0780   2.03 0.275 0.0388   6.59 0.636 0.0780\n# ℹ 3 more variables: meansw &lt;dbl&gt;, sdsw &lt;dbl&gt;, sesw &lt;dbl&gt;\n\n\n\n\n\np1&lt;-ggplot(meaniris, aes(x=Species, y=meanpl, color=Species))+\n  geom_point()+\n  geom_errorbar(aes(x=Species, ymin=meanpl-sepl, ymax=meanpl+sepl), width=0.2)+\n  scale_color_aaas()+\n  theme_classic()+\n  labs(title='Petal Length')\n\np2&lt;-ggplot(meaniris, aes(x=Species, y=meanpw, color=Species))+\n  geom_point()+\n  geom_errorbar(aes(x=Species, ymin=meanpw-sepw, ymax=meanpw+sepw), width=0.2)+\n  scale_color_aaas()+\n  theme_classic()+\n  labs(title='Petal Width')\n\np3&lt;-ggplot(meaniris, aes(x=Species, y=meansl, color=Species))+\n  geom_point()+\n  geom_errorbar(aes(x=Species, ymin=meansl-sesl, ymax=meansl+sesl), width=0.2)+\n  scale_color_aaas()+\n  theme_classic()+\n  labs(title='Sepal Length')\n\np4&lt;-ggplot(meaniris, aes(x=Species, y=meansw, color=Species))+\n  geom_point()+\n  geom_errorbar(aes(x=Species, ymin=meansw-sesw, ymax=meansw+sesw), width=0.2)+\n  scale_color_aaas()+\n  theme_classic()+\n  labs(title='Sepal Width')\n\n(p1+p2)/(p3+p4)\n\n\n\n\n\n\nDoes Petal Length differ by species?\n\nt1&lt;-t.test(data=iris2, Petal.Length~Species, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. But, we can also change them (as I will show later)\n\nt1 #p&lt;0.05 suggests that there is a significant difference in petal length between species\n\n\n    Welch Two Sample t-test\n\ndata:  Petal.Length by Species\nt = -12.604, df = 95.57, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -1.49549 -1.08851\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   4.260                    5.552 \n\n\n\nOur p&lt;0.05 suggests that there is a significant effect of species on petal length (petal length differs by species). BUT, do we get a clear explanation of which group is higher or lower? Look at the Welch T-test output and you can see the means! You can also use the graph we made to visualize this!\nDoes Petal Width differ by species?\n\nt2&lt;-t.test(data=iris2, Petal.Width~Species, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. But, we can also change them (as I will show later)\n\nt2\n\n\n    Welch Two Sample t-test\n\ndata:  Petal.Width by Species\nt = -14.625, df = 89.043, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -0.7951002 -0.6048998\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   1.326                    2.026 \n\n\n\nDoes Sepal Width differ between species?\n\nt3&lt;-t.test(data=iris2, Sepal.Width~Species, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. But, we can also change them (as I will show later)\n\nt3\n\n\n    Welch Two Sample t-test\n\ndata:  Sepal.Width by Species\nt = -3.2058, df = 97.927, p-value = 0.001819\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -0.33028364 -0.07771636\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   2.770                    2.974 \n\n\n\nDoes Sepal Length differ between species?\n\nt4&lt;-t.test(data=iris2, Sepal.Length~Species, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. But, we can also change them (as I will show later)\n\nt4\n\n\n    Welch Two Sample t-test\n\ndata:  Sepal.Length by Species\nt = -5.6292, df = 94.025, p-value = 1.866e-07\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -0.8819731 -0.4220269\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   5.936                    6.588 \n\n\nSO, when is a t-test actually useful and when isn’t it? We use a T-test ONLY when we want to compare two means / two populations. If we have more than 2 groups, a T-test is not appropriate! Instead, we need to use an analysis of variance (ANOVA) or possibly something more complex!\n\n\n\n\n\n4: Lab 4 Assignment\n\nGeneral Instructions\n1.) Please label your responses with a number and organize your assignment file in a neat and easy to read fashion! You should be able to explain what every line of code does – please do include some writing in the document so I (and future you) can follow your logic and work.\n\n2.) IF you modify a data frame, make a graph, or DO anything with a line of code, you should check your work! A visual check to make sure that what you did worked and actually worked as intended is very important. When you modify a dataframe you should give the resulting dataframe a name and then have a look at it (you can use head(df) or glimpse(df) in most cases). If you make a graph, make sure it will show up below. I need to see a confirmation step for all of your work. This will also help you, so when you go back over this work you can understand what everything does.\n\nRemember: When you run a statistical test, it is important to check assumptions. So, when you are asked to run a test below, do not forget to assess assumptions (when possible)\n\n1.) Using the penguins data (make a data frame :) ), test for correlation between flipper length and bill length. Make a graph, run the appropriate correlation test, and interpret the results.\n\n2.) Run an analysis for multiple correlations between all of the numerical variables in penguins. Make a nice looking table of the statistical outputs (r and p) using kable (we have not learned this– this is a test of your ability to use your new R problem solving skills). Interpret your statistical results (what is and what isn’t correlated, is the correlation strong, weak, positive, negative). A multiple correlation plot may help.\n\n3.) Filter the penguins data so there is only 1 species. Test the hypothesis “There is no relationship between bill length and bill depth” for this species. Make a graph, use the appropriate statistical test, and assess whether the hypothesis is rejected or whether we fail to reject it (and why). Repeat for each of the 3 species.\n\n4.) Filter out chinstrap penguins so we only have 2 species. Now, assess the following hypothesis: “There is no difference in flipper length between gentoo and adelie penguins.” Make a graph that is a good visual hypothesis test, run the appropriate stats, report the results, and use those results to assess the hypothesis.\n\n5.) Render your document and submit on Lyceum."
  },
  {
    "objectID": "Lab_4.html#plotting-a-regression-line",
    "href": "Lab_4.html#plotting-a-regression-line",
    "title": "Lab 4: Beginner Stats",
    "section": "plotting a regression line",
    "text": "plotting a regression line\nIt is very easy to make a regression line in ggplot. We can plot our scatterplot as we normally would and then we add the regression line using the geom_smooth() argument.\n\nggplot(iris, aes(x=Petal.Length, y=Sepal.Length))+\n  geom_point()+\n  geom_smooth(method='lm')+\n  theme_classic()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nThe blue line represents our regression line (y~x). The gray around the line is the SE. We can add SE=FALSE to our geom_smooth() to turn that off:\ngeom_smooth(method=‘lm’, SE=FALSE)"
  },
  {
    "objectID": "Lab_4.html#assumptions",
    "href": "Lab_4.html#assumptions",
    "title": "Lab 4: Beginner Stats",
    "section": "Assumptions",
    "text": "Assumptions\nLinear regressions have 4 assumptions:\n1.) Linearity of the data: We assume the relationship between predictor (x) and outcome/dependent variable (y) is approx. linear. At each value of X there is a population of possible Y-values whose mean lies on the regression line.\n\n2.) Normality of residuals: The residual error are assumed to be normally distributed. In other words: at each value of X, the distribution of possible Y values is normal\n\n3.) Homogeneity of residual variance (homoscedasticity): We assume residual variance is approx. constant. In other words: the variance of Y values is the same at all values of X\n4.) Independence of residual error terms: At each value of X, the Y-measurements represent a random sample from the population of possible Y values.\n\nWe can also make a residual plot to check some of our assumptions. Residuals measure the scatter of points above or below the least-squares regression line. When we calculate the residuals for a linear regression and plot them, y=0 is the least squares line. Residuals essentially represent the distance between each point and the linear regression line we see in our regression graph.\n\nresiduals(lm1)\n\n          1           2           3           4           5           6 \n 0.22090540  0.02090540 -0.13820238 -0.31998683  0.12090540  0.39822871 \n          7           8           9          10          11          12 \n-0.27909460  0.08001317 -0.47909460 -0.01998683  0.48001317 -0.16087906 \n         13          14          15          16          17          18 \n-0.07909460 -0.45641792  1.00268985  0.78001317  0.56179762  0.22090540 \n         19          20          21          22          23          24 \n 0.69822871  0.18001317  0.39822871  0.18001317 -0.11552569  0.09822871 \n         25          26          27          28          29          30 \n-0.28355574  0.03912094  0.03912094  0.28001317  0.32090540 -0.26087906 \n         31          32          33          34          35          36 \n-0.16087906  0.48001317  0.28001317  0.62090540 -0.01998683  0.20268985 \n         37          38          39          40          41          42 \n 0.66179762  0.02090540 -0.43820238  0.18001317  0.16179762 -0.33820238 \n         43          44          45          46          47          48 \n-0.43820238  0.03912094  0.01644426 -0.07909460  0.13912094 -0.27909460 \n         49          50          51          52          53          54 \n 0.38001317  0.12090540  0.77146188  0.25324634  0.58967743 -0.44229252 \n         55          56          57          58          59          60 \n 0.31235411 -0.44675366  0.07146188 -0.75604693  0.41235411 -0.70140030 \n         61          62          63          64          65          66 \n-0.73783139 -0.12407698  0.05770748 -0.12853812 -0.17872361  0.59413856 \n         67          68          69          70          71          72 \n-0.54675366 -0.18318475  0.05324634 -0.30140030 -0.36943035  0.15770748 \n         73          74          75          76          77          78 \n-0.01032257 -0.12853812  0.33503079  0.49413856  0.53056965  0.34878520 \n         79          80          81          82          83          84 \n-0.14675366 -0.03783139 -0.36050807 -0.31961584 -0.10140030 -0.39210703 \n         85          86          87          88          89          90 \n-0.74675366 -0.14675366  0.47146188  0.19413856 -0.38318475 -0.44229252 \n         91          92          93          94          95          96 \n-0.60586144 -0.08764589 -0.14229252 -0.65604693 -0.42407698 -0.32407698 \n         97          98          99         100         101         102 \n-0.32407698  0.13503079 -0.43337025 -0.28318475 -0.46013708 -0.59210703 \n        103         104         105         106         107         108 \n 0.38075515 -0.29656817 -0.17835262  0.59450955 -1.24675366  0.41718624 \n        109         110         111         112         113         114 \n 0.02164738  0.39897069  0.10789297 -0.07389149  0.24432406 -0.65121480 \n        115         116         117         118         119         120 \n-0.59210703 -0.07389149 -0.05567594  0.65361733  0.57183287 -0.35121480 \n        121         122         123         124         125         126 \n 0.26253960 -0.71032257  0.65361733 -0.01032257  0.06253960  0.43986292 \n        127         128         129         130         131         132 \n-0.06943035 -0.21032257 -0.19656817  0.52164738  0.59897069  0.97629401 \n        133         134         135         136         137         138 \n-0.19656817 -0.09210703 -0.49656817  0.89897069 -0.29656817 -0.15567594 \n        139         140         141         142         143         144 \n-0.26943035  0.38521629  0.10343183  0.50789297 -0.59210703  0.08075515 \n        145         146         147         148         149         150 \n 0.06253960  0.26700074 -0.05121480  0.06700074 -0.31478371 -0.49210703 \n\nggplot(lm1, aes(x=.fitted, y=.resid))+\n  geom_point()+\n  geom_hline(yintercept=0, linetype='dashed')+\n  labs(x='Petal Legnth', y='Residuals')+\n  theme_classic()\n\n\n\n\n\nIf assumptions of normality and equal variance are met, a residual plot should have: - A roughly symmetric cloud of points above and below the horizontal line at 0 with a higher density of points close to the line ran away from it.\n- Little noticeable curvature as we move from left to right\n- Approx. equal variance of points above and below the line at all values of X\n\n\nThe residual plot above shows meets all assumptions, though this analysis is somewhat subjective."
  },
  {
    "objectID": "Lab_4.html#an-alternative-assumption-check",
    "href": "Lab_4.html#an-alternative-assumption-check",
    "title": "Lab 4: Beginner Stats",
    "section": "An alternative assumption check",
    "text": "An alternative assumption check\nI think it is easier to do a more comprehensive visual check with the performance package in R. We can easily visually check the first 3 assumptions using check_model(). Assumption 4 requires us to think about experimental design.\n\nlm1&lt;-lm(Sepal.Length ~ Petal.Length, data=iris)\n\ncheck_model(lm1)\n\n\n\n\n\nUsing the plots above, we can check 3 / 4 of our assumptions and look for influential observations/outliers. The plots even tell us what to look for on them! This is a bit simpler than trying to analyze the residual plot.\nAs with the residual plot, this analysis of assumptions is somewhat subjective. That is ok."
  },
  {
    "objectID": "Lab_4.html#when-data-are-not-linear",
    "href": "Lab_4.html#when-data-are-not-linear",
    "title": "Lab 4: Beginner Stats",
    "section": "when data are not linear",
    "text": "when data are not linear\nSometimes the relationship between two variables is not linear! There are many types of common relationships including logarithmic and exponential. We can often visualize these relationships and Transform our data to make them linear with some simple math.\nLet’s look at an example:\n\nhead(Loblolly)\n\nGrouped Data: height ~ age | Seed\n   height age Seed\n1    4.51   3  301\n15  10.89   5  301\n29  28.72  10  301\n43  41.74  15  301\n57  52.70  20  301\n71  60.92  25  301\n\np1&lt;-ggplot(Loblolly, aes(x=age, y=height))+\n  geom_point()+\n  geom_smooth()+\n  geom_smooth(method='lm', linetype='dashed', color='firebrick')+\n  theme_classic()+\n  labs(title='original')\n#this is roughly logarithmic in shape\n\nlob&lt;-Loblolly\nlob$age2&lt;-log(lob$age)\n\np2&lt;-ggplot(lob, aes(x=age2, y=height))+\n  geom_point()+\n  geom_smooth()+\n  geom_smooth(method='lm', linetype='dashed', color='firebrick')+\n  theme_classic()+\n  labs(title='log transformed')\n\nlob$age3=(lob$age2)^2\np3&lt;-ggplot(lob, aes(x=age3, y=height))+\n  geom_point()+\n  geom_smooth()+\n  geom_smooth(method='lm', linetype='dashed', color='firebrick')+\n  theme_classic()+\n  labs(title='squared')\n\np1/p2/p3\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nHere we can see that the transformation was fairly trivial (the data were close enough to a straight line already). BUT, technically, the first plot shows a logarithmic trend. We can transform one of the variables to generate a more linear trend. We can guess a transformation and check it with graphs or we can use our knowledge of mathematical relationships to understand how we might make our relationship more linear."
  },
  {
    "objectID": "Lab_4.html#linear-regression-with-categorical-variables",
    "href": "Lab_4.html#linear-regression-with-categorical-variables",
    "title": "Lab 4: Beginner Stats",
    "section": "Linear Regression with categorical variables",
    "text": "Linear Regression with categorical variables\nWe can look at mtcars this time…\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nNow, I want to hypothesize that there will be no effect of cylinder on horsepower (this is called a “null hypothesis”). We’ve seen similar hypothesis before in our ANOVA.\nFirst, let’s make cylinder a factor and plot a boxplot so we can see whether there may be a trend here…\n\nmtcars$cyl1=as.factor(mtcars$cyl)\n\nggplot(mtcars, aes(x=cyl1, y=hp))+\n         geom_boxplot()+\n         theme_bw()\n\n\n\n\n\nI think it is safe to say we see what we might suspect to be a linear(ish) relationship between cyl and hp, where hp increases as cyl increases. What do you think?\nNow, let’s do some stats on this.\nRun the lm\n\nlmhp&lt;-lm(hp~cyl1, data = mtcars)\nsummary(lmhp)\n\n\nCall:\nlm(formula = hp ~ cyl1, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-59.21 -22.78  -8.25  15.97 125.79 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    82.64      11.43   7.228 5.86e-08 ***\ncyl16          39.65      18.33   2.163   0.0389 *  \ncyl18         126.58      15.28   8.285 3.92e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 37.92 on 29 degrees of freedom\nMultiple R-squared:  0.7139,    Adjusted R-squared:  0.6941 \nF-statistic: 36.18 on 2 and 29 DF,  p-value: 1.319e-08\n\n\nThis time we used a categorical x variable, which makes things a little more interesting. In the coefficients table this time we see cyl = 6 and cyl =8 represented as well as “intercept.” R takes the categorical variables and places them in alpha numeric order in these tables. So “intercept” is actually cyl=4. The “estimate” tells us the effect size of each category relative to “intercept.” SO, the mean of cyl=4 should be 82.64 (check the boxplot above to confirm). The mean of cyl=6 is not 39.65, but is actually 39.65 higher than mean of cyl=4 (82.64 + 39.65 = 132.29, which checks out). The p-values associated with each of the coefficients test the null hypothesis that each coefficient has no effect. A p &lt;0.05 indicates that the coefficient is likely to be meaningful in the model (changes in the predictor’s value are related to changes in the response value). \nFurther down, we see an R-squared of nearly 0.70, which is very good evidence of a linear relationship (70% of the variance in y can be explained by x!). The p-value is very nearly 0.00, which indicates a significant linear correlation.\nCheck assumptions!\n\ncheck_model(lmhp)\n\n\n\n\n\nHere we see some concern about Homoscedasticity and homogeneity of variance. We can probably still assume our model is reliable, but we may want to be careful. We learned ways to numerically assess this last week, but again, with high enough sample size, this won’t be an issue. Here, I would suggest that n is too small, so if this were a real statistical test we would have limitations to discuss.\n\nRemember our hypothesis (null) was: “There will be no effect of cylinder on horsepower.” We are able to reject this null hypothesis and suggest that indeed horsepower increases as cylinder increases. We might also add caveats that homoscedasticity was not confirmed due to low sample size, but the result seems clear enough that this likely doesn’t matter.\n\n# 3.) T-test\nThe t-test (or students’ t-test) is a basic statistical test used to assess whether or not the means of two groups are different from one another. In this test, the null hypothesis is that the two means are equal (or that there is no difference between the two means).\nA t-test should only be used if the following assumptions are met:\n1.) the two distributions whose means we are comparing must be normally distributed\n2.) The variances of the two groups must be equal\n\nGenerate example data\n\niris2&lt;-iris %&gt;%\n  filter(Species != 'setosa') %&gt;%\n  droplevels() #removes the empty levels so when we check levels below we only get the ones that are still in the data!\n\n#check levels to make sure we only have 2 species!\nhead(iris2)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1          7.0         3.2          4.7         1.4 versicolor\n2          6.4         3.2          4.5         1.5 versicolor\n3          6.9         3.1          4.9         1.5 versicolor\n4          5.5         2.3          4.0         1.3 versicolor\n5          6.5         2.8          4.6         1.5 versicolor\n6          5.7         2.8          4.5         1.3 versicolor\n\nlevels(iris2$Species)\n\n[1] \"versicolor\" \"virginica\" \n\n\nWe will use these data for our examples today. T-test requires only 2 groups/populations. We will assess the alternative hypothesis that one of our numerical variables (sepal length, sepal width, petal length, or petal width) differs by species.\nBut first, we must test our assumptions\n\nAssumption 1.) Assessing normalityAssumption 2.) Assessing equal varianceWhen can we ignore assumptions?A basic T-test in RSome examples\n\n\nMethod 1: the Shapiro-Wilk Test If p &lt; 0.05 then the distribution is significantly different from normal.\nStep 1: we need to create separate data frames for each species to assess normality of each variable by species!\n\nversi&lt;-iris2 %&gt;%\n  filter(Species=='versicolor') %&gt;%\n  droplevels()\n\nvirg&lt;-iris2 %&gt;%\n  filter(Species=='virginica') %&gt;%\n  droplevels()\n\n\n\nStep 2: We can run our shapiro-wilk tests on each variable if we’d like\n\nshapiro.test(versi$Petal.Length) #this is normally distributed\n\n\n    Shapiro-Wilk normality test\n\ndata:  versi$Petal.Length\nW = 0.966, p-value = 0.1585\n\nshapiro.test(versi$Petal.Width) # this is not\n\n\n    Shapiro-Wilk normality test\n\ndata:  versi$Petal.Width\nW = 0.94763, p-value = 0.02728\n\nshapiro.test(versi$Sepal.Length) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  versi$Sepal.Length\nW = 0.97784, p-value = 0.4647\n\nshapiro.test(versi$Sepal.Width) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  versi$Sepal.Width\nW = 0.97413, p-value = 0.338\n\nshapiro.test(virg$Petal.Length) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  virg$Petal.Length\nW = 0.96219, p-value = 0.1098\n\nshapiro.test(virg$Petal.Width) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  virg$Petal.Width\nW = 0.95977, p-value = 0.08695\n\nshapiro.test(virg$Sepal.Length) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  virg$Sepal.Length\nW = 0.97118, p-value = 0.2583\n\nshapiro.test(virg$Sepal.Width) #normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  virg$Sepal.Width\nW = 0.96739, p-value = 0.1809\n\n\n\nMethod 2: Visualization\nExplore the following visualizations. Do you see clear evidence of normality?\n\na1&lt;-ggplot(data=iris2, aes(Petal.Length, fill=Species))+\n  geom_histogram(binwidth = 0.3)+ \n  facet_wrap(~Species)+\n  theme_classic()+\n  scale_fill_aaas()\n\na2&lt;-ggplot(data=iris2, aes(x=Petal.Length, y=Species, fill=Species))+\n  geom_density_ridges()+ #makes a smooth density curve instead of a histogram!\n  theme_classic()+\n  scale_fill_aaas()\n\na1/a2 #compare the visualizations (they are of the same data)- do we see normality here?\n\nPicking joint bandwidth of 0.206\n\n\n\n\n\n\nb1&lt;-ggplot(data=iris2, aes(Petal.Width, fill=Species))+\n  geom_histogram(binwidth = 0.3)+ \n  facet_wrap(~Species)+\n  theme_classic()+\n  scale_fill_aaas()\n\nb2&lt;-ggplot(data=iris2, aes(x=Petal.Width, y=Species, fill=Species))+\n  geom_density_ridges()+ #makes a smooth density curve instead of a histogram!\n  theme_classic()+\n  scale_fill_aaas()\n\nb1/b2 #compare the visualizations (they are of the same data)- do we see normality here?\n\nPicking joint bandwidth of 0.0972\n\n\n\n\n\n\nc1&lt;-ggplot(data=iris2, aes(Sepal.Width, fill=Species))+\n  geom_histogram(binwidth = 0.3)+ \n  facet_wrap(~Species)+\n  theme_classic()+\n  scale_fill_aaas()\n\nc2&lt;-ggplot(data=iris2, aes(x=Sepal.Width, y=Species, fill=Species))+\n  geom_density_ridges()+ #makes a smooth density curve instead of a histogram!\n  theme_classic()+\n  scale_fill_aaas()\n\nc1/c2 #compare the visualizations (they are of the same data)- do we see normality here?\n\nPicking joint bandwidth of 0.122\n\n\n\n\n\n\nd1&lt;-ggplot(data=iris2, aes(Sepal.Length, fill=Species))+\n  geom_histogram(binwidth = 0.3)+ \n  facet_wrap(~Species)+\n  theme_classic()+\n  scale_fill_aaas()\n\nd2&lt;-ggplot(data=iris2, aes(x=Sepal.Length, y=Species, fill=Species))+\n  geom_density_ridges()+ #makes a smooth density curve instead of a histogram!\n  theme_classic()+\n  scale_fill_aaas()\n\nd1/d2 #compare the visualizations (they are of the same data)- do we see normality here?\n\nPicking joint bandwidth of 0.21\n\n\n\n\n\n\n\nAKA homogeneity of variance\n\nMethods 1: F-test We will use the F-Test to compare the variance of two populations. This can only be used with 2 populations and is thus only useful when we run a t-test.\nH0 for an F-test is: The variances of the two groups are equal.\nHa: The variances are different\np&lt;0.05 allows us to reject the null (H0) and suggests that the variances are different\n\nnote: The F-test assumes our data are already normal! You should not run it on non-normal data\n\n#we use var.test to run an F-test\nf1&lt;- var.test(Petal.Length ~ Species, data=iris2)\nf1 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\n\n    F test to compare two variances\n\ndata:  Petal.Length by Species\nF = 0.72497, num df = 49, denom df = 49, p-value = 0.2637\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.411402 1.277530\nsample estimates:\nratio of variances \n         0.7249678 \n\nf2&lt;- var.test(Petal.Width ~ Species, data=iris2)\nf2 # p&lt;0.05, so we reject H0 (variances are likely different)\n\n\n    F test to compare two variances\n\ndata:  Petal.Width by Species\nF = 0.51842, num df = 49, denom df = 49, p-value = 0.02335\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.2941935 0.9135614\nsample estimates:\nratio of variances \n         0.5184243 \n\nf3&lt;- var.test(Sepal.Length ~ Species, data=iris2)\nf3 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\n\n    F test to compare two variances\n\ndata:  Sepal.Length by Species\nF = 0.65893, num df = 49, denom df = 49, p-value = 0.1478\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3739257 1.1611546\nsample estimates:\nratio of variances \n         0.6589276 \n\nf4&lt;- var.test(Sepal.Width ~ Species, data=iris2)\nf4 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\n\n    F test to compare two variances\n\ndata:  Sepal.Width by Species\nF = 0.94678, num df = 49, denom df = 49, p-value = 0.849\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.5372773 1.6684117\nsample estimates:\nratio of variances \n         0.9467839 \n\n\n\nMethod 2: Levene Test\nA more flexible test of homogeneity of variance is the Levene Test. It can be used to compare the variance of many populations (not just 2) and is more flexible than the F-test, so it can be used even if the normality assumption is violated.\nthis is the most commonly used test for homogeneity of variance\nleveneTest() is in the car package in R!\n\nN0: Variances of all populations are equal\np&lt;0.05 allows us to reject H0\n\nl1&lt;- leveneTest(Petal.Length ~ Species, data=iris2)\nl1 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  1.0674 0.3041\n      98               \n\nl2&lt;- leveneTest(Petal.Width ~ Species, data=iris2)\nl2 # p&lt;0.05, so we reject H0 (variances are likely different)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)  \ngroup  1  6.5455 0.01205 *\n      98                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nl3&lt;- leveneTest(Sepal.Length ~ Species, data=iris2)\nl3 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  1.0245 0.3139\n      98               \n\nl4&lt;- leveneTest(Sepal.Width ~ Species, data=iris2)\nl4 # p&gt;0.05, so we fail to reject H0 (the variances are likely equal)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  0.0873 0.7683\n      98               \n\n\n\nMethod 3: Visualization\nSince p-values are more like guidelines, we also want to visualize our data to assess homogeneity of variance. We can do that in several ways. You might already have some ideas about this! In general, it seems smart to display the raw data as points and as boxplots. Let’s start there!\n\nv1.1&lt;-ggplot(data=iris2, aes(x=Species, y=Petal.Length, color=Species))+\n  geom_point()+\n  theme_classic()+\n  scale_color_aaas()\n\nv1.2&lt;-ggplot(data=iris2, aes(x=Species, y=Petal.Length, color=Species))+\n  geom_boxplot()+\n  theme_classic()+\n  scale_color_aaas()\n\nv1.1+v1.2\n\n\n\n\n\nv2.1&lt;-ggplot(data=iris2, aes(x=Species, y=Petal.Width, color=Species))+\n  geom_point()+\n  theme_classic()+\n  scale_color_aaas()\n\nv2.2&lt;-ggplot(data=iris2, aes(x=Species, y=Petal.Width, color=Species))+\n  geom_boxplot()+\n  theme_classic()+\n  scale_color_aaas()\n\nv2.1+v2.2\n\n\n\n\n\nv3.1&lt;-ggplot(data=iris2, aes(x=Species, y=Sepal.Width, color=Species))+\n  geom_point()+\n  theme_classic()+\n  scale_color_aaas()\n\nv3.2&lt;-ggplot(data=iris2, aes(x=Species, y=Sepal.Width, color=Species))+\n  geom_boxplot()+\n  theme_classic()+\n  scale_color_aaas()\n\nv3.1+v3.2\n\n\n\n\n\nv4.1&lt;-ggplot(data=iris2, aes(x=Species, y=Sepal.Length, color=Species))+\n  geom_point()+\n  theme_classic()+\n  scale_color_aaas()\n\nv4.2&lt;-ggplot(data=iris2, aes(x=Species, y=Sepal.Length, color=Species))+\n  geom_boxplot()+\n  theme_classic()+\n  scale_color_aaas()\n\nv4.1+v4.2\n\n\n\n\n\n\nWe can if our sample sizes are large. If n is small, we should not ignore this assumption. There are alternatives to dealing with normality that we can discuss in the ANOVA section (such as transforming the data)\nFor more info on that\nWe can also ignore the equal variance requirement if we use the Welch t-test (default in R)\n\n\n\nFinally, let’s do some T-tests!\n\nH0: No difference between the means of the 2 populations p&lt;0.05 allows us to reject this H0 (indicating a likely difference)\nStep 1: Calculate means and error and plot!\n\nmeaniris&lt;-iris2 %&gt;%\n  group_by(Species) %&gt;%\n  dplyr::summarise(meanpl=mean(Petal.Length), sdpl=sd(Petal.Length), n=n(), sepl=sdpl/sqrt(n), meanpw=mean(Petal.Width), sdpw=sd(Petal.Width), n=n(), sepw=sdpw/sqrt(n), meansl=mean(Sepal.Length), sdsl=sd(Sepal.Length), n=n(), sesl=sdpl/sqrt(n), meansw=mean(Sepal.Width), sdsw=sd(Sepal.Width), n=n(), sesw=sdsw/sqrt(n))\n\nmeaniris\n\n# A tibble: 2 × 14\n  Species    meanpl  sdpl     n   sepl meanpw  sdpw   sepw meansl  sdsl   sesl\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 versicolor   4.26 0.470    50 0.0665   1.33 0.198 0.0280   5.94 0.516 0.0665\n2 virginica    5.55 0.552    50 0.0780   2.03 0.275 0.0388   6.59 0.636 0.0780\n# ℹ 3 more variables: meansw &lt;dbl&gt;, sdsw &lt;dbl&gt;, sesw &lt;dbl&gt;\n\n\n\n\n\np1&lt;-ggplot(meaniris, aes(x=Species, y=meanpl, color=Species))+\n  geom_point()+\n  geom_errorbar(aes(x=Species, ymin=meanpl-sepl, ymax=meanpl+sepl), width=0.2)+\n  scale_color_aaas()+\n  theme_classic()+\n  labs(title='Petal Length')\n\np2&lt;-ggplot(meaniris, aes(x=Species, y=meanpw, color=Species))+\n  geom_point()+\n  geom_errorbar(aes(x=Species, ymin=meanpw-sepw, ymax=meanpw+sepw), width=0.2)+\n  scale_color_aaas()+\n  theme_classic()+\n  labs(title='Petal Width')\n\np3&lt;-ggplot(meaniris, aes(x=Species, y=meansl, color=Species))+\n  geom_point()+\n  geom_errorbar(aes(x=Species, ymin=meansl-sesl, ymax=meansl+sesl), width=0.2)+\n  scale_color_aaas()+\n  theme_classic()+\n  labs(title='Sepal Length')\n\np4&lt;-ggplot(meaniris, aes(x=Species, y=meansw, color=Species))+\n  geom_point()+\n  geom_errorbar(aes(x=Species, ymin=meansw-sesw, ymax=meansw+sesw), width=0.2)+\n  scale_color_aaas()+\n  theme_classic()+\n  labs(title='Sepal Width')\n\n(p1+p2)/(p3+p4)\n\n\n\n\n\n\nDoes Petal Length differ by species?\n\nt1&lt;-t.test(data=iris2, Petal.Length~Species, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. BUt, we can also change them (as I will show later)\n\nt1 #p&lt;0.05 suggests that there is a significant difference in petal length between species\n\n\n    Welch Two Sample t-test\n\ndata:  Petal.Length by Species\nt = -12.604, df = 95.57, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -1.49549 -1.08851\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   4.260                    5.552 \n\n\n\nOur p&lt;0.05 suggests that there is a significant effect of species on petal length (petal length differs by species). BUT, do we get a clear explanation of which group is higher or lower? Look at the Welch T-test output and you can see the means! You can also use the graph we made to visualize this!\nDoes Petal Width differ by species?\n\nt2&lt;-t.test(data=iris2, Petal.Width~Species, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. BUt, we can also change them (as I will show later)\n\nt2\n\n\n    Welch Two Sample t-test\n\ndata:  Petal.Width by Species\nt = -14.625, df = 89.043, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -0.7951002 -0.6048998\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   1.326                    2.026 \n\n\n\nDoes Sepal Width differ between species?\n\nt3&lt;-t.test(data=iris2, Sepal.Width~Species, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. BUt, we can also change them (as I will show later)\n\nt3\n\n\n    Welch Two Sample t-test\n\ndata:  Sepal.Width by Species\nt = -3.2058, df = 97.927, p-value = 0.001819\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -0.33028364 -0.07771636\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   2.770                    2.974 \n\n\n\nDoes Sepal Length differ between species?\n\nt4&lt;-t.test(data=iris2, Sepal.Length~Species, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. BUt, we can also change them (as I will show later)\n\nt4\n\n\n    Welch Two Sample t-test\n\ndata:  Sepal.Length by Species\nt = -5.6292, df = 94.025, p-value = 1.866e-07\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -0.8819731 -0.4220269\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   5.936                    6.588 \n\n\nSO, when is a t-test actually useful and when isn’t it? We use a T-test ONLY when we want to compare two means / two populations. If we have more than 2 groups, a T-test is not appropriate! Instead, we need to use an analysis of variance (ANOVA) or possibly something more complex!"
  },
  {
    "objectID": "Lab_5.html",
    "href": "Lab_5.html",
    "title": "Lab 5: ANOVA",
    "section": "",
    "text": "IN THIS TUTORIAL YOU WILL LEARN:\n1.) The theory behind analysis of variance\n2.) How to perform 1 way and multi-way ANOVA in R\n3.) How to use TukeyHSD in R\n4.) How to test assumptions of ANOVA"
  },
  {
    "objectID": "Lab_5.html#another-anova-example",
    "href": "Lab_5.html#another-anova-example",
    "title": "Lab 5: ANOVA",
    "section": "Another Anova example",
    "text": "Another Anova example\nLet’s apply those ANOVA skills to a new dataset. We will use the dataset called ‘iris’ that is built into R. It’s a simply dataframe that contains attributes of different flowers. It is similar to penguins!\nFirst step: Let’s plot some data! I want to assess the effect of species on petal.length\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\nggplot(data=iris, aes(x=Species, y=Petal.Length, group=Species))+\n  geom_boxplot(aes(fill=Species))+\n  theme_bw()+ #remember this?\n  scale_color_npg()+\n  scale_fill_npg()\n\n\n\n\n\nOur plot shows us that there is almost certainly an effect of species on petal length. But, let’s check that with an anova.\nSecond step: Check ANOVA assumptions.\nIndependence: We will assume this is fine. We can’t possibly know for sure given the source of the data. Outliers: We already checked visually. Nothing stands out as an extreme outlier. But let’s confirm\n\niris %&gt;% \n  group_by(Species) %&gt;%\n  identify_outliers(Petal.Length) #No extreme outliers, so let's carry on\n\n# A tibble: 5 × 7\n  Species    Sepal.Length Sepal.Width Petal.Length Petal.Width is.outlier\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;lgl&gt;     \n1 setosa              4.3         3            1.1         0.1 TRUE      \n2 setosa              4.6         3.6          1           0.2 TRUE      \n3 setosa              4.8         3.4          1.9         0.2 TRUE      \n4 setosa              5.1         3.8          1.9         0.4 TRUE      \n5 versicolor          5.1         2.5          3           1.1 TRUE      \n# ℹ 1 more variable: is.extreme &lt;lgl&gt;\n\n\nNormality and Homoscedasticity\nVisual Check Using check_model() we see that we have a normal distribution (yay) but there is some concern about homogeneity of variance.\n\nmodeliris&lt;-aov(Petal.Length ~ Species, data=iris)\ncheck_model(modeliris)\n\n\n\n\nNumbers check (shapiro for normality, levene for homosced) Normality is fine, but homoscedasticity assumption is not met, so we need a Welch test\n\n#entire model normality\nshapiro_test(residuals(modeliris)) #this comes out significant indicating non-normality in the residuals. This is clearly not the case when we look at all of the other data and figures we have, so let's not worry about it. \n\n# A tibble: 1 × 3\n  variable             statistic p.value\n  &lt;chr&gt;                    &lt;dbl&gt;   &lt;dbl&gt;\n1 residuals(modeliris)     0.981  0.0368\n\n#normality of each group\niris %&gt;%\n  group_by(Species)%&gt;%\n  shapiro_test(Petal.Length) # All come out as normal, note the almost significant p for setosa\n\n# A tibble: 3 × 4\n  Species    variable     statistic      p\n  &lt;fct&gt;      &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n1 setosa     Petal.Length     0.955 0.0548\n2 versicolor Petal.Length     0.966 0.158 \n3 virginica  Petal.Length     0.962 0.110 \n\n#Homogeneity of variance /Homoscedasticity \nleveneTest(Petal.Length~Species, data=iris) #This is significant, so there is an issue with heterogeneity of variance and that assumption is not met. Thus, we need to do a \n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup   2   19.48 3.129e-08 ***\n      147                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFinally, let’s run the anova and see if we need a welch test\n\n#regular anova\naoviris&lt;-aov(Petal.Length~Species, data=iris) \naoviris #this tells us we may have unbalanced effects\n\nCall:\n   aov(formula = Petal.Length ~ Species, data = iris)\n\nTerms:\n                 Species Residuals\nSum of Squares  437.1028   27.2226\nDeg. of Freedom        2       147\n\nResidual standard error: 0.4303345\nEstimated effects may be unbalanced\n\nsummary(aoviris) #this summary still gives us a p value, so are we really doing something wrong??\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  437.1  218.55    1180 &lt;2e-16 ***\nResiduals   147   27.2    0.19                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#welch anova\nwelchiris&lt;-welch_anova_test(Petal.Length~Species, data=iris) # We see a significant effect\nwelchiris\n\n# A tibble: 1 × 7\n  .y.              n statistic   DFn   DFd        p method     \n* &lt;chr&gt;        &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      \n1 Petal.Length   150     1828.     2  78.1 2.69e-66 Welch ANOVA"
  },
  {
    "objectID": "lec_1.html",
    "href": "lec_1.html",
    "title": "1 - Intro to data and stats",
    "section": "",
    "text": "Recording\n\n\n\nSlides\nThis is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "lec_2.html",
    "href": "lec_2.html",
    "title": "2 - Distributions and Data",
    "section": "",
    "text": "Recording\n\n\n\nSlides\nThis is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "lec_3.html",
    "href": "lec_3.html",
    "title": "3 - Probability",
    "section": "",
    "text": "Recording\n\n\n\nSlides\nThis is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "lec_4.html",
    "href": "lec_4.html",
    "title": "4 - Experimental Design",
    "section": "",
    "text": "Recording\n\n\n\nSlides\nThis is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "lec_5.html",
    "href": "lec_5.html",
    "title": "5 - Hypothesis Testing",
    "section": "",
    "text": "Recording\n\n\n\nSlides\nThis is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "lec_6.html",
    "href": "lec_6.html",
    "title": "6 - Normal Distribution and t-test",
    "section": "",
    "text": "Recording\n\n\n\nSlides\nThis is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "lec_7.html",
    "href": "lec_7.html",
    "title": "7 - Correlation and Linear Regression",
    "section": "",
    "text": "Recording\n\n\n\nSlides\nThis is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "lec_8.html",
    "href": "lec_8.html",
    "title": "8 - ANOVA",
    "section": "",
    "text": "Recording\n\n\n\nSlides\nThis is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;."
  },
  {
    "objectID": "Lab_0.html",
    "href": "Lab_0.html",
    "title": "Lab 0: Getting Started",
    "section": "",
    "text": "IN THIS TUTORIAL YOU WILL LEARN:\n1.) How to access and/or install R and RStudio\n2.) How to navigate RStudio\n3.) How to set and change the working directory\n4.) How to setup an RStudio Project\n5.) How to make a quarto doc\n6.) Some R basics\n7.) How to annotate code"
  },
  {
    "objectID": "Lab_0.html#how-to-set-the-working-directory",
    "href": "Lab_0.html#how-to-set-the-working-directory",
    "title": "Lab 0: Getting Started",
    "section": "How to SET the working directory",
    "text": "How to SET the working directory\n1.) Using the “Files” tab to set manually: a.) Using the ‘…’ in the ‘Files’ tab you can select any directory (folder) on your computer. You can also set a google drive, box, dropbox, or other shared folder as your working directory if you’d like (as long as you are syncing a folder between the cloud and your computer – ASK me if you have questions about this!) b.) Once you navigate to a directory you still need to SET IT as your working directory. You do this in the “More” cog– select “Set as working directory”\n2.) Set working directory with code: We use the ‘setwd()’ function for this. Below is an example. You will need to replace the path details with your own!\n\nsetwd(\"C:/Users/Justin Baumann/Teaching/Bates College/ENV 282 - Research Design in Env Sci\")"
  },
  {
    "objectID": "Lab_0.html#in-line-comments",
    "href": "Lab_0.html#in-line-comments",
    "title": "Lab 0: Getting Started",
    "section": "in-line comments",
    "text": "in-line comments\nCommenting on each line is the traditional way to comment code. This predates Rstudio and quarto and can be a bit clunky, but is still effective and is the only easy way to comment in a typical Rscript (note that you are using a quarto doc - .qmd, not an rscript - .R). You can see many examples of in-line commenting in these tutorials, as I use it all the time. To make a commennt on a line of code, you use the ‘#’. Everything after the ‘#’ is a comment and should be green.\n\n#This is a comment\n\n#In this code chunk, I will load packages and read in some data from palmerpenguins\n\n# 1. load packages\nlibrary(tidyverse) #this line loads tidyverse\nlibrary(palmerpenguins) # this line loads palmerpenguins\n\n# 2. read data from penguins\npens&lt;-penguins #reads the dataframe 'penguins' from palmerpenguins and saves it in my environment as 'pens'"
  },
  {
    "objectID": "Lab_0.html#quarto-as-comments",
    "href": "Lab_0.html#quarto-as-comments",
    "title": "Lab 0: Getting Started",
    "section": "quarto as comments",
    "text": "quarto as comments\nQuarto and other markdown programs allow you to write a lot more text than a standard R script does. SO, USE IT! Write in sentences, paragraphs, etc about what you are doing. Use your formatting tricks (section headers, bold, italic, etc). Look at section 5 above for examples."
  },
  {
    "objectID": "Lab_0.html#code-annotation-in-quarto",
    "href": "Lab_0.html#code-annotation-in-quarto",
    "title": "Lab 0: Getting Started",
    "section": "code annotation in quarto",
    "text": "code annotation in quarto\nQuarto is very functional and allows you to make really nice looking annotations in a footnote style. Here is what that looks like, using the same code from the previous section:\n\n#This is a comment\n\n#In this code chunk, I will load packages and read in some data from palmerpenguins\n\n# 1. load packages\n1library(tidyverse)\n2library(palmerpenguins)\n\n# 2. read data from penguins\n3pens&lt;-penguins\n\n\n1\n\nthis line loads tidyverse\n\n2\n\nthis line loads palmerpenguins\n\n3\n\nreads the dataframe ‘penguins’ from palmerpenguins and saves it in my environment as ‘pens’\n\nHere is what the code for the above footnotes looks like"
  },
  {
    "objectID": "Lab_7_multiple_regression.html",
    "href": "Lab_7_multiple_regression.html",
    "title": "Lab 7: Multiple Regression",
    "section": "",
    "text": "Lab 7: Multiple Regression and Model Selection\n\nload packages\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(data.table)\nlibrary(performance)\nlibrary(patchwork)\nlibrary(car) \nlibrary(rsample)\nlibrary(moderndive) #helps us make useful regression tables\n\nMultiple Regression is a powerful tool that allows us to investigate the effects of multiple varibles on our response variable of choice. This is great for real world data, as it is almost never the case that we have the effect of a single variable on another. Instead, we often have so many that we need to dig into mixed effects modeling, which is the next step up from multiple regression.\nMy favorite mixed models selection tutorial: Our Coding Club\n\nWe will see examples of how to use multiple regression on real data in this tutorial. To start with, multiple regression can come in multiple forms. The first is additive and the second is interactive. Let’s look at an example.\n\n\n\n\n1.) Using regression to understand the relationship between weight and surface in corals – An example\n\nPrepare dataSimple lminteractive regressionComparing additive to interactive\n\n\nRead in the data\n\nbz&lt;-read_csv('https://raw.githubusercontent.com/jbaumann3/Belize-RT-Baumann-et-al-2021/main/rt_master_data.csv')\n\nRows: 1064 Columns: 36\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (14): timepoint, bw_date, ID, species, colony, colony_ID, home_reef, tra...\ndbl (22): coralID, parent colony length, parent colony width, parent colony ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nMake surface area into one word for ease of use or make a new column with a better title. up to you!\n\nbz$surface_area&lt;-bz$`surface area`\nhead(bz)\n\n# A tibble: 6 × 37\n  timepoint bw_date    ID       species colony coralID colony_ID home_reef\n  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 T0        12/11/2017 PSTR 1 1 PSTR    P1           1 NSP1      NS       \n2 T0        12/11/2017 PSTR 1 2 PSTR    P1           2 NSP1      NS       \n3 T0        12/11/2017 PSTR 1 3 PSTR    P1           3 NSP1      NS       \n4 T0        12/11/2017 PSTR 1 4 PSTR    P1           4 NSP1      NS       \n5 T0        12/11/2017 PSTR 1 5 PSTR    P1           5 NSP1      NS       \n6 T0        12/11/2017 PSTR 1 6 PSTR    P1           6 NSP1      NS       \n# ℹ 29 more variables: transplant_reef &lt;chr&gt;, collection_site &lt;chr&gt;,\n#   transplant_site &lt;chr&gt;, transplant &lt;chr&gt;, `pale or bleached` &lt;chr&gt;,\n#   `partial mortality` &lt;chr&gt;, mortality &lt;chr&gt;, `parent colony length` &lt;dbl&gt;,\n#   `parent colony width` &lt;dbl&gt;, `parent colony height` &lt;dbl&gt;,\n#   `parent colony area` &lt;dbl&gt;, `parent colony volume` &lt;dbl&gt;, bw &lt;dbl&gt;,\n#   bw_sd &lt;dbl&gt;, bw_se &lt;dbl&gt;, `surface area` &lt;dbl&gt;, sa_sd &lt;dbl&gt;, sa_se &lt;dbl&gt;,\n#   chla &lt;dbl&gt;, symb_den &lt;dbl&gt;, symb_sd &lt;dbl&gt;, symb_se &lt;dbl&gt;, carbs_sa &lt;dbl&gt;, …\n\n\n\n\nWe will first run a simple lm to check the relationship between weight (bw) and surface are for each of our two species, separately.\nWe will start with SSID (one of our species)\n\n#simple lm surface area x bw (for SSID)\nssid&lt;- bz |&gt;\n  filter(species=='SSID')\n\nggplot(ssid, aes(x=surface_area, y=bw))+\n  geom_point()+\n  theme_classic()+\n  geom_smooth(method='lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 62 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 62 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n#p=2*10-16, R2=.3439-&gt; SO, not bad. But a lot of spread in the data. \n\nlm1&lt;-lm(bw~surface_area, data=ssid)\nsummary(lm1)\n\n\nCall:\nlm(formula = bw ~ surface_area, data = ssid)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-24.150  -5.683  -0.730   4.351  32.856 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   8.39205    0.90971   9.225   &lt;2e-16 ***\nsurface_area  0.77870    0.04951  15.727   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.858 on 469 degrees of freedom\n  (62 observations deleted due to missingness)\nMultiple R-squared:  0.3453,    Adjusted R-squared:  0.3439 \nF-statistic: 247.3 on 1 and 469 DF,  p-value: &lt; 2.2e-16\n\n\nHere we can see that the R-squared is 0.344 and the p-value is &lt;0.05, so we have a significant positive relationship between X and Y, though there is considerable spread.\nAnd now for PSTR (the other species)\n\n#simple lm surface area x bw (for PSTR)\npstr&lt;- bz |&gt;\n  filter(species=='PSTR')\n\nggplot(pstr, aes(x=surface_area, y=bw))+\n  geom_point()+\n  theme_classic()+\n  geom_smooth(method='lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 15 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 15 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n#p=2*10-16, R2=.3439-&gt; SO, not bad. But a lot of spread in the data. \n\nlm2&lt;-lm(bw~surface_area, data=pstr)\nsummary(lm2)\n\n\nCall:\nlm(formula = bw ~ surface_area, data = pstr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.5937  -4.1412  -0.5371   3.0975  29.7465 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.21829    0.62256   6.776  3.4e-11 ***\nsurface_area  0.84963    0.02665  31.877  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.349 on 514 degrees of freedom\n  (15 observations deleted due to missingness)\nMultiple R-squared:  0.6641,    Adjusted R-squared:  0.6634 \nF-statistic:  1016 on 1 and 514 DF,  p-value: &lt; 2.2e-16\n\n\nHere we see that the lm is better of PSTR (better fit because of the higher R-squared). From these analyses, we can generally conclude that there is a relationship between surface area and weight, but that it maybe varies by species.\n## additive regression Since we are interested in considering the effect of species on this relationship quantitatively, we need to run a regression model that include species in the model! We can do in two ways– an additive model or an interactive model. Let’s start with additive\n\nlm3&lt;-lm(bw~surface_area+species, data=bz)\nsummary(lm3)\n\n\nCall:\nlm(formula = bw ~ surface_area + species, data = bz)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-24.888  -4.955  -0.625   3.795  32.245 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.67334    0.60574   7.715 2.96e-14 ***\nsurface_area  0.82782    0.02485  33.319  &lt; 2e-16 ***\nspeciesSSID   2.89076    0.46404   6.230 6.92e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.111 on 984 degrees of freedom\n  (77 observations deleted due to missingness)\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.5294 \nF-statistic: 555.5 on 2 and 984 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s hold off on interpretation until we make a graph. Here, I am showing you a REALLY POWERFUL trick, which is using the augment() function in the broom package to pass model fits directly to ggplot.\n\nlm3g&lt;-lm3 %&gt;% \n  augment() %&gt;%\n  ggplot(aes(x=surface_area, y=bw, color=species))+\n  geom_point(alpha=0.3)+\n  geom_line(aes(y=.fitted), size=1)+\n  theme_classic()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nlm3g\n\n\n\n\nWe can view tables of our multiple regression data in similar ways to how we view our simple lm\n\nsummary(lm3) #check R2 and p-value! How well does the model fit?\n\n\nCall:\nlm(formula = bw ~ surface_area + species, data = bz)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-24.888  -4.955  -0.625   3.795  32.245 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.67334    0.60574   7.715 2.96e-14 ***\nsurface_area  0.82782    0.02485  33.319  &lt; 2e-16 ***\nspeciesSSID   2.89076    0.46404   6.230 6.92e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.111 on 984 degrees of freedom\n  (77 observations deleted due to missingness)\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.5294 \nF-statistic: 555.5 on 2 and 984 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm3)$coefficient #just the coef table from the summary!\n\n              Estimate Std. Error   t value      Pr(&gt;|t|)\n(Intercept)  4.6733416 0.60574157  7.715075  2.957752e-14\nsurface_area 0.8278243 0.02484574 33.318557 1.455258e-163\nspeciesSSID  2.8907561 0.46404112  6.229526  6.923713e-10\n\n\nHowever, you may find these difficult to interpret. We still see our R squared and p-value for the model itself, which is still important. But, we also have a more complex coefficients table to read. This table shows us an intercept and the effects of surface area and speciesSSID. Where is PSTR? Confusing, huh? In R, the Intercept is something called a ‘base case for comparison’ in which R takes the first category (in alphabetical order) in each categorical variable and isolates it. Here, we only have 1 categorical variable (species) and PSTR comes first. So, intercept is showing us the model for bw~surface area within PSTR only. The ESTIMATE is the y intercept of the PSTR model. 0.82782 is the slope of surface area for the intercept model. And 2.89076 is the offset between the PSTR model and the SSID model. Since it is positive, the offset of the intercepts is 2.89076. Thus, the SSID line should be HIGHER on the y-axis.\nNotably, since this is an additive model, the slopes of the lines for each category should ALWAYS be parallel. This is also called a parallel slopes model. Let’s look at the graph (above) to confirm!\n\nFinally, since we understand ANOVA better, we might try this approach\n\nanova(lm3) # an ANOVA table of our lm\n\nAnalysis of Variance Table\n\nResponse: bw\n              Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nsurface_area   1  54224   54224 1072.239 &lt; 2.2e-16 ***\nspecies        1   1962    1962   38.807 6.924e-10 ***\nResiduals    984  49761      51                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconfint(lm3) #CIs for our model predictors!\n\n                 2.5 %   97.5 %\n(Intercept)  3.4846479 5.862035\nsurface_area 0.7790675 0.876581\nspeciesSSID  1.9801321 3.801380\n\n\nThe confint shows us 95% confidence intervals for our model predictors, which is something we will learn about a little later.\n\n\nNow, sometimes we care about the effects of 2 variables at the same time and not only how 2 variables (separately) impact our response variable. In this case, we need interactive regression…\nInteractive models include multiplication in the formula instead of addition. In interactive models, both the slope and y intercept of the model can vary, so the slopes will no longer be parallel (automatically). Let’s look at the interactive model of the same data we just used.\n\nlm4&lt;-lm(bw~surface_area*species, data=bz)\nsummary(lm4)\n\n\nCall:\nlm(formula = bw ~ surface_area * species, data = bz)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-24.150  -4.871  -0.570   3.781  32.856 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               4.21829    0.69708   6.051 2.04e-09 ***\nsurface_area              0.84963    0.02984  28.470  &lt; 2e-16 ***\nspeciesSSID               4.17376    1.07855   3.870 0.000116 ***\nsurface_area:speciesSSID -0.07092    0.05383  -1.318 0.187925    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.109 on 983 degrees of freedom\n  (77 observations deleted due to missingness)\nMultiple R-squared:  0.5311,    Adjusted R-squared:  0.5297 \nF-statistic: 371.2 on 3 and 983 DF,  p-value: &lt; 2.2e-16\n\n\nAnd the other tables\n\nanova(lm4)\n\nAnalysis of Variance Table\n\nResponse: bw\n                      Df Sum Sq Mean Sq   F value    Pr(&gt;F)    \nsurface_area           1  54224   54224 1073.0408 &lt; 2.2e-16 ***\nspecies                1   1962    1962   38.8360 6.828e-10 ***\nsurface_area:species   1     88      88    1.7362    0.1879    \nResiduals            983  49674      51                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconfint(lm4)\n\n                              2.5 %     97.5 %\n(Intercept)               2.8503641 5.58621772\nsurface_area              0.7910625 0.90819063\nspeciesSSID               2.0572380 6.29028297\nsurface_area:speciesSSID -0.1765531 0.03470322\n\nget_regression_table(lm4)\n\n# A tibble: 4 × 7\n  term                    estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept                  4.22      0.697      6.05   0        2.85     5.59 \n2 surface_area               0.85      0.03      28.5    0        0.791    0.908\n3 species: SSID              4.17      1.08       3.87   0        2.06     6.29 \n4 surface_area:speciesSS…   -0.071     0.054     -1.32   0.188   -0.177    0.035\n\n\nWe read this regression table the same way we read the additive model one. Except this time, we also have an interactive term. We are interested in how surface area impacts bw, how species impacts bw, and how the relationship between surface area (x) and bw (y) varies by species – basically, do these two regression lines look different in slope, intercept, or both? The last line of the get_regression_table shows us the offset of the slope of surface area in SSID relative to PSTR (the SSID slope is lower). Notably, the p-value is not significant here, so this interaction does not have a significant effect.\nFinally, the graph\n\nlm4g&lt;-lm4 %&gt;% \n  augment() %&gt;%\n  ggplot(aes(x=surface_area, y=bw, color=species))+\n  geom_point(alpha=0.3)+\n  geom_line(aes(y=.fitted),size=1)+\n  theme_classic()\n\nlm4g\n\n\n\n\n\n\nOur interactive model suggests that the interaction term does not have a significant effect. Thus, the additive and interactive models should not really signifiacntly differ. AND, the relationship between x and y would then not change based on species.\nLet’s compare the graphs side by side to confirm\n\nlm3g+lm4g\n\n\n\n\nAnd if you prefer to have them stacked…\n\nlm3g/lm4g\n\n\n\n\nRemember that our eyes are often the most valuable tool! We can see that there are only slight differences between these two models and that there are only slight differences between the two species (if that). It turns out that there is a difference between the two species and there is an effect of surface area on bw, BUT there is not a significant interaction (the relationship between surface area and bw does not differ by species). Given that the two models are essentially showing us the same thing, but the additive model is simpler (less complexity because there is not an interaction to calcualte), we would generally prefer to pick the less complex option.\nWhile this is a great rule of thumb, we can also do model selection quantitatively, which we will get to in our model selection section later on…\n\n\n\n\n\n\nModel fit assessment and model selection\n\nChecking modelstest assumptionscomparing modelsA 95% CI plot of model coefficients\n\n\nHere, we want to know how well the models represent the data. We need: 1. The R2 value of the models (closer to 1 is best) 2. The p-value of the models (&lt;0.05 is required for there to be a relationship) 3. We can calculate residual standard error. Lower = more accurate!\nThe R2 and p are in the summary! Below is the formula for RMSE\n\nsummary(lm3)\n\n\nCall:\nlm(formula = bw ~ surface_area + species, data = bz)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-24.888  -4.955  -0.625   3.795  32.245 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.67334    0.60574   7.715 2.96e-14 ***\nsurface_area  0.82782    0.02485  33.319  &lt; 2e-16 ***\nspeciesSSID   2.89076    0.46404   6.230 6.92e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.111 on 984 degrees of freedom\n  (77 observations deleted due to missingness)\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.5294 \nF-statistic: 555.5 on 2 and 984 DF,  p-value: &lt; 2.2e-16\n\n#R2=0.5294, p&lt;0.001\n\nsummary(lm4)\n\n\nCall:\nlm(formula = bw ~ surface_area * species, data = bz)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-24.150  -4.871  -0.570   3.781  32.856 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               4.21829    0.69708   6.051 2.04e-09 ***\nsurface_area              0.84963    0.02984  28.470  &lt; 2e-16 ***\nspeciesSSID               4.17376    1.07855   3.870 0.000116 ***\nsurface_area:speciesSSID -0.07092    0.05383  -1.318 0.187925    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.109 on 983 degrees of freedom\n  (77 observations deleted due to missingness)\nMultiple R-squared:  0.5311,    Adjusted R-squared:  0.5297 \nF-statistic: 371.2 on 3 and 983 DF,  p-value: &lt; 2.2e-16\n\n#R2=0.5297, p&lt;0.01\n\n#RSE: &lt;- LOWER RSE= more accurate the model!\nsigma(lm3)\n\n[1] 7.111301\n\nsigma(lm4)\n\n[1] 7.108642\n\nmean(bz$bw, na.rm=T)\n\n[1] 21.54501\n\nsigma(lm3)/mean(bz$bw, na.rm=T)\n\n[1] 0.3300672\n\n#0.3300672\n\nsigma(lm4)/mean(bz$bw, na.rm=T)\n\n[1] 0.3299438\n\n#0.3299438\n\nWhen we look at R2, p, and RMSE, we see that these two models are nearly identical! Not suprising. As we discussed earlier, the “simpler” model is typically the choice when the models are about the same.\nWe can also get this information from the performance package using model_performance(). This function tells us many things, including R2 and RMSE. We will discuss the rest of this later\n\nmodel_performance(lm3)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n6678.347 | 6678.387 | 6697.925 | 0.530 |     0.529 | 7.100 | 7.111\n\n\n\nmodel_performance(lm4)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n6678.605 | 6678.666 | 6703.078 | 0.531 |     0.530 | 7.094 | 7.109\n\n\nHere we see two tables of MANY values. / The first is AIC or Akaike Information Criterion. AIC is an estimator of prediction error and is generally thought of as a way to rank the relative quality of statistical models. LOWER AIC is best. In our case, the AIC scores are almost exactly the same! AICc is a corrected version of AIC that is better suited for use with small sample sizes. BIC is Bayesian Information Criterion, another way to assess models. Again, lower is better. In our case, we seee that lm4 has a very slightly lower BIC score, but it likely isn’t enough to matter. **R2 and adjusted R2* are familiar to use. RMSE and Sigma are similar to what we just calculated by hand. RMSE = Root mean squared error, which is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data and is a difference between observed data and models predicted values. Sigma is the residual standard deviation. More deviation is less ideal.\nWe are pretty new to stats, so it is great to have some quick ways to evaluate a model, BUT, we are likely to struggle to interpret this output. Luckily, we can compare performance pretty easily…\n\n\nWe can use check_model to check our assumptions. We care most about collinearity, homogeneity of variance, and outliers (influential observations). Collinearity is often assess with Variance Inflation Factor (VIF). VIF &gt;5 is a potential issue and VIF &gt;10 is a problem that means the model is too complex!\n\nmodel_performance(lm3)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n6678.347 | 6678.387 | 6697.925 | 0.530 |     0.529 | 7.100 | 7.111\n\ncheck_model(lm3)#things look good, including low collinearity (VIF)\n\n\n\nvif(lm3)\n\nsurface_area      species \n      1.0485       1.0485 \n\n\nHere, we are good. Let’s check on lm4\n\n\nmodel_performance(lm4)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n6678.605 | 6678.666 | 6703.078 | 0.531 |     0.530 | 7.094 | 7.109\n\ncheck_model(lm4) #things look good, but we have VIF that look a little high...\n\n\n\ncheck_collinearity(lm4) #a table of collinearity results - we see a few VIF above 5..\n\n# Check for Multicollinearity\n\nLow Correlation\n\n         Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n surface_area 1.51 [1.40, 1.66]         1.23      0.66     [0.60, 0.71]\n\nModerate Correlation\n\n                 Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n              species 5.67 [5.07, 6.36]         2.38      0.18     [0.16, 0.20]\n surface_area:species 5.45 [4.88, 6.11]         2.34      0.18     [0.16, 0.20]\n\nvif(lm4) #gives us a simpler table---again, some VIF above 5. That is cause for concern (model is overfit/too complex)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n        surface_area              species surface_area:species \n            1.513859             5.668388             5.454545 \n\n\nFrom these results, we would say that lm4 is perhaps overfit or too complex, which would cause us to hesitate to use it. BUT, for the sake of learning, let’s keep going and compare the performance of the models.\n\n\n\ncompare_performance(lm3,lm4, rank=TRUE)\n\n# Comparison of Model Performance Indices\n\nName | Model |    R2 | R2 (adj.) |  RMSE | Sigma | AIC weights | AICc weights | BIC weights | Performance-Score\n---------------------------------------------------------------------------------------------------------------\nlm4  |    lm | 0.531 |     0.530 | 7.094 | 7.109 |       0.468 |        0.465 |       0.071 |            57.14%\nlm3  |    lm | 0.530 |     0.529 | 7.100 | 7.111 |       0.532 |        0.535 |       0.929 |            42.86%\n\n\nHere, the performance package has given us a ranking and has chosen Model 4, the more complex model, ahead of Model 3. We did some sleuthing earlier and chose model 3 over model 4 because it was simpler and they showed the same results.\nInteresting, huh? So, what is right? Which model do we choose? You can make a case for either decision and being able to back up your decision with data is the key. That said, since there really is not a difference and model 3 is simpler, I’d stick with it. EVEN if the model evaluation materials I have suggest that model 4 could be ever so slightly “better.”\nKeep in mind that stats are tool! A useful one. But, without being driven by human decision making and our hypotheses, they can become unnecessarily complex. If you have a few models and one of them fits your hypotheses and allows you to evaluate them and the others do not, that is probably the “best” model for you even if it is not the “best” model statistically. As long as it is a valid model, it is worth using. There are a lot of exceptions and special circumstances to consider and I am sure you will encounter these as you progress in science. For now, we have at least learned a simple pipeline for doing regression!\nIf we remember, the VIF are a little high in model 4– just above 5 is really not THAT high, but it may be some cause for concern if we have low sample sizes. So, we might pick lm3 instead on that basis alone.\n\n\nSometimes we want to visualize the effects of our model predictors. This is especially key when we have super complex models with a lot of variables! This is not one of those cases, but you will encounter these later in your life (possible in your assignment here!).\ncombine data! Use tidy() from the broom package to get nice neat dataframes from models\n\n#make a neat data frame\ncoefs&lt;-tidy(lm3, quick=FALSE)\ncoefs\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     4.67     0.606       7.72 2.96e- 14\n2 surface_area    0.828    0.0248     33.3  1.46e-163\n3 speciesSSID     2.89     0.464       6.23 6.92e- 10\n\n#make confint into a dataframe\nci&lt;-data.table(confint(lm3), keep.rownames='term')\nci\n\n           term     2.5 %   97.5 %\n         &lt;char&gt;     &lt;num&gt;    &lt;num&gt;\n1:  (Intercept) 3.4846479 5.862035\n2: surface_area 0.7790675 0.876581\n3:  speciesSSID 1.9801321 3.801380\n\n#bind the coefs data frame with the ci dataframe\ncidf&lt;-cbind(coefs,ci)\ncidf\n\n          term  estimate  std.error statistic       p.value         term\n1  (Intercept) 4.6733416 0.60574157  7.715075  2.957752e-14  (Intercept)\n2 surface_area 0.8278243 0.02484574 33.318557 1.455258e-163 surface_area\n3  speciesSSID 2.8907561 0.46404112  6.229526  6.923713e-10  speciesSSID\n      2.5 %   97.5 %\n1 3.4846479 5.862035\n2 0.7790675 0.876581\n3 1.9801321 3.801380\n\ncolnames(cidf)\n\n[1] \"term\"      \"estimate\"  \"std.error\" \"statistic\" \"p.value\"   \"term\"     \n[7] \"2.5 %\"     \"97.5 %\"   \n\n#remove a column we don't need\ncidf&lt;-cidf[,-6]\n\n#rename the % columnns to uppper and lower because it is easier to use\ncidf&lt;- cidf %&gt;%\n  rename(\"lower\"=\"2.5 %\",\n         \"upper\"=\"97.5 %\")\n\ncidf\n\n          term  estimate  std.error statistic       p.value     lower    upper\n1  (Intercept) 4.6733416 0.60574157  7.715075  2.957752e-14 3.4846479 5.862035\n2 surface_area 0.8278243 0.02484574 33.318557 1.455258e-163 0.7790675 0.876581\n3  speciesSSID 2.8907561 0.46404112  6.229526  6.923713e-10 1.9801321 3.801380\n\n#make term into a factor\ncidf$term=as.factor(cidf$term)\n\nNow make a plot!\n\nggplot(data=cidf, aes(x=estimate, y=term))+\n  geom_vline(xintercept = 0, linetype=2)+\n  geom_point(size=3)+\n  geom_errorbarh(aes(xmax=lower, xmin=upper),height=0.2)+\n  theme_classic()\n\n\n\n\nNote that there are many ways to build a dataframe and plot for these. This is just one example. What we see in this kind of plot, called a coefficient or ‘coef’ plot, is the estimated effect of each term of the model. A trick here is that some of these estimates (from our coefficient table) are slopes and others may be intercepts. You can see them with 95% confidence intervals displayed here. Generally, there is a significant effect if the mean and error do not overlap with the 0 line (check this with p values from the table). I wonder if lm4 looks different?\n\nCoef plot for lm4\n\n#make a neat data frame\ncoefs&lt;-tidy(lm4, quick=FALSE)\ncoefs\n\n# A tibble: 4 × 5\n  term                     estimate std.error statistic   p.value\n  &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)                4.22      0.697       6.05 2.04e-  9\n2 surface_area               0.850     0.0298     28.5  1.67e-130\n3 speciesSSID                4.17      1.08        3.87 1.16e-  4\n4 surface_area:speciesSSID  -0.0709    0.0538     -1.32 1.88e-  1\n\n#make confint into a dataframe\nci&lt;-data.table(confint(lm4), keep.rownames='term')\nci\n\n                       term      2.5 %     97.5 %\n                     &lt;char&gt;      &lt;num&gt;      &lt;num&gt;\n1:              (Intercept)  2.8503641 5.58621772\n2:             surface_area  0.7910625 0.90819063\n3:              speciesSSID  2.0572380 6.29028297\n4: surface_area:speciesSSID -0.1765531 0.03470322\n\n#bind the coefs data frame with the ci dataframe\ncidf&lt;-cbind(coefs,ci)\ncidf\n\n                      term    estimate  std.error statistic       p.value\n1              (Intercept)  4.21829090 0.69707532  6.051413  2.039310e-09\n2             surface_area  0.84962656 0.02984339 28.469509 1.666402e-130\n3              speciesSSID  4.17376049 1.07854863  3.869794  1.161104e-04\n4 surface_area:speciesSSID -0.07092496 0.05382656 -1.317657  1.879254e-01\n                      term      2.5 %     97.5 %\n1              (Intercept)  2.8503641 5.58621772\n2             surface_area  0.7910625 0.90819063\n3              speciesSSID  2.0572380 6.29028297\n4 surface_area:speciesSSID -0.1765531 0.03470322\n\ncolnames(cidf)\n\n[1] \"term\"      \"estimate\"  \"std.error\" \"statistic\" \"p.value\"   \"term\"     \n[7] \"2.5 %\"     \"97.5 %\"   \n\n#remove a column we don't need\ncidf&lt;-cidf[,-6]\n\n#rename the % columnns to uppper and lower because it is easier to use\ncidf&lt;- cidf %&gt;%\n  rename(\"lower\"=\"2.5 %\",\n         \"upper\"=\"97.5 %\")\n\ncidf\n\n                      term    estimate  std.error statistic       p.value\n1              (Intercept)  4.21829090 0.69707532  6.051413  2.039310e-09\n2             surface_area  0.84962656 0.02984339 28.469509 1.666402e-130\n3              speciesSSID  4.17376049 1.07854863  3.869794  1.161104e-04\n4 surface_area:speciesSSID -0.07092496 0.05382656 -1.317657  1.879254e-01\n       lower      upper\n1  2.8503641 5.58621772\n2  0.7910625 0.90819063\n3  2.0572380 6.29028297\n4 -0.1765531 0.03470322\n\n#make term into a factor\ncidf$term=as.factor(cidf$term)\n\nggplot(data=cidf, aes(x=estimate, y=term))+\n  geom_vline(xintercept = 0, linetype=2)+\n  geom_point(size=3)+\n  geom_errorbarh(aes(xmax=lower, xmin=upper),height=0.2)+\n  theme_classic()\n\n\n\n\nAs before, the interaction term does not have a significant effect! While it can be a bit hard to interpret these, at least we can quickly see a visual of statistical significance. When we have 10 terms in out model, this is super helpful!\n\n\n\n\n\nLab 7 Assignment:\n\n1.) Load the penguins data from palmerpenguins and build an additive and interactive model for the effects of species and sex on bill_length_mm.\n2. Generate model summary tables using summary() and get_regression_table. Interpret these.\n2.) Make representative graphs for each model and plot them side by side to compare them visually. Make a visual assessment of the models. Do your visual assessments agree with your stats tables? Why or why not?\n3.) Make coef plots for each model. How do these compare to your tables?\n4.) Head over to the TidyTuesdsay database and grab a dataset to do more practice with. Your data MUST have at least 2 numerical variables and at least 2 useful categorical variables. Show me clear evidence that this is the case in your data. DO NOT just type the name of the df here. I don’t need to see 100 pages of data, just show me that you have a few useful categorical and numrical variables. HINT if you are trying to use a dataset that is all of the lines from a tv show or something similar, this is probably not appropriate data…\n5.) Pick a response variable (must be numeric) and build a series of models using a top-down modeling approach. You will start by using all of your variables (you have 4, 1 is the response, so the other 3 are explanatory- likely 2 categorical and 1 numeric). Your “top” model will be y~ xab, you will then replace one * with + in an iterative way until you reach y~x+a+b. For each model, generate the appropriate table(s), a useful graph using augment() (if possible), and a coef plot. You will need to CHECK each model to see if the models violate assumptions or not (check_model() is from the performance package, and is pretty easy to use– it generates a series of graphs of each assumption check and tells you what to look for in the subtitle of the graph). Do a visual assumption check for each assumption. If a model violates assumptions and is invalid, you should not use it in your model performance assessment. Each model that passes a model check needs to be compared to the others. Which model is the best fit? Is this the model you would use? Why or why not?\n6.) Render and submit on Lyceum"
  },
  {
    "objectID": "Lab_7_multiple_regression.html#get-our-penguin-data-ready",
    "href": "Lab_7_multiple_regression.html#get-our-penguin-data-ready",
    "title": "Lab 7: Multiple Regression",
    "section": "",
    "text": "penguins &lt;- palmerpenguins::penguins\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\npenguins&lt;-drop_na(penguins)\n\npenguins$year=as.factor(penguins$year) #we are interested in year as a grouping/categorical variable so we will make it a factor"
  },
  {
    "objectID": "Lab_7_multiple_regression.html#essentials",
    "href": "Lab_7_multiple_regression.html#essentials",
    "title": "Lab 7: Multiple Regression and Bootstrapping",
    "section": "Essentials",
    "text": "Essentials\n\n1.) Load data ‘soccer’ from tidytuesday\nfind soccer here\nAfter you load the data, record which variables are categorical and which are numeric.\n\n\n2.) Let’s consider the effects of home team shots (HS), home team (HomeTeam), and home team fouls (HF) on home team goals (full time home goals). Build a fully interactive multiple linear regression model. Assess model fit and then model assumptions. How well does the model fit the data? Is the model valid?\n\n\n3.) Run through a top-down modeling approach to find the best fit model! Be sure to check assumptions after each change and compare performance. What model is the best fit?\n\n\n4.) After identifying the best fit model, build the appropriate graph! See our multiple regression tutorial. Next, Build a coef plot for the model. Using patchwork, show me a 2-panel figure with the coef plot and the graph for the model"
  },
  {
    "objectID": "Lab_7_multiple_regression.html#depth",
    "href": "Lab_7_multiple_regression.html#depth",
    "title": "Lab 7: Multiple Regression and Bootstrapping",
    "section": "Depth",
    "text": "Depth\n\n1.) Bootstrap the coef plot from Essential #4, above.\n\n\n2.) Calculate means and 95% CIs of full time home goals and full time away goals (using bootstrapping). Plot the results and interpret the plot (is there a home advantage or not?)\n\n\n3.) Add raw data behind your 95% CI plot above!"
  },
  {
    "objectID": "Lab_7_multiple_regression.html#load-data-soccer-from-tidytuesday",
    "href": "Lab_7_multiple_regression.html#load-data-soccer-from-tidytuesday",
    "title": "Lab 7: Multiple Regression",
    "section": "1.) Load data ‘soccer’ from tidytuesday",
    "text": "1.) Load data ‘soccer’ from tidytuesday\nfind soccer here\nAfter you load the data, record which variables are categorical and which are numeric."
  },
  {
    "objectID": "Lab_7_multiple_regression.html#lets-consider-the-effects-of-home-team-shots-hs-home-team-hometeam-and-home-team-fouls-hf-on-home-team-goals-full-time-home-goals.-build-a-fully-interactive-multiple-linear-regression-model.-assess-model-fit-and-then-model-assumptions.-how-well-does-the-model-fit-the-data-is-the-model-valid",
    "href": "Lab_7_multiple_regression.html#lets-consider-the-effects-of-home-team-shots-hs-home-team-hometeam-and-home-team-fouls-hf-on-home-team-goals-full-time-home-goals.-build-a-fully-interactive-multiple-linear-regression-model.-assess-model-fit-and-then-model-assumptions.-how-well-does-the-model-fit-the-data-is-the-model-valid",
    "title": "Lab 7: Multiple Regression",
    "section": "2.) Let’s consider the effects of home team shots (HS), home team (HomeTeam), and home team fouls (HF) on home team goals (full time home goals). Build a fully interactive multiple linear regression model. Assess model fit and then model assumptions. How well does the model fit the data? Is the model valid?",
    "text": "2.) Let’s consider the effects of home team shots (HS), home team (HomeTeam), and home team fouls (HF) on home team goals (full time home goals). Build a fully interactive multiple linear regression model. Assess model fit and then model assumptions. How well does the model fit the data? Is the model valid?"
  },
  {
    "objectID": "Lab_7_multiple_regression.html#run-through-a-top-down-modeling-approach-to-find-the-best-fit-model-be-sure-to-check-assumptions-after-each-change-and-compare-performance.-what-model-is-the-best-fit",
    "href": "Lab_7_multiple_regression.html#run-through-a-top-down-modeling-approach-to-find-the-best-fit-model-be-sure-to-check-assumptions-after-each-change-and-compare-performance.-what-model-is-the-best-fit",
    "title": "Lab 7: Multiple Regression",
    "section": "3.) Run through a top-down modeling approach to find the best fit model! Be sure to check assumptions after each change and compare performance. What model is the best fit?",
    "text": "3.) Run through a top-down modeling approach to find the best fit model! Be sure to check assumptions after each change and compare performance. What model is the best fit?"
  },
  {
    "objectID": "Lab_7_multiple_regression.html#after-identifying-the-best-fit-model-build-the-appropriate-graph-see-our-multiple-regression-tutorial.-next-build-a-coef-plot-for-the-model.-using-patchwork-show-me-a-2-panel-figure-with-the-coef-plot-and-the-graph-for-the-model",
    "href": "Lab_7_multiple_regression.html#after-identifying-the-best-fit-model-build-the-appropriate-graph-see-our-multiple-regression-tutorial.-next-build-a-coef-plot-for-the-model.-using-patchwork-show-me-a-2-panel-figure-with-the-coef-plot-and-the-graph-for-the-model",
    "title": "Lab 7: Multiple Regression",
    "section": "4.) After identifying the best fit model, build the appropriate graph! See our multiple regression tutorial. Next, Build a coef plot for the model. Using patchwork, show me a 2-panel figure with the coef plot and the graph for the model",
    "text": "4.) After identifying the best fit model, build the appropriate graph! See our multiple regression tutorial. Next, Build a coef plot for the model. Using patchwork, show me a 2-panel figure with the coef plot and the graph for the model"
  },
  {
    "objectID": "Lab_6_multiple_regression.html",
    "href": "Lab_6_multiple_regression.html",
    "title": "Lab 6: Multiple Regression",
    "section": "",
    "text": "Lab 6: Multiple Regression and Model Selection\n\nload packages\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(data.table)\nlibrary(performance)\nlibrary(patchwork)\nlibrary(car) \nlibrary(rsample)\nlibrary(moderndive) #helps us make useful regression tables\n\nMultiple Regression is a powerful tool that allows us to investigate the effects of multiple varibles on our response variable of choice. This is great for real world data, as it is almost never the case that we have the effect of a single variable on another. Instead, we often have so many that we need to dig into mixed effects modeling, which is the next step up from multiple regression.\nMy favorite mixed models selection tutorial: Our Coding Club\n\nWe will see examples of how to use multiple regression on real data in this tutorial. To start with, multiple regression can come in multiple forms. The first is additive and the second is interactive. Let’s look at an example.\n\n\n\n\n1.) Using regression to understand the relationship between weight and surface in corals – An example\n\nPrepare dataSimple lminteractive regressionComparing additive to interactive\n\n\nRead in the data\n\nbz&lt;-read_csv('https://raw.githubusercontent.com/jbaumann3/Belize-RT-Baumann-et-al-2021/main/rt_master_data.csv')\n\nRows: 1064 Columns: 36\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (14): timepoint, bw_date, ID, species, colony, colony_ID, home_reef, tra...\ndbl (22): coralID, parent colony length, parent colony width, parent colony ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nMake surface area into one word for ease of use or make a new column with a better title. up to you!\n\nbz$surface_area&lt;-bz$`surface area`\nhead(bz)\n\n# A tibble: 6 × 37\n  timepoint bw_date    ID       species colony coralID colony_ID home_reef\n  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 T0        12/11/2017 PSTR 1 1 PSTR    P1           1 NSP1      NS       \n2 T0        12/11/2017 PSTR 1 2 PSTR    P1           2 NSP1      NS       \n3 T0        12/11/2017 PSTR 1 3 PSTR    P1           3 NSP1      NS       \n4 T0        12/11/2017 PSTR 1 4 PSTR    P1           4 NSP1      NS       \n5 T0        12/11/2017 PSTR 1 5 PSTR    P1           5 NSP1      NS       \n6 T0        12/11/2017 PSTR 1 6 PSTR    P1           6 NSP1      NS       \n# ℹ 29 more variables: transplant_reef &lt;chr&gt;, collection_site &lt;chr&gt;,\n#   transplant_site &lt;chr&gt;, transplant &lt;chr&gt;, `pale or bleached` &lt;chr&gt;,\n#   `partial mortality` &lt;chr&gt;, mortality &lt;chr&gt;, `parent colony length` &lt;dbl&gt;,\n#   `parent colony width` &lt;dbl&gt;, `parent colony height` &lt;dbl&gt;,\n#   `parent colony area` &lt;dbl&gt;, `parent colony volume` &lt;dbl&gt;, bw &lt;dbl&gt;,\n#   bw_sd &lt;dbl&gt;, bw_se &lt;dbl&gt;, `surface area` &lt;dbl&gt;, sa_sd &lt;dbl&gt;, sa_se &lt;dbl&gt;,\n#   chla &lt;dbl&gt;, symb_den &lt;dbl&gt;, symb_sd &lt;dbl&gt;, symb_se &lt;dbl&gt;, carbs_sa &lt;dbl&gt;, …\n\n\n\n\nWe will first run a simple lm to check the relationship between weight (bw) and surface are for each of our two species, separately.\nWe will start with SSID (one of our species)\n\n#simple lm surface area x bw (for SSID)\nssid&lt;- bz |&gt;\n  filter(species=='SSID')\n\nggplot(ssid, aes(x=surface_area, y=bw))+\n  geom_point()+\n  theme_classic()+\n  geom_smooth(method='lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 62 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 62 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n#p=2*10-16, R2=.3439-&gt; SO, not bad. But a lot of spread in the data. \n\nlm1&lt;-lm(bw~surface_area, data=ssid)\nsummary(lm1)\n\n\nCall:\nlm(formula = bw ~ surface_area, data = ssid)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-24.150  -5.683  -0.730   4.351  32.856 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   8.39205    0.90971   9.225   &lt;2e-16 ***\nsurface_area  0.77870    0.04951  15.727   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.858 on 469 degrees of freedom\n  (62 observations deleted due to missingness)\nMultiple R-squared:  0.3453,    Adjusted R-squared:  0.3439 \nF-statistic: 247.3 on 1 and 469 DF,  p-value: &lt; 2.2e-16\n\n\nHere we can see that the R-squared is 0.344 and the p-value is &lt;0.05, so we have a significant positive relationship between X and Y, though there is considerable spread.\nAnd now for PSTR (the other species)\n\n#simple lm surface area x bw (for PSTR)\npstr&lt;- bz |&gt;\n  filter(species=='PSTR')\n\nggplot(pstr, aes(x=surface_area, y=bw))+\n  geom_point()+\n  theme_classic()+\n  geom_smooth(method='lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 15 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 15 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n#p=2*10-16, R2=.3439-&gt; SO, not bad. But a lot of spread in the data. \n\nlm2&lt;-lm(bw~surface_area, data=pstr)\nsummary(lm2)\n\n\nCall:\nlm(formula = bw ~ surface_area, data = pstr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.5937  -4.1412  -0.5371   3.0975  29.7465 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.21829    0.62256   6.776  3.4e-11 ***\nsurface_area  0.84963    0.02665  31.877  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.349 on 514 degrees of freedom\n  (15 observations deleted due to missingness)\nMultiple R-squared:  0.6641,    Adjusted R-squared:  0.6634 \nF-statistic:  1016 on 1 and 514 DF,  p-value: &lt; 2.2e-16\n\n\nHere we see that the lm is better of PSTR (better fit because of the higher R-squared). From these analyses, we can generally conclude that there is a relationship between surface area and weight, but that it maybe varies by species.\n## additive regression Since we are interested in considering the effect of species on this relationship quantitatively, we need to run a regression model that include species in the model! We can do in two ways– an additive model or an interactive model. Let’s start with additive\n\nlm3&lt;-lm(bw~surface_area+species, data=bz)\nsummary(lm3)\n\n\nCall:\nlm(formula = bw ~ surface_area + species, data = bz)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-24.888  -4.955  -0.625   3.795  32.245 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.67334    0.60574   7.715 2.96e-14 ***\nsurface_area  0.82782    0.02485  33.319  &lt; 2e-16 ***\nspeciesSSID   2.89076    0.46404   6.230 6.92e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.111 on 984 degrees of freedom\n  (77 observations deleted due to missingness)\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.5294 \nF-statistic: 555.5 on 2 and 984 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s hold off on interpretation until we make a graph. Here, I am showing you a REALLY POWERFUL trick, which is using the augment() function in the broom package to pass model fits directly to ggplot.\n\nlm3g&lt;-lm3 %&gt;% \n  augment() %&gt;%\n  ggplot(aes(x=surface_area, y=bw, color=species))+\n  geom_point(alpha=0.3)+\n  geom_line(aes(y=.fitted), size=1)+\n  theme_classic()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nlm3g\n\n\n\n\nWe can view tables of our multiple regression data in similar ways to how we view our simple lm\n\nsummary(lm3) #check R2 and p-value! How well does the model fit?\n\n\nCall:\nlm(formula = bw ~ surface_area + species, data = bz)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-24.888  -4.955  -0.625   3.795  32.245 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.67334    0.60574   7.715 2.96e-14 ***\nsurface_area  0.82782    0.02485  33.319  &lt; 2e-16 ***\nspeciesSSID   2.89076    0.46404   6.230 6.92e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.111 on 984 degrees of freedom\n  (77 observations deleted due to missingness)\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.5294 \nF-statistic: 555.5 on 2 and 984 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm3)$coefficient #just the coef table from the summary!\n\n              Estimate Std. Error   t value      Pr(&gt;|t|)\n(Intercept)  4.6733416 0.60574157  7.715075  2.957752e-14\nsurface_area 0.8278243 0.02484574 33.318557 1.455258e-163\nspeciesSSID  2.8907561 0.46404112  6.229526  6.923713e-10\n\n\nHowever, you may find these difficult to interpret. We still see our R squared and p-value for the model itself, which is still important. But, we also have a more complex coefficients table to read. This table shows us an intercept and the effects of surface area and speciesSSID. Where is PSTR? Confusing, huh? In R, the Intercept is something called a ‘base case for comparison’ in which R takes the first category (in alphabetical order) in each categorical variable and isolates it. Here, we only have 1 categorical variable (species) and PSTR comes first. So, intercept is showing us the model for bw~surface area within PSTR only. The ESTIMATE is the y intercept of the PSTR model. 0.82782 is the slope of surface area for the intercept model. And 2.89076 is the offset between the PSTR model and the SSID model. Since it is positive, the offset of the intercepts is 2.89076. Thus, the SSID line should be HIGHER on the y-axis.\nNotably, since this is an additive model, the slopes of the lines for each category should ALWAYS be parallel. This is also called a parallel slopes model. Let’s look at the graph (above) to confirm!\n\nFinally, since we understand ANOVA better, we might try this approach\n\nanova(lm3) # an ANOVA table of our lm\n\nAnalysis of Variance Table\n\nResponse: bw\n              Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nsurface_area   1  54224   54224 1072.239 &lt; 2.2e-16 ***\nspecies        1   1962    1962   38.807 6.924e-10 ***\nResiduals    984  49761      51                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconfint(lm3) #CIs for our model predictors!\n\n                 2.5 %   97.5 %\n(Intercept)  3.4846479 5.862035\nsurface_area 0.7790675 0.876581\nspeciesSSID  1.9801321 3.801380\n\n\nThe confint shows us 95% confidence intervals for our model predictors, which is something we will learn about a little later.\n\n\nNow, sometimes we care about the effects of 2 variables at the same time and not only how 2 variables (separately) impact our response variable. In this case, we need interactive regression…\nInteractive models include multiplication in the formula instead of addition. In interactive models, both the slope and y intercept of the model can vary, so the slopes will no longer be parallel (automatically). Let’s look at the interactive model of the same data we just used.\n\nlm4&lt;-lm(bw~surface_area*species, data=bz)\nsummary(lm4)\n\n\nCall:\nlm(formula = bw ~ surface_area * species, data = bz)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-24.150  -4.871  -0.570   3.781  32.856 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               4.21829    0.69708   6.051 2.04e-09 ***\nsurface_area              0.84963    0.02984  28.470  &lt; 2e-16 ***\nspeciesSSID               4.17376    1.07855   3.870 0.000116 ***\nsurface_area:speciesSSID -0.07092    0.05383  -1.318 0.187925    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.109 on 983 degrees of freedom\n  (77 observations deleted due to missingness)\nMultiple R-squared:  0.5311,    Adjusted R-squared:  0.5297 \nF-statistic: 371.2 on 3 and 983 DF,  p-value: &lt; 2.2e-16\n\n\nAnd the other tables\n\nanova(lm4)\n\nAnalysis of Variance Table\n\nResponse: bw\n                      Df Sum Sq Mean Sq   F value    Pr(&gt;F)    \nsurface_area           1  54224   54224 1073.0408 &lt; 2.2e-16 ***\nspecies                1   1962    1962   38.8360 6.828e-10 ***\nsurface_area:species   1     88      88    1.7362    0.1879    \nResiduals            983  49674      51                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconfint(lm4)\n\n                              2.5 %     97.5 %\n(Intercept)               2.8503641 5.58621772\nsurface_area              0.7910625 0.90819063\nspeciesSSID               2.0572380 6.29028297\nsurface_area:speciesSSID -0.1765531 0.03470322\n\nget_regression_table(lm4)\n\n# A tibble: 4 × 7\n  term                    estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept                  4.22      0.697      6.05   0        2.85     5.59 \n2 surface_area               0.85      0.03      28.5    0        0.791    0.908\n3 species: SSID              4.17      1.08       3.87   0        2.06     6.29 \n4 surface_area:speciesSS…   -0.071     0.054     -1.32   0.188   -0.177    0.035\n\n\nWe read this regression table the same way we read the additive model one. Except this time, we also have an interactive term. We are interested in how surface area impacts bw, how species impacts bw, and how the relationship between surface area (x) and bw (y) varies by species – basically, do these two regression lines look different in slope, intercept, or both? The last line of the get_regression_table shows us the offset of the slope of surface area in SSID relative to PSTR (the SSID slope is lower). Notably, the p-value is not significant here, so this interaction does not have a significant effect.\nFinally, the graph\n\nlm4g&lt;-lm4 %&gt;% \n  augment() %&gt;%\n  ggplot(aes(x=surface_area, y=bw, color=species))+\n  geom_point(alpha=0.3)+\n  geom_line(aes(y=.fitted),size=1)+\n  theme_classic()\n\nlm4g\n\n\n\n\n\n\nOur interactive model suggests that the interaction term does not have a significant effect. Thus, the additive and interactive models should not really signifiacntly differ. AND, the relationship between x and y would then not change based on species.\nLet’s compare the graphs side by side to confirm\n\nlm3g+lm4g\n\n\n\n\nAnd if you prefer to have them stacked…\n\nlm3g/lm4g\n\n\n\n\nRemember that our eyes are often the most valuable tool! We can see that there are only slight differences between these two models and that there are only slight differences between the two species (if that). It turns out that there is a difference between the two species and there is an effect of surface area on bw, BUT there is not a significant interaction (the relationship between surface area and bw does not differ by species). Given that the two models are essentially showing us the same thing, but the additive model is simpler (less complexity because there is not an interaction to calcualte), we would generally prefer to pick the less complex option.\nWhile this is a great rule of thumb, we can also do model selection quantitatively, which we will get to in our model selection section later on…\n\n\n\n\n\n\nModel fit assessment and model selection\n\nChecking modelstest assumptionscomparing modelsA 95% CI plot of model coefficients\n\n\nHere, we want to know how well the models represent the data. We need: 1. The R2 value of the models (closer to 1 is best) 2. The p-value of the models (&lt;0.05 is required for there to be a relationship) 3. We can calculate residual standard error. Lower = more accurate!\nThe R2 and p are in the summary! Below is the formula for RMSE\n\nsummary(lm3)\n\n\nCall:\nlm(formula = bw ~ surface_area + species, data = bz)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-24.888  -4.955  -0.625   3.795  32.245 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.67334    0.60574   7.715 2.96e-14 ***\nsurface_area  0.82782    0.02485  33.319  &lt; 2e-16 ***\nspeciesSSID   2.89076    0.46404   6.230 6.92e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.111 on 984 degrees of freedom\n  (77 observations deleted due to missingness)\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.5294 \nF-statistic: 555.5 on 2 and 984 DF,  p-value: &lt; 2.2e-16\n\n#R2=0.5294, p&lt;0.001\n\nsummary(lm4)\n\n\nCall:\nlm(formula = bw ~ surface_area * species, data = bz)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-24.150  -4.871  -0.570   3.781  32.856 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               4.21829    0.69708   6.051 2.04e-09 ***\nsurface_area              0.84963    0.02984  28.470  &lt; 2e-16 ***\nspeciesSSID               4.17376    1.07855   3.870 0.000116 ***\nsurface_area:speciesSSID -0.07092    0.05383  -1.318 0.187925    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.109 on 983 degrees of freedom\n  (77 observations deleted due to missingness)\nMultiple R-squared:  0.5311,    Adjusted R-squared:  0.5297 \nF-statistic: 371.2 on 3 and 983 DF,  p-value: &lt; 2.2e-16\n\n#R2=0.5297, p&lt;0.01\n\n#RSE: &lt;- LOWER RSE= more accurate the model!\nsigma(lm3)\n\n[1] 7.111301\n\nsigma(lm4)\n\n[1] 7.108642\n\nmean(bz$bw, na.rm=T)\n\n[1] 21.54501\n\nsigma(lm3)/mean(bz$bw, na.rm=T)\n\n[1] 0.3300672\n\n#0.3300672\n\nsigma(lm4)/mean(bz$bw, na.rm=T)\n\n[1] 0.3299438\n\n#0.3299438\n\nWhen we look at R2, p, and RMSE, we see that these two models are nearly identical! Not suprising. As we discussed earlier, the “simpler” model is typically the choice when the models are about the same.\nWe can also get this information from the performance package using model_performance(). This function tells us many things, including R2 and RMSE. We will discuss the rest of this later\n\nmodel_performance(lm3)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n6678.347 | 6678.387 | 6697.925 | 0.530 |     0.529 | 7.100 | 7.111\n\n\n\nmodel_performance(lm4)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n6678.605 | 6678.666 | 6703.078 | 0.531 |     0.530 | 7.094 | 7.109\n\n\nHere we see two tables of MANY values. / The first is AIC or Akaike Information Criterion. AIC is an estimator of prediction error and is generally thought of as a way to rank the relative quality of statistical models. LOWER AIC is best. In our case, the AIC scores are almost exactly the same! AICc is a corrected version of AIC that is better suited for use with small sample sizes. BIC is Bayesian Information Criterion, another way to assess models. Again, lower is better. In our case, we seee that lm4 has a very slightly lower BIC score, but it likely isn’t enough to matter. **R2 and adjusted R2* are familiar to use. RMSE and Sigma are similar to what we just calculated by hand. RMSE = Root mean squared error, which is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data and is a difference between observed data and models predicted values. Sigma is the residual standard deviation. More deviation is less ideal.\nWe are pretty new to stats, so it is great to have some quick ways to evaluate a model, BUT, we are likely to struggle to interpret this output. Luckily, we can compare performance pretty easily…\n\n\nWe can use check_model to check our assumptions. We care most about collinearity, homogeneity of variance, and outliers (influential observations). Collinearity is often assess with Variance Inflation Factor (VIF). VIF &gt;5 is a potential issue and VIF &gt;10 is a problem that means the model is too complex!\n\nmodel_performance(lm3)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n6678.347 | 6678.387 | 6697.925 | 0.530 |     0.529 | 7.100 | 7.111\n\ncheck_model(lm3)#things look good, including low collinearity (VIF)\n\n\n\nvif(lm3)\n\nsurface_area      species \n      1.0485       1.0485 \n\n\nHere, we are good. Let’s check on lm4\n\n\nmodel_performance(lm4)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n6678.605 | 6678.666 | 6703.078 | 0.531 |     0.530 | 7.094 | 7.109\n\ncheck_model(lm4) #things look good, but we have VIF that look a little high...\n\n\n\ncheck_collinearity(lm4) #a table of collinearity results - we see a few VIF above 5..\n\n# Check for Multicollinearity\n\nLow Correlation\n\n         Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n surface_area 1.51 [1.40, 1.66]         1.23      0.66     [0.60, 0.71]\n\nModerate Correlation\n\n                 Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n              species 5.67 [5.07, 6.36]         2.38      0.18     [0.16, 0.20]\n surface_area:species 5.45 [4.88, 6.11]         2.34      0.18     [0.16, 0.20]\n\nvif(lm4) #gives us a simpler table---again, some VIF above 5. That is cause for concern (model is overfit/too complex)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n        surface_area              species surface_area:species \n            1.513859             5.668388             5.454545 \n\n\nFrom these results, we would say that lm4 is perhaps overfit or too complex, which would cause us to hesitate to use it. BUT, for the sake of learning, let’s keep going and compare the performance of the models.\n\n\n\ncompare_performance(lm3,lm4, rank=TRUE)\n\n# Comparison of Model Performance Indices\n\nName | Model |    R2 | R2 (adj.) |  RMSE | Sigma | AIC weights | AICc weights | BIC weights | Performance-Score\n---------------------------------------------------------------------------------------------------------------\nlm4  |    lm | 0.531 |     0.530 | 7.094 | 7.109 |       0.468 |        0.465 |       0.071 |            57.14%\nlm3  |    lm | 0.530 |     0.529 | 7.100 | 7.111 |       0.532 |        0.535 |       0.929 |            42.86%\n\n\nHere, the performance package has given us a ranking and has chosen Model 4, the more complex model, ahead of Model 3. We did some sleuthing earlier and chose model 3 over model 4 because it was simpler and they showed the same results.\nInteresting, huh? So, what is right? Which model do we choose? You can make a case for either decision and being able to back up your decision with data is the key. That said, since there really is not a difference and model 3 is simpler, I’d stick with it. EVEN if the model evaluation materials I have suggest that model 4 could be ever so slightly “better.”\nKeep in mind that stats are tool! A useful one. But, without being driven by human decision making and our hypotheses, they can become unnecessarily complex. If you have a few models and one of them fits your hypotheses and allows you to evaluate them and the others do not, that is probably the “best” model for you even if it is not the “best” model statistically. As long as it is a valid model, it is worth using. There are a lot of exceptions and special circumstances to consider and I am sure you will encounter these as you progress in science. For now, we have at least learned a simple pipeline for doing regression!\nIf we remember, the VIF are a little high in model 4– just above 5 is really not THAT high, but it may be some cause for concern if we have low sample sizes. So, we might pick lm3 instead on that basis alone.\n\n\nSometimes we want to visualize the effects of our model predictors. This is especially key when we have super complex models with a lot of variables! This is not one of those cases, but you will encounter these later in your life (possible in your assignment here!).\ncombine data! Use tidy() from the broom package to get nice neat dataframes from models\n\n#make a neat data frame\ncoefs&lt;-tidy(lm3, quick=FALSE)\ncoefs\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     4.67     0.606       7.72 2.96e- 14\n2 surface_area    0.828    0.0248     33.3  1.46e-163\n3 speciesSSID     2.89     0.464       6.23 6.92e- 10\n\n#make confint into a dataframe\nci&lt;-data.table(confint(lm3), keep.rownames='term')\nci\n\n           term     2.5 %   97.5 %\n         &lt;char&gt;     &lt;num&gt;    &lt;num&gt;\n1:  (Intercept) 3.4846479 5.862035\n2: surface_area 0.7790675 0.876581\n3:  speciesSSID 1.9801321 3.801380\n\n#bind the coefs data frame with the ci dataframe\ncidf&lt;-cbind(coefs,ci)\ncidf\n\n          term  estimate  std.error statistic       p.value         term\n1  (Intercept) 4.6733416 0.60574157  7.715075  2.957752e-14  (Intercept)\n2 surface_area 0.8278243 0.02484574 33.318557 1.455258e-163 surface_area\n3  speciesSSID 2.8907561 0.46404112  6.229526  6.923713e-10  speciesSSID\n      2.5 %   97.5 %\n1 3.4846479 5.862035\n2 0.7790675 0.876581\n3 1.9801321 3.801380\n\ncolnames(cidf)\n\n[1] \"term\"      \"estimate\"  \"std.error\" \"statistic\" \"p.value\"   \"term\"     \n[7] \"2.5 %\"     \"97.5 %\"   \n\n#remove a column we don't need\ncidf&lt;-cidf[,-6]\n\n#rename the % columnns to uppper and lower because it is easier to use\ncidf&lt;- cidf %&gt;%\n  rename(\"lower\"=\"2.5 %\",\n         \"upper\"=\"97.5 %\")\n\ncidf\n\n          term  estimate  std.error statistic       p.value     lower    upper\n1  (Intercept) 4.6733416 0.60574157  7.715075  2.957752e-14 3.4846479 5.862035\n2 surface_area 0.8278243 0.02484574 33.318557 1.455258e-163 0.7790675 0.876581\n3  speciesSSID 2.8907561 0.46404112  6.229526  6.923713e-10 1.9801321 3.801380\n\n#make term into a factor\ncidf$term=as.factor(cidf$term)\n\nNow make a plot!\n\nggplot(data=cidf, aes(x=estimate, y=term))+\n  geom_vline(xintercept = 0, linetype=2)+\n  geom_point(size=3)+\n  geom_errorbarh(aes(xmax=lower, xmin=upper),height=0.2)+\n  theme_classic()\n\n\n\n\nNote that there are many ways to build a dataframe and plot for these. This is just one example. What we see in this kind of plot, called a coefficient or ‘coef’ plot, is the estimated effect of each term of the model. A trick here is that some of these estimates (from our coefficient table) are slopes and others may be intercepts. You can see them with 95% confidence intervals displayed here. Generally, there is a significant effect if the mean and error do not overlap with the 0 line (check this with p values from the table). I wonder if lm4 looks different?\n\nCoef plot for lm4\n\n#make a neat data frame\ncoefs&lt;-tidy(lm4, quick=FALSE)\ncoefs\n\n# A tibble: 4 × 5\n  term                     estimate std.error statistic   p.value\n  &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)                4.22      0.697       6.05 2.04e-  9\n2 surface_area               0.850     0.0298     28.5  1.67e-130\n3 speciesSSID                4.17      1.08        3.87 1.16e-  4\n4 surface_area:speciesSSID  -0.0709    0.0538     -1.32 1.88e-  1\n\n#make confint into a dataframe\nci&lt;-data.table(confint(lm4), keep.rownames='term')\nci\n\n                       term      2.5 %     97.5 %\n                     &lt;char&gt;      &lt;num&gt;      &lt;num&gt;\n1:              (Intercept)  2.8503641 5.58621772\n2:             surface_area  0.7910625 0.90819063\n3:              speciesSSID  2.0572380 6.29028297\n4: surface_area:speciesSSID -0.1765531 0.03470322\n\n#bind the coefs data frame with the ci dataframe\ncidf&lt;-cbind(coefs,ci)\ncidf\n\n                      term    estimate  std.error statistic       p.value\n1              (Intercept)  4.21829090 0.69707532  6.051413  2.039310e-09\n2             surface_area  0.84962656 0.02984339 28.469509 1.666402e-130\n3              speciesSSID  4.17376049 1.07854863  3.869794  1.161104e-04\n4 surface_area:speciesSSID -0.07092496 0.05382656 -1.317657  1.879254e-01\n                      term      2.5 %     97.5 %\n1              (Intercept)  2.8503641 5.58621772\n2             surface_area  0.7910625 0.90819063\n3              speciesSSID  2.0572380 6.29028297\n4 surface_area:speciesSSID -0.1765531 0.03470322\n\ncolnames(cidf)\n\n[1] \"term\"      \"estimate\"  \"std.error\" \"statistic\" \"p.value\"   \"term\"     \n[7] \"2.5 %\"     \"97.5 %\"   \n\n#remove a column we don't need\ncidf&lt;-cidf[,-6]\n\n#rename the % columnns to uppper and lower because it is easier to use\ncidf&lt;- cidf %&gt;%\n  rename(\"lower\"=\"2.5 %\",\n         \"upper\"=\"97.5 %\")\n\ncidf\n\n                      term    estimate  std.error statistic       p.value\n1              (Intercept)  4.21829090 0.69707532  6.051413  2.039310e-09\n2             surface_area  0.84962656 0.02984339 28.469509 1.666402e-130\n3              speciesSSID  4.17376049 1.07854863  3.869794  1.161104e-04\n4 surface_area:speciesSSID -0.07092496 0.05382656 -1.317657  1.879254e-01\n       lower      upper\n1  2.8503641 5.58621772\n2  0.7910625 0.90819063\n3  2.0572380 6.29028297\n4 -0.1765531 0.03470322\n\n#make term into a factor\ncidf$term=as.factor(cidf$term)\n\nggplot(data=cidf, aes(x=estimate, y=term))+\n  geom_vline(xintercept = 0, linetype=2)+\n  geom_point(size=3)+\n  geom_errorbarh(aes(xmax=lower, xmin=upper),height=0.2)+\n  theme_classic()\n\n\n\n\nAs before, the interaction term does not have a significant effect! While it can be a bit hard to interpret these, at least we can quickly see a visual of statistical significance. When we have 10 terms in out model, this is super helpful!\n\n\n\n\n\nLab 7 Assignment:\n\n1.) Load the penguins data from palmerpenguins and build an additive and interactive model for the effects of species and sex on bill_length_mm.\n2. Generate model summary tables using summary() and get_regression_table. Interpret these.\n2.) Make representative graphs for each model and plot them side by side to compare them visually. Make a visual assessment of the models. Do your visual assessments agree with your stats tables? Why or why not?\n3.) Make coef plots for each model. How do these compare to your tables?\n4.) Head over to the TidyTuesdsay database and grab a dataset to do more practice with. Your data MUST have at least 2 numerical variables and at least 2 useful categorical variables. Show me clear evidence that this is the case in your data. DO NOT just type the name of the df here. I don’t need to see 100 pages of data, just show me that you have a few useful categorical and numrical variables. HINT if you are trying to use a dataset that is all of the lines from a tv show or something similar, this is probably not appropriate data…\n5.) Pick a response variable (must be numeric) and build a series of models using a top-down modeling approach. You will start by using all of your variables (you have 4, 1 is the response, so the other 3 are explanatory- likely 2 categorical and 1 numeric). Your “top” model will be y~ xab, you will then replace one * with + in an iterative way until you reach y~x+a+b. For each model, generate the appropriate table(s), a useful graph using augment() (if possible), and a coef plot. You will need to CHECK each model to see if the models violate assumptions or not (check_model() is from the performance package, and is pretty easy to use– it generates a series of graphs of each assumption check and tells you what to look for in the subtitle of the graph). Do a visual assumption check for each assumption. If a model violates assumptions and is invalid, you should not use it in your model performance assessment. Each model that passes a model check needs to be compared to the others. Which model is the best fit? Is this the model you would use? Why or why not?\n6.) Render and submit on Lyceum"
  }
]